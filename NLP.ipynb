{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk.data\n",
    "import gensim\n",
    "from distutils.version import LooseVersion, StrictVersion\n",
    "import os\n",
    "import codecs\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "global word2vec_model\n",
    "from deepai_nlp.tokenization.crf_tokenizer import CrfTokenizer\n",
    "from deepai_nlp.tokenization.utils import preprocess_text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/github/ngxbac/aivivn_phanloaisacthaibinhluan/blob/master/baseline_lgbm_tfidf.ipynb#scrollTo=ojmynVfZtFRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSource(object):\n",
    "    def _load_raw_data(self,filename, is_train=True):\n",
    "        a = []\n",
    "        b = []\n",
    "        regex = 'train_'\n",
    "        if not is_train:\n",
    "            regex = 'test_'\n",
    "        with open(filename, 'r', encoding=\"utf8\") as file:\n",
    "            for line in file :\n",
    "                if regex in line:\n",
    "                    b.append(a)\n",
    "                    a = [line]\n",
    "                elif line!='\\n':\n",
    "                    a.append(line)       \n",
    "        b.append(a)      \n",
    "        return b[1:]\n",
    "    \n",
    "    def _create_row(self, sample, is_train=True):\n",
    "        d = {}\n",
    "        d['id'] = sample[0].replace('\\n','')\n",
    "        review = \"\"\n",
    "        if is_train:\n",
    "            for clause in sample[1:-1]:\n",
    "                review+= clause.replace('\\n','').strip()\n",
    "            d['label'] = int(sample[-1].replace('\\n',''))          \n",
    "        else:         \n",
    "            for clause in sample[1:]:\n",
    "                review+= clause.replace('\\n','').strip()\n",
    "        d['review'] = review\n",
    "        return d\n",
    "    \n",
    "    \n",
    "    def load_data(self, filename, is_train=True):\n",
    "        raw_data = self._load_raw_data(filename, is_train)\n",
    "        lst = []\n",
    "        for row in raw_data:\n",
    "            lst.append(self._create_row(row, is_train))\n",
    "        return lst"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "search = \"d you don't need a dog. but if you like dogs, you should think of getting one for your own. Or a cat?\"\n",
    "mapping =  {\"you\": \"aaaaa\", r\"\\bd\\b\": \"!!!\" }\n",
    "\n",
    "#search = [search.replace(key, value) for key, value in mapping.items()][0]\n",
    "\n",
    "for key, value in mapping.items():\n",
    "    search = re.sub(key,value,search)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mapping = {\n",
    "    \"ship\": \"v·∫≠n chuy·ªÉn\",\n",
    "    \"shop\": \"c·ª≠a h√†ng\",\n",
    "    \"sp\": \"s·∫£n ph·∫©m\",\n",
    "    r\"\\bm\\b\": \" m√¨nh\",\n",
    "    \"mik\": \"m√¨nh\",\n",
    "    r\"\\bk\\b\": \"kh√¥ng\",\n",
    "    r\"\\bkh\\b\": \"kh√¥ng\",\n",
    "    r\"\\btl\\b\": \"tr·∫£ l·ªùi\",\n",
    "    r\"\\br\\b\": \"r·ªìi\",\n",
    "    \"fb\": \"m·∫°ng x√£ h·ªôi \", # facebook\n",
    "    \"face\": \"m·∫°ng x√£ h·ªôi\",\n",
    "    \"thanks\": \"c·∫£m ∆°n\",\n",
    "    \"thank\": \"c·∫£m ∆°n\",\n",
    "    \"tks\": \"c·∫£m ∆°n\", \n",
    "    r\"\\bdc\\b\": \"ƒë∆∞·ª£c\",\n",
    "    r\"\\bok\\b\": \"t·ªët\",\n",
    "    r\"\\bdt\\b\": \"ƒëi·ªán tho·∫°i\",\n",
    "    r\"\\bh\\b\": \"gi·ªù\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mapping = {\n",
    "    \"ship\": \"v·∫≠n chuy·ªÉn\",\n",
    "    \"shop\": \"c·ª≠a h√†ng\",\n",
    "    \"sp\": \"s·∫£n ph·∫©m\",\n",
    "    \"m\": \" m√¨nh\",\n",
    "    \"mik\": \"m√¨nh\",\n",
    "    \"k\": \"kh√¥ng\",\n",
    "    \"kh\": \"kh√¥ng\",\n",
    "    \"tl\": \"tr·∫£ l·ªùi\",\n",
    "    \"r\": \"r·ªìi\",\n",
    "    \"fb\": \"m·∫°ng x√£ h·ªôi \", # facebook\n",
    "    \"face\": \"m·∫°ng x√£ h·ªôi\",\n",
    "    \"thanks\": \"c·∫£m ∆°n\",\n",
    "    \"thank\": \"c·∫£m ∆°n\",\n",
    "    \"tks\": \"c·∫£m ∆°n\", \n",
    "    \"dc\": \"ƒë∆∞·ª£c\",\n",
    "    \"ok\": \"t·ªët\",\n",
    "    \"dt\": \"ƒëi·ªán tho·∫°i\",\n",
    "    \"h\": \"gi·ªù\",\n",
    "    \"hsd\": \"h·∫°n s·ª≠ d·ª•ng\",\n",
    "    \"trc\": \"tr∆∞·ªõc\",\n",
    "    \"oki\": \"t·ªët\",\n",
    "    \"ad\": \"c·ª≠a h√†ng\"\n",
    "}\n",
    "for i, m in mapping.items():\n",
    "    mapping[i] = m.strip().replace(' ','_')\n",
    "#Load stopwords\n",
    "stopwords_file = 'vietnamese-stopwords.txt'\n",
    "stopwords = []\n",
    "with open(stopwords_file, 'r', encoding=\"utf8\") as file:\n",
    "    for line in file :\n",
    "        stopwords.append(line.replace('\\n','').strip().replace(' ','_'))\n",
    "tokenizer = CrfTokenizer()\n",
    "vietnamese_chars = \"[^a-zA-Z_√Ä√Å√Ç√É√à√â√ä√å√ç√í√ì√î√ï√ô√öƒÇƒêƒ®≈®∆†√†√°√¢√£√®√©√™√¨√≠√≤√≥√¥√µ√π√∫ƒÉƒëƒ©≈©∆°∆ØƒÇ·∫†·∫¢·∫§·∫¶·∫®·∫™·∫¨·∫Æ·∫∞·∫≤·∫¥·∫∂·∫∏·∫∫·∫º·ªÄ·ªÄ·ªÇ∆∞ƒÉ·∫°·∫£·∫•·∫ß·∫©·∫´·∫≠·∫Ø·∫±·∫≥·∫µ·∫∑·∫π·∫ª·∫Ω·ªÅ·ªÅ·ªÉ·ªÑ·ªÜ·ªà·ªä·ªå·ªé·ªê·ªí·ªî·ªñ·ªò·ªö·ªú·ªû·ª†·ª¢·ª§·ª¶·ª®·ª™·ªÖ·ªá·ªâ·ªã·ªç·ªè·ªë·ªì·ªï·ªó·ªô·ªõ·ªù·ªü·ª°·ª£·ª•·ªß·ª©·ª´·ª¨·ªÆ·ª∞·ª≤·ª¥√ù·ª∂·ª∏·ª≠·ªØ·ª±·ª≥·ªµ·ª∑·ªπ]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ship': 'v·∫≠n_chuy·ªÉn',\n",
       " 'shop': 'c·ª≠a_h√†ng',\n",
       " 'sp': 's·∫£n_ph·∫©m',\n",
       " 'm': 'm√¨nh',\n",
       " 'mik': 'm√¨nh',\n",
       " 'k': 'kh√¥ng',\n",
       " 'kh': 'kh√¥ng',\n",
       " 'tl': 'tr·∫£_l·ªùi',\n",
       " 'r': 'r·ªìi',\n",
       " 'fb': 'm·∫°ng_x√£_h·ªôi',\n",
       " 'face': 'm·∫°ng_x√£_h·ªôi',\n",
       " 'thanks': 'c·∫£m_∆°n',\n",
       " 'thank': 'c·∫£m_∆°n',\n",
       " 'tks': 'c·∫£m_∆°n',\n",
       " 'dc': 'ƒë∆∞·ª£c',\n",
       " 'ok': 't·ªët',\n",
       " 'dt': 'ƒëi·ªán_tho·∫°i',\n",
       " 'h': 'gi·ªù',\n",
       " 'hsd': 'h·∫°n_s·ª≠_d·ª•ng',\n",
       " 'trc': 'tr∆∞·ªõc',\n",
       " 'oki': 't·ªët',\n",
       " 'ad': 'c·ª≠a_h√†ng'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vietnamese_chars = \"[^a-z0-9A-Z_√Ä√Å√Ç√É√à√â√ä√å√ç√í√ì√î√ï√ô√öƒÇƒêƒ®≈®∆†√†√°√¢√£√®√©√™√¨√≠√≤√≥√¥√µ√π√∫ƒÉƒëƒ©≈©∆°∆ØƒÇ·∫†·∫¢·∫§·∫¶·∫®·∫™·∫¨·∫Æ·∫∞·∫≤·∫¥·∫∂·∫∏·∫∫·∫º·ªÄ·ªÄ·ªÇ∆∞ƒÉ·∫°·∫£·∫•·∫ß·∫©·∫´·∫≠·∫Ø·∫±·∫≥·∫µ·∫∑·∫π·∫ª·∫Ω·ªÅ·ªÅ·ªÉ·ªÑ·ªÜ·ªà·ªä·ªå·ªé·ªê·ªí·ªî·ªñ·ªò·ªö·ªú·ªû·ª†·ª¢·ª§·ª¶·ª®·ª™·ªÖ·ªá·ªâ·ªã·ªç·ªè·ªë·ªì·ªï·ªó·ªô·ªõ·ªù·ªü·ª°·ª£·ª•·ªß·ª©·ª´·ª¨·ªÆ·ª∞·ª≤·ª¥√ù·ª∂·ª∏·ª≠·ªØ·ª±·ª≥·ªµ·ª∑·ªπ]\"\n",
    "def review_wordlist(review, remove_stopwords= False):\n",
    "    review_text = str(review)\n",
    "    # 2. Removing non-letter.\n",
    "    review_text = re.sub(vietnamese_chars,\" \",review_text)\n",
    "    # 3. Converting to lower case and splitting\n",
    "    words = review_text.lower().split()\n",
    "    # 4. Optionally remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords)     \n",
    "        words = [w for w in words if not w in stops]\n",
    "    for i,w in enumerate(words):\n",
    "        if w in mapping:\n",
    "            words[i]= mapping[w]\n",
    "    \n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokenizer = CrfTokenizer()\n",
    "vietnamese_chars = \"[^a-zA-Z_√Ä√Å√Ç√É√à√â√ä√å√ç√í√ì√î√ï√ô√öƒÇƒêƒ®≈®∆†√†√°√¢√£√®√©√™√¨√≠√≤√≥√¥√µ√π√∫ƒÉƒëƒ©≈©∆°∆ØƒÇ·∫†·∫¢·∫§·∫¶·∫®·∫™·∫¨·∫Æ·∫∞·∫≤·∫¥·∫∂·∫∏·∫∫·∫º·ªÄ·ªÄ·ªÇ∆∞ƒÉ·∫°·∫£·∫•·∫ß·∫©·∫´·∫≠·∫Ø·∫±·∫≥·∫µ·∫∑·∫π·∫ª·∫Ω·ªÅ·ªÅ·ªÉ·ªÑ·ªÜ·ªà·ªä·ªå·ªé·ªê·ªí·ªî·ªñ·ªò·ªö·ªú·ªû·ª†·ª¢·ª§·ª¶·ª®·ª™·ªÖ·ªá·ªâ·ªã·ªç·ªè·ªë·ªì·ªï·ªó·ªô·ªõ·ªù·ªü·ª°·ª£·ª•·ªß·ª©·ª´·ª¨·ªÆ·ª∞·ª≤·ª¥√ù·ª∂·ª∏·ª≠·ªØ·ª±·ª≥·ªµ·ª∑·ªπ]\"\n",
    "def review_wordlist(review, remove_stopwords= False):\n",
    "    review_text = str(review)\n",
    "    # 2. Removing non-letter.\n",
    "    review_text = re.sub(vietnamese_chars,\" \",review_text)\n",
    "    # 3. Converting to lower case and splitting\n",
    "    review_text = review_text.lower()\n",
    "    for key, value in mapping.items():\n",
    "        review_text = re.sub(key,value,review_text)\n",
    "    words = tokenizer.tokenize(review_text)\n",
    "    # 4. Optionally remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords)     \n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CrfTokenizer()\n",
    "def clean_text(review, char_reg, stopwords, mapping):\n",
    "    review_text = str(review)\n",
    "    # 2. Removing non-letter.\n",
    "    review_text = re.sub(char_reg,\" \",review_text)\n",
    "    # 3. Converting to lower case and splitting\n",
    "    review_text = review_text.lower()\n",
    "    # 4. Subtitute words\n",
    "    #words = review_text.split()\n",
    "    words = tokenizer.tokenize(review_text)\n",
    "    print(words)\n",
    "    # 5. Remove stopwords\n",
    "    #words = [w.replace('_',' ') for w in words]\n",
    "    stops = set(stopwords)  \n",
    "    #print(stops)\n",
    "    words = [w for w in words if not w in stops]\n",
    "    print(words)\n",
    "    for i,w in enumerate(words):\n",
    "        if w in mapping:\n",
    "            words[i]= mapping[w]\n",
    "    review_text = ' '.join(words)\n",
    "    return(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from file C:\\ProgramData\\Anaconda3\\lib\\site-packages\\deepai_nlp-0.0.1-py3.7.egg\\deepai_nlp\\models/pretrained_tokenizer.crfsuite\n",
      "['ch∆∞a', 'd√πng', 'th·ª≠', 'n√™n', 'ch∆∞a', 'bi', 't']\n",
      "['th·ª≠', 'bi', 't']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'th·ª≠ bi t'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"ch∆∞a d√πng th·ª≠ n√™n ch∆∞a bi·∫øt\",char_reg = vietnamese_chars, mapping =mapping, stopwords = stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from file C:\\ProgramData\\Anaconda3\\lib\\site-packages\\deepai_nlp-0.0.1-py3.7.egg\\deepai_nlp\\models/pretrained_tokenizer.crfsuite\n"
     ]
    }
   ],
   "source": [
    "ds = DataSource()\n",
    "train_data = pd.DataFrame(ds.load_data('dataset/train.crash'))\n",
    "test_data = pd.DataFrame(ds.load_data('dataset/test.crash', is_train=False))\n",
    "#train_data['review'] = train_data['review'].fillna(\"none\")\n",
    "#test_data['review'] = test_data['review'].fillna(\"none\")\n",
    "df = pd.concat([train_data,test_data], axis=0, sort=False)\n",
    "df['review_cleaned'] = df['review'].apply(lambda s: clean_text(s,char_reg = vietnamese_chars, mapping =mapping, stopwords = stopwords))\n",
    "df['num_words'] = df['review'].apply(lambda s: len(s.split()))\n",
    "df['num_unique_words'] = df['review'].apply(lambda s: len(set(w for w in s.split())))\n",
    "df['words_vs_unique'] = df['num_unique_words'] / df['num_words'] * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "      <th>review_cleaned</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Dung dc sp tot cam onshop ƒê√≥ng g√≥i s·∫£n ph·∫©m r...</td>\n",
       "      <td>dung ƒë∆∞·ª£c s·∫£n ph·∫©m tot cam onshop ƒë√≥ng_g√≥i s·∫£n...</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>90.476190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi . Son m·ªãn nh∆∞n...</td>\n",
       "      <td>ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi son m·ªãn ƒë√°nh m√†u...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi nh∆∞ng k c√≥ h·ªôp...</td>\n",
       "      <td>ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi kh√¥ng h·ªôp kh√¥ng ...</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>78.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_000003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\":(( M√¨nh h∆°i th·∫•t v·ªçng 1 ch√∫t v√¨ m√¨nh ƒë√£ k·ª≥ v...</td>\n",
       "      <td>h∆°i th·∫•t_v·ªçng ch√∫t k·ª≥_v·ªçng s√°ch hi_v·ªçng h·ªçc_t·∫≠...</td>\n",
       "      <td>114</td>\n",
       "      <td>92</td>\n",
       "      <td>80.701754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_000004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"L·∫ßn tr∆∞·ªõc m√¨nh mua √°o gi√≥ m√†u h·ªìng r·∫•t ok m√† ...</td>\n",
       "      <td>mua √°o_gi√≥ m√†u h·ªìng t·ªët ƒë·ª£t giao √°o_gi√≥ ch·∫•t v...</td>\n",
       "      <td>26</td>\n",
       "      <td>24</td>\n",
       "      <td>92.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train_000005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi c√≥ ƒëi·ªÅu kh√¥ng ...</td>\n",
       "      <td>ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi c·ª©ng_c√°p c·ªë_ƒë·ªãnh...</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>95.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train_000006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"ƒê√£ nh·∫≠n ƒëc h√†ng r·∫•t nhanh m·ªõi ƒë·∫∑t bu·ªïi t·ªëi m√†...</td>\n",
       "      <td>ƒëc h√†ng t·ªëi tr∆∞a mai ƒë√≥ng_g√≥i s·∫£n_ph·∫©m ƒë·∫πp c·ª≠a...</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>93.548387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train_000007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"C√°c si√™u ph·∫©m th·∫•y c·∫•u h√¨nh to√†n t·ª±a t·ª±a nhau...</td>\n",
       "      <td>si√™u ph·∫©m c·∫•u_h√¨nh to√†n t·ª±a t·ª±a ko ƒë·ªôt_ph√° n√¢n...</td>\n",
       "      <td>48</td>\n",
       "      <td>44</td>\n",
       "      <td>91.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train_000008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"H√†ng ship nhanh  ch·∫•t l∆∞·ª£ng t·ªët  t∆∞ v·∫•n nhi·ªát...</td>\n",
       "      <td>h√†ng v·∫≠n chuy·ªÉn ch·∫•t_l∆∞·ª£ng t∆∞_v·∫•n nhi·ªát_t√¨nh v...</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>95.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train_000009</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"ƒê·ªìng h·ªì ƒë·∫πp nh∆∞ng 1 c√°i ƒë·ª©t d√¢y  1 c√°i k ch·∫°y...</td>\n",
       "      <td>ƒë·ªìng_h·ªì ƒë·∫πp ƒë·ª©t d√¢y kh√¥ng ch·∫°y mua ve s·ª≠a</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>87.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>train_000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi.y h√¨nh ch·ª•p.ƒë√°...</td>\n",
       "      <td>ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi y h√¨nh ch·ª•p ti·ªÅn</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>90.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>train_000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Hjhj shop giao h√†ng nhanh qu√°. ƒê·∫πp l·∫Øm ·∫° b√© n...</td>\n",
       "      <td>hjhj c·ª≠a h√†ng giao h√†ng ƒë·∫πp l·∫Øm b√©  m√¨nh</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>train_000012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"\"nh√¨n ƒë·∫πp ph·∫øt nh·ªâ..\"\"</td>\n",
       "      <td>ƒë·∫πp ph t</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>train_000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"ƒê√≥ng g√≥i r·∫•t ƒë·∫πp. Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m r·∫•t t·ªët...</td>\n",
       "      <td>ƒë√≥ng_g√≥i ƒë·∫πp ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m ch·∫•t_l∆∞·ª£ng s·∫£...</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>68.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>train_000014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"SƒÉn ƒëc v·ªõi gi√° 11k. To·∫πt v·ªùi\"</td>\n",
       "      <td>sƒÉn ƒëc gi√° kh√¥ng v·ªùi</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>train_000015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"OK r·∫•t h√†i l√≤ng\"</td>\n",
       "      <td>t·ªët h√†i_l√≤ng</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>train_000016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Giao thi·∫øu m√¨nh c√°i n√†y r·ªìi shop ∆°i T^T\"</td>\n",
       "      <td>giao thi u c·ª≠a h√†ng t t</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>train_000017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi t√¥i r·∫•t th√≠ch\"</td>\n",
       "      <td>ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>train_000018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Gi√†y ƒë·∫πp l·∫Øm c√≥ ƒëi·ªÅu d√¢y h∆°i ng·∫Øn t√≠ ·∫°¬† Ch·∫•t ...</td>\n",
       "      <td>gi√†y ƒë·∫πp l·∫Øm d√¢y h∆°i ng·∫Øn t√≠ ch·∫•t_l∆∞·ª£ng s·∫£n_ph...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>train_000019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Y·∫øm v·∫£i ƒë·∫πp nh∆∞ng √≠t m·∫´u ƒë·∫πp\"</td>\n",
       "      <td>y  m√¨nh v·∫£i ƒë·∫πp m·∫´u ƒë·∫πp</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>train_000020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒê√≥ng g√≥i s·∫£n p...</td>\n",
       "      <td>ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi ƒë√≥ng_g√≥i s·∫£n_ph·∫©...</td>\n",
       "      <td>33</td>\n",
       "      <td>28</td>\n",
       "      <td>84.848485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>train_000021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"kh√¥ng h√†i l√≤ng s·∫£n ph·∫©m cho l·∫Øm. gi·∫∑t lan ƒë·∫ßu...</td>\n",
       "      <td>h√†i_l√≤ng s·∫£n_ph·∫©m l·∫Øm gi·∫∑t lan da nhoe m√†u gi·ªù t</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>train_000022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Giao h√†ng nhanh, m·∫∑c ƒë·∫πpC√°m ∆°n shop\"</td>\n",
       "      <td>giao h√†ng m·∫∑c ƒë·∫πpc√°m ∆°n c·ª≠a h√†ng</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>train_000023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi bao b√¨ cute ph...</td>\n",
       "      <td>ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi bao_b√¨ cute ph√¥_...</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>train_000024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"ƒê·ªìng h·ªì th√¨ ƒë·∫πp th·∫≠t. Nh∆∞ng t·∫°i sao kim l√∫c c...</td>\n",
       "      <td>ƒë·ªìng_h·ªì ƒë·∫πp kim ch·∫°y ƒë·ª©ng nh·∫Øn_tin ko tr·∫£_l·ªùi ...</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>92.592593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>train_000025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Giao h√†ng si√™u nhanh.ƒê√≥ng g√≥i c·∫©n th·∫≠n v√† t∆∞ ...</td>\n",
       "      <td>giao h√†ng si√™u ƒë√≥ng_g√≥i c·∫©n_th·∫≠n t∆∞_v·∫•n nhi·ªát_...</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>94.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>train_000026</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"\"C≈©ng h∆°i b·∫•t ti·ªán xu th·∫ø n√†y e r·∫±ng ƒëa ph·∫±n ...</td>\n",
       "      <td>h∆°i b·∫•t_ti·ªán xu th e ƒëa ph·∫±n ∆∞a_th√≠ch</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>train_000027</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"To√†n h√†ng trungkhi mua qu√™n ko coi kƒ©\"</td>\n",
       "      <td>to√†n h√†ng trungkhi mua qu√™n ko coi kƒ©</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>train_000028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" ƒê√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn. ƒê∆∞·ª£c...</td>\n",
       "      <td>ƒë√≥ng_g√≥i s·∫£n_ph·∫©m ƒë·∫πp c·ª≠a h√†ng t·∫∑ng qu√†</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>train_000029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"H√¥m nay chi√™n th·ª≠ c√° h·ªìi, c√° chi√™n ƒÉn ng·ªçt h∆°...</td>\n",
       "      <td>h√¥m_nay chi√™n th·ª≠ c√°_h·ªìi c√°_chi√™n chi√™n ki·ªÉu t...</td>\n",
       "      <td>33</td>\n",
       "      <td>28</td>\n",
       "      <td>84.848485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>train_000070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"ƒê·∫πp th√≠ch h·∫øt\"</td>\n",
       "      <td>ƒë·∫πp gi·ªù t</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>train_000071</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Kem d√πng ·ªïn m·ªói t·ªôi h∆°i t·ªëi do v·ªõi da m√¨nh\"</td>\n",
       "      <td>kem ·ªïn m·ªói_t·ªôi h∆°i t·ªëi da</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>train_000072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"ƒê√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn ch·∫•t l...</td>\n",
       "      <td>ƒë√≥ng_g√≥i s·∫£n_ph·∫©m ƒë·∫πp ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>85.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>train_000073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Good value for money Good value for money Fa...</td>\n",
       "      <td>good value for money good value for money fast...</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>73.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>train_000074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m t·ªët ƒê√≥ng g√≥i s·∫£n ph·∫©m r·∫•...</td>\n",
       "      <td>ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m ƒë√≥ng_g√≥i s·∫£n_ph·∫©m c·∫©n_th·∫≠n...</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>86.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>train_000075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Ch·ªã ch·ªß t∆∞ v·∫•n r·∫•t nhi·ªát t√¨nh üòÜüòÜ h√¨nh d√°ng v√†...</td>\n",
       "      <td>ch·ªß t∆∞_v·∫•n nhi·ªát_t√¨nh_h√¨nh_d√°ng ch·∫•t_l∆∞·ª£ng s·∫£n...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>train_000076</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"SHOP ƒêƒÇNG ·∫¢NH M·ªòT ƒê·∫∞NG S·∫¢N PH·∫®M M·ªòT L·∫∫O .B√ÅN ...</td>\n",
       "      <td>c·ª≠a h√†ng ƒëƒÉng ·∫£nh ƒë·∫±ng s·∫£n_ph·∫©m l·∫ªo b√°n_c√°p √∫p...</td>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>87.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>train_000077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi. Shop t∆∞ v·∫•n n...</td>\n",
       "      <td>ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi c·ª≠a h√†ng t∆∞_v·∫•n ...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>train_000078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\" Poor delivery speed Poor product quality\"</td>\n",
       "      <td>poor delivery speed poor product quality</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>85.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>train_000079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"ƒê√∫ng l√† ti·ªÅn n√†o c·ªßa ƒë√≥ ko ngon m√πi k√¨ k√¨ ko ...</td>\n",
       "      <td>ti·ªÅn ko ngon m√πi k√¨ k√¨ ko th∆°m l·∫°p_x∆∞·ªüng b√¨nh_...</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>90.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>train_000080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi. ƒê√≥ng g√≥i s·∫£n ...</td>\n",
       "      <td>ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi ƒë√≥ng_g√≥i s·∫£n_ph·∫©...</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>92.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>train_000081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Gi√†y Ok l·∫Øm shop ship h√†ng nhanh n·ªØa ‚ù§Ô∏è\"</td>\n",
       "      <td>gi√†y t·ªët l·∫Øm c·ª≠a h√†ng v·∫≠n chuy·ªÉn h√†ng</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>train_000082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi.r·∫•t  OK \"</td>\n",
       "      <td>ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi t·ªët</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>88.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>train_000083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Gi√†y ch·∫Øc ch·∫Øn . ƒê·∫ø h∆°i b·∫©n t√≠ nh∆∞ng ch·∫Øc s·∫Ω ...</td>\n",
       "      <td>gi√†y ƒë h∆°i b·∫©n t√≠ lau giao h√†ng</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>88.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>train_000084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"H√†ng giao thi·∫øu k√™u g·ª≠i b√π cho m√† c·∫£ th√°ng ch...</td>\n",
       "      <td>h√†ng giao thi u k√™u g·ª≠i b√π g·ª≠i nh·∫Øn_tin ko tr·∫£...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>train_000085</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Shop h·∫øt h√†ng nh∆∞ng kh√¥ng b√°o. M√¨nh ƒë·∫∑t 1 c·∫∑p...</td>\n",
       "      <td>c·ª≠a h√†ng gi·ªù t h√†ng b√°o c·∫∑p th g·ªüi ·ªëp k√®m ti·ªÅn...</td>\n",
       "      <td>55</td>\n",
       "      <td>40</td>\n",
       "      <td>72.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>train_000086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Giao ko ƒë√∫ng ƒê·ªì. T√¥i ƒë√£ b·ªã l·ª´a \"</td>\n",
       "      <td>giao ko ƒë·ªì l·ª´a</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>train_000087</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"hk giong m·∫©u g√¨ c·∫£ da vay bi l·ªói nua¬†\"</td>\n",
       "      <td>hk giong m·∫©u da vay bi l·ªói nua</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>train_000088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"T∆∞∆°ng ƒë·ªëi h√†i l√≤ng gi√†y ch·∫Øc ch·∫Øn √™m nh∆∞ng ƒë∆∞...</td>\n",
       "      <td>t∆∞∆°ng_ƒë·ªëi h√†i_l√≤ng gi√†y √™m ƒë∆∞·ªùng may th·ª´a t·ªët ...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>train_000089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"D√©p xinh. Ch·ªß shop rep tn r·∫•t nhi·ªát t√¨nh. Ch√∫...</td>\n",
       "      <td>d√©p xinh ch·ªß c·ª≠a h√†ng rep tn nhi·ªát_t√¨nh ch√∫c c...</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>93.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>train_000090</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Mua 2ƒë c√≥ 2 l·ªó x·ªè d√¢y ib shop kh tr·∫£ l·ªùi gi√†y...</td>\n",
       "      <td>mua ƒë l·ªó x·ªè d√¢y ib c·ª≠a h√†ng kh√¥ng tr·∫£_l·ªùi gi√†y...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>train_000091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"T·ªët tgian giao h√†ng nhanh\"</td>\n",
       "      <td>tgian giao h√†ng</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>train_000092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"N√™n mua nha m·ªçi ng∆∞·ªùi\"</td>\n",
       "      <td>mua nha</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>train_000093</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Th·∫•y ghi ƒëc t·∫∑ng k√®m t√£ qu·∫ßn.ƒë·∫øn khi nh·∫≠n h√†n...</td>\n",
       "      <td>ghi ƒëc t·∫∑ng k√®m t√£ qu·∫ßn ƒë n h√†ng kh√¥ng c·ª≠a h√†n...</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>88.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>train_000094</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\" Shop ph·ª•c v·ª• r·∫•t k√©m. ƒê√£ ph·∫£i nh·∫Øn tin h·ªèi c...</td>\n",
       "      <td>c·ª≠a h√†ng ph·ª•c_v·ª• k√©m nh·∫Øn_tin c·∫©n_th·∫≠n kh√¥ng c...</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>87.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>train_000095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Shop ph·ª•c v·ª• r·∫•t t·ªët. S·∫£n ph·∫©m ƒë√∫ng v·ªõi th√¥ng...</td>\n",
       "      <td>c·ª≠a h√†ng ph·ª•c_v·ª• s·∫£n_ph·∫©m th√¥ng_tin ƒëƒÉng_t·∫£i</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>train_000096</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"ƒê·∫∑t combo m√† shop ch·ªâ giao b√°nh g·∫°o v√† s·ªët k ...</td>\n",
       "      <td>combo c·ª≠a h√†ng giao b√°nh g·∫°o s·ªët kh√¥ng ph√¥_mai...</td>\n",
       "      <td>45</td>\n",
       "      <td>39</td>\n",
       "      <td>86.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>train_000097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Shop n√†y uy t√≠n n√® h√†ng ƒë√£ ƒë·∫πp m√† c√≤n ƒë·∫ßy ƒë·ªß ...</td>\n",
       "      <td>c·ª≠a h√†ng uy_t√≠n n√® h√†ng ƒë·∫πp ƒë·∫ßy_ƒë·ªß h·ªôp pin t·∫∑n...</td>\n",
       "      <td>37</td>\n",
       "      <td>35</td>\n",
       "      <td>94.594595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>train_000098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒê√≥ng g√≥i s·∫£n p...</td>\n",
       "      <td>ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi ƒë√≥ng_g√≥i s·∫£n_ph·∫©...</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>train_000099</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"S·∫£n ph·∫©m k nh∆∞ qu·∫£ng c√°o l√† 9058 li√™n h·ªá theo...</td>\n",
       "      <td>s·∫£n_ph·∫©m kh√¥ng qu·∫£ng_c√°o li√™n_h·ªá ƒët c·ª≠a h√†ng v...</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>96.428571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  label                                             review  \\\n",
       "0   train_000000    0.0  \"Dung dc sp tot cam onshop ƒê√≥ng g√≥i s·∫£n ph·∫©m r...   \n",
       "1   train_000001    0.0  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi . Son m·ªãn nh∆∞n...   \n",
       "2   train_000002    0.0  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi nh∆∞ng k c√≥ h·ªôp...   \n",
       "3   train_000003    1.0  \":(( M√¨nh h∆°i th·∫•t v·ªçng 1 ch√∫t v√¨ m√¨nh ƒë√£ k·ª≥ v...   \n",
       "4   train_000004    1.0  \"L·∫ßn tr∆∞·ªõc m√¨nh mua √°o gi√≥ m√†u h·ªìng r·∫•t ok m√† ...   \n",
       "5   train_000005    0.0  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi c√≥ ƒëi·ªÅu kh√¥ng ...   \n",
       "6   train_000006    0.0  \"ƒê√£ nh·∫≠n ƒëc h√†ng r·∫•t nhanh m·ªõi ƒë·∫∑t bu·ªïi t·ªëi m√†...   \n",
       "7   train_000007    1.0  \"C√°c si√™u ph·∫©m th·∫•y c·∫•u h√¨nh to√†n t·ª±a t·ª±a nhau...   \n",
       "8   train_000008    0.0  \"H√†ng ship nhanh  ch·∫•t l∆∞·ª£ng t·ªët  t∆∞ v·∫•n nhi·ªát...   \n",
       "9   train_000009    1.0  \"ƒê·ªìng h·ªì ƒë·∫πp nh∆∞ng 1 c√°i ƒë·ª©t d√¢y  1 c√°i k ch·∫°y...   \n",
       "10  train_000010    0.0  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi.y h√¨nh ch·ª•p.ƒë√°...   \n",
       "11  train_000011    0.0  \"Hjhj shop giao h√†ng nhanh qu√°. ƒê·∫πp l·∫Øm ·∫° b√© n...   \n",
       "12  train_000012    0.0                            \"\"nh√¨n ƒë·∫πp ph·∫øt nh·ªâ..\"\"   \n",
       "13  train_000013    0.0  \"ƒê√≥ng g√≥i r·∫•t ƒë·∫πp. Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m r·∫•t t·ªët...   \n",
       "14  train_000014    0.0                     \"SƒÉn ƒëc v·ªõi gi√° 11k. To·∫πt v·ªùi\"   \n",
       "15  train_000015    0.0                                  \"OK r·∫•t h√†i l√≤ng\"   \n",
       "16  train_000016    1.0          \"Giao thi·∫øu m√¨nh c√°i n√†y r·ªìi shop ∆°i T^T\"   \n",
       "17  train_000017    0.0      \"Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi t√¥i r·∫•t th√≠ch\"   \n",
       "18  train_000018    0.0  \"Gi√†y ƒë·∫πp l·∫Øm c√≥ ƒëi·ªÅu d√¢y h∆°i ng·∫Øn t√≠ ·∫°¬† Ch·∫•t ...   \n",
       "19  train_000019    0.0                     \"Y·∫øm v·∫£i ƒë·∫πp nh∆∞ng √≠t m·∫´u ƒë·∫πp\"   \n",
       "20  train_000020    0.0  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒê√≥ng g√≥i s·∫£n p...   \n",
       "21  train_000021    1.0  \"kh√¥ng h√†i l√≤ng s·∫£n ph·∫©m cho l·∫Øm. gi·∫∑t lan ƒë·∫ßu...   \n",
       "22  train_000022    0.0              \"Giao h√†ng nhanh, m·∫∑c ƒë·∫πpC√°m ∆°n shop\"   \n",
       "23  train_000023    0.0  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi bao b√¨ cute ph...   \n",
       "24  train_000024    1.0  \"ƒê·ªìng h·ªì th√¨ ƒë·∫πp th·∫≠t. Nh∆∞ng t·∫°i sao kim l√∫c c...   \n",
       "25  train_000025    0.0  \"Giao h√†ng si√™u nhanh.ƒê√≥ng g√≥i c·∫©n th·∫≠n v√† t∆∞ ...   \n",
       "26  train_000026    1.0  \"\"C≈©ng h∆°i b·∫•t ti·ªán xu th·∫ø n√†y e r·∫±ng ƒëa ph·∫±n ...   \n",
       "27  train_000027    1.0            \"To√†n h√†ng trungkhi mua qu√™n ko coi kƒ©\"   \n",
       "28  train_000028    0.0  \" ƒê√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn. ƒê∆∞·ª£c...   \n",
       "29  train_000029    0.0  \"H√¥m nay chi√™n th·ª≠ c√° h·ªìi, c√° chi√™n ƒÉn ng·ªçt h∆°...   \n",
       "..           ...    ...                                                ...   \n",
       "70  train_000070    0.0                                    \"ƒê·∫πp th√≠ch h·∫øt\"   \n",
       "71  train_000071    0.0       \"Kem d√πng ·ªïn m·ªói t·ªôi h∆°i t·ªëi do v·ªõi da m√¨nh\"   \n",
       "72  train_000072    0.0  \"ƒê√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn ch·∫•t l...   \n",
       "73  train_000073    0.0  \" Good value for money Good value for money Fa...   \n",
       "74  train_000074    0.0  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m t·ªët ƒê√≥ng g√≥i s·∫£n ph·∫©m r·∫•...   \n",
       "75  train_000075    0.0  \"Ch·ªã ch·ªß t∆∞ v·∫•n r·∫•t nhi·ªát t√¨nh üòÜüòÜ h√¨nh d√°ng v√†...   \n",
       "76  train_000076    1.0  \"SHOP ƒêƒÇNG ·∫¢NH M·ªòT ƒê·∫∞NG S·∫¢N PH·∫®M M·ªòT L·∫∫O .B√ÅN ...   \n",
       "77  train_000077    0.0  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi. Shop t∆∞ v·∫•n n...   \n",
       "78  train_000078    1.0        \" Poor delivery speed Poor product quality\"   \n",
       "79  train_000079    0.0  \"ƒê√∫ng l√† ti·ªÅn n√†o c·ªßa ƒë√≥ ko ngon m√πi k√¨ k√¨ ko ...   \n",
       "80  train_000080    0.0  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi. ƒê√≥ng g√≥i s·∫£n ...   \n",
       "81  train_000081    0.0          \"Gi√†y Ok l·∫Øm shop ship h√†ng nhanh n·ªØa ‚ù§Ô∏è\"   \n",
       "82  train_000082    0.0          \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi.r·∫•t  OK \"   \n",
       "83  train_000083    0.0  \"Gi√†y ch·∫Øc ch·∫Øn . ƒê·∫ø h∆°i b·∫©n t√≠ nh∆∞ng ch·∫Øc s·∫Ω ...   \n",
       "84  train_000084    0.0  \"H√†ng giao thi·∫øu k√™u g·ª≠i b√π cho m√† c·∫£ th√°ng ch...   \n",
       "85  train_000085    1.0  \"Shop h·∫øt h√†ng nh∆∞ng kh√¥ng b√°o. M√¨nh ƒë·∫∑t 1 c·∫∑p...   \n",
       "86  train_000086    0.0                  \"Giao ko ƒë√∫ng ƒê·ªì. T√¥i ƒë√£ b·ªã l·ª´a \"   \n",
       "87  train_000087    1.0            \"hk giong m·∫©u g√¨ c·∫£ da vay bi l·ªói nua¬†\"   \n",
       "88  train_000088    0.0  \"T∆∞∆°ng ƒë·ªëi h√†i l√≤ng gi√†y ch·∫Øc ch·∫Øn √™m nh∆∞ng ƒë∆∞...   \n",
       "89  train_000089    0.0  \"D√©p xinh. Ch·ªß shop rep tn r·∫•t nhi·ªát t√¨nh. Ch√∫...   \n",
       "90  train_000090    1.0  \"Mua 2ƒë c√≥ 2 l·ªó x·ªè d√¢y ib shop kh tr·∫£ l·ªùi gi√†y...   \n",
       "91  train_000091    0.0                        \"T·ªët tgian giao h√†ng nhanh\"   \n",
       "92  train_000092    0.0                            \"N√™n mua nha m·ªçi ng∆∞·ªùi\"   \n",
       "93  train_000093    1.0  \"Th·∫•y ghi ƒëc t·∫∑ng k√®m t√£ qu·∫ßn.ƒë·∫øn khi nh·∫≠n h√†n...   \n",
       "94  train_000094    1.0  \" Shop ph·ª•c v·ª• r·∫•t k√©m. ƒê√£ ph·∫£i nh·∫Øn tin h·ªèi c...   \n",
       "95  train_000095    0.0  \"Shop ph·ª•c v·ª• r·∫•t t·ªët. S·∫£n ph·∫©m ƒë√∫ng v·ªõi th√¥ng...   \n",
       "96  train_000096    1.0  \"ƒê·∫∑t combo m√† shop ch·ªâ giao b√°nh g·∫°o v√† s·ªët k ...   \n",
       "97  train_000097    0.0  \"Shop n√†y uy t√≠n n√® h√†ng ƒë√£ ƒë·∫πp m√† c√≤n ƒë·∫ßy ƒë·ªß ...   \n",
       "98  train_000098    0.0  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒê√≥ng g√≥i s·∫£n p...   \n",
       "99  train_000099    1.0  \"S·∫£n ph·∫©m k nh∆∞ qu·∫£ng c√°o l√† 9058 li√™n h·ªá theo...   \n",
       "\n",
       "                                       review_cleaned  num_words  \\\n",
       "0   dung ƒë∆∞·ª£c s·∫£n ph·∫©m tot cam onshop ƒë√≥ng_g√≥i s·∫£n...         21   \n",
       "1   ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi son m·ªãn ƒë√°nh m√†u...         19   \n",
       "2   ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi kh√¥ng h·ªôp kh√¥ng ...         19   \n",
       "3   h∆°i th·∫•t_v·ªçng ch√∫t k·ª≥_v·ªçng s√°ch hi_v·ªçng h·ªçc_t·∫≠...        114   \n",
       "4   mua √°o_gi√≥ m√†u h·ªìng t·ªët ƒë·ª£t giao √°o_gi√≥ ch·∫•t v...         26   \n",
       "5   ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi c·ª©ng_c√°p c·ªë_ƒë·ªãnh...         23   \n",
       "6   ƒëc h√†ng t·ªëi tr∆∞a mai ƒë√≥ng_g√≥i s·∫£n_ph·∫©m ƒë·∫πp c·ª≠a...         31   \n",
       "7   si√™u ph·∫©m c·∫•u_h√¨nh to√†n t·ª±a t·ª±a ko ƒë·ªôt_ph√° n√¢n...         48   \n",
       "8   h√†ng v·∫≠n chuy·ªÉn ch·∫•t_l∆∞·ª£ng t∆∞_v·∫•n nhi·ªát_t√¨nh v...         20   \n",
       "9           ƒë·ªìng_h·ªì ƒë·∫πp ƒë·ª©t d√¢y kh√¥ng ch·∫°y mua ve s·ª≠a         16   \n",
       "10     ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi y h√¨nh ch·ª•p ti·ªÅn         11   \n",
       "11           hjhj c·ª≠a h√†ng giao h√†ng ƒë·∫πp l·∫Øm b√©  m√¨nh         14   \n",
       "12                                           ƒë·∫πp ph t          4   \n",
       "13  ƒë√≥ng_g√≥i ƒë·∫πp ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m ch·∫•t_l∆∞·ª£ng s·∫£...         16   \n",
       "14                               sƒÉn ƒëc gi√° kh√¥ng v·ªùi          7   \n",
       "15                                       t·ªët h√†i_l√≤ng          4   \n",
       "16                            giao thi u c·ª≠a h√†ng t t          9   \n",
       "17                      ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi          9   \n",
       "18  gi√†y ƒë·∫πp l·∫Øm d√¢y h∆°i ng·∫Øn t√≠ ch·∫•t_l∆∞·ª£ng s·∫£n_ph...         16   \n",
       "19                            y  m√¨nh v·∫£i ƒë·∫πp m·∫´u ƒë·∫πp          7   \n",
       "20  ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi ƒë√≥ng_g√≥i s·∫£n_ph·∫©...         33   \n",
       "21   h√†i_l√≤ng s·∫£n_ph·∫©m l·∫Øm gi·∫∑t lan da nhoe m√†u gi·ªù t         16   \n",
       "22                   giao h√†ng m·∫∑c ƒë·∫πpc√°m ∆°n c·ª≠a h√†ng          7   \n",
       "23  ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi bao_b√¨ cute ph√¥_...         28   \n",
       "24  ƒë·ªìng_h·ªì ƒë·∫πp kim ch·∫°y ƒë·ª©ng nh·∫Øn_tin ko tr·∫£_l·ªùi ...         27   \n",
       "25  giao h√†ng si√™u ƒë√≥ng_g√≥i c·∫©n_th·∫≠n t∆∞_v·∫•n nhi·ªát_...         18   \n",
       "26              h∆°i b·∫•t_ti·ªán xu th e ƒëa ph·∫±n ∆∞a_th√≠ch         17   \n",
       "27              to√†n h√†ng trungkhi mua qu√™n ko coi kƒ©          8   \n",
       "28            ƒë√≥ng_g√≥i s·∫£n_ph·∫©m ƒë·∫πp c·ª≠a h√†ng t·∫∑ng qu√†         17   \n",
       "29  h√¥m_nay chi√™n th·ª≠ c√°_h·ªìi c√°_chi√™n chi√™n ki·ªÉu t...         33   \n",
       "..                                                ...        ...   \n",
       "70                                          ƒë·∫πp gi·ªù t          3   \n",
       "71                          kem ·ªïn m·ªói_t·ªôi h∆°i t·ªëi da         11   \n",
       "72          ƒë√≥ng_g√≥i s·∫£n_ph·∫©m ƒë·∫πp ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m         14   \n",
       "73  good value for money good value for money fast...         15   \n",
       "74  ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m ƒë√≥ng_g√≥i s·∫£n_ph·∫©m c·∫©n_th·∫≠n...         22   \n",
       "75  ch·ªß t∆∞_v·∫•n nhi·ªát_t√¨nh_h√¨nh_d√°ng ch·∫•t_l∆∞·ª£ng s·∫£n...         17   \n",
       "76  c·ª≠a h√†ng ƒëƒÉng ·∫£nh ƒë·∫±ng s·∫£n_ph·∫©m l·∫ªo b√°n_c√°p √∫p...         54   \n",
       "77  ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi c·ª≠a h√†ng t∆∞_v·∫•n ...         20   \n",
       "78           poor delivery speed poor product quality          7   \n",
       "79  ti·ªÅn ko ngon m√πi k√¨ k√¨ ko th∆°m l·∫°p_x∆∞·ªüng b√¨nh_...         22   \n",
       "80  ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi ƒë√≥ng_g√≥i s·∫£n_ph·∫©...         25   \n",
       "81              gi√†y t·ªët l·∫Øm c·ª≠a h√†ng v·∫≠n chuy·ªÉn h√†ng          9   \n",
       "82                  ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi t·ªët          9   \n",
       "83                    gi√†y ƒë h∆°i b·∫©n t√≠ lau giao h√†ng         17   \n",
       "84  h√†ng giao thi u k√™u g·ª≠i b√π g·ª≠i nh·∫Øn_tin ko tr·∫£...         18   \n",
       "85  c·ª≠a h√†ng gi·ªù t h√†ng b√°o c·∫∑p th g·ªüi ·ªëp k√®m ti·ªÅn...         55   \n",
       "86                                     giao ko ƒë·ªì l·ª´a          9   \n",
       "87                     hk giong m·∫©u da vay bi l·ªói nua         11   \n",
       "88  t∆∞∆°ng_ƒë·ªëi h√†i_l√≤ng gi√†y √™m ƒë∆∞·ªùng may th·ª´a t·ªët ...         24   \n",
       "89  d√©p xinh ch·ªß c·ª≠a h√†ng rep tn nhi·ªát_t√¨nh ch√∫c c...         16   \n",
       "90  mua ƒë l·ªó x·ªè d√¢y ib c·ª≠a h√†ng kh√¥ng tr·∫£_l·ªùi gi√†y...         21   \n",
       "91                                    tgian giao h√†ng          5   \n",
       "92                                            mua nha          5   \n",
       "93  ghi ƒëc t·∫∑ng k√®m t√£ qu·∫ßn ƒë n h√†ng kh√¥ng c·ª≠a h√†n...         18   \n",
       "94  c·ª≠a h√†ng ph·ª•c_v·ª• k√©m nh·∫Øn_tin c·∫©n_th·∫≠n kh√¥ng c...         24   \n",
       "95       c·ª≠a h√†ng ph·ª•c_v·ª• s·∫£n_ph·∫©m th√¥ng_tin ƒëƒÉng_t·∫£i         13   \n",
       "96  combo c·ª≠a h√†ng giao b√°nh g·∫°o s·ªët kh√¥ng ph√¥_mai...         45   \n",
       "97  c·ª≠a h√†ng uy_t√≠n n√® h√†ng ƒë·∫πp ƒë·∫ßy_ƒë·ªß h·ªôp pin t·∫∑n...         37   \n",
       "98  ch·∫•t_l∆∞·ª£ng s·∫£n_ph·∫©m tuy·ªát_v·ªùi ƒë√≥ng_g√≥i s·∫£n_ph·∫©...         25   \n",
       "99  s·∫£n_ph·∫©m kh√¥ng qu·∫£ng_c√°o li√™n_h·ªá ƒët c·ª≠a h√†ng v...         28   \n",
       "\n",
       "    num_unique_words  words_vs_unique  \n",
       "0                 19        90.476190  \n",
       "1                 19       100.000000  \n",
       "2                 15        78.947368  \n",
       "3                 92        80.701754  \n",
       "4                 24        92.307692  \n",
       "5                 22        95.652174  \n",
       "6                 29        93.548387  \n",
       "7                 44        91.666667  \n",
       "8                 19        95.000000  \n",
       "9                 14        87.500000  \n",
       "10                10        90.909091  \n",
       "11                14       100.000000  \n",
       "12                 4       100.000000  \n",
       "13                11        68.750000  \n",
       "14                 7       100.000000  \n",
       "15                 4       100.000000  \n",
       "16                 9       100.000000  \n",
       "17                 9       100.000000  \n",
       "18                16       100.000000  \n",
       "19                 7       100.000000  \n",
       "20                28        84.848485  \n",
       "21                16       100.000000  \n",
       "22                 7       100.000000  \n",
       "23                28       100.000000  \n",
       "24                25        92.592593  \n",
       "25                17        94.444444  \n",
       "26                17       100.000000  \n",
       "27                 8       100.000000  \n",
       "28                17       100.000000  \n",
       "29                28        84.848485  \n",
       "..               ...              ...  \n",
       "70                 3       100.000000  \n",
       "71                11       100.000000  \n",
       "72                12        85.714286  \n",
       "73                11        73.333333  \n",
       "74                19        86.363636  \n",
       "75                17       100.000000  \n",
       "76                47        87.037037  \n",
       "77                20       100.000000  \n",
       "78                 6        85.714286  \n",
       "79                20        90.909091  \n",
       "80                23        92.000000  \n",
       "81                 9       100.000000  \n",
       "82                 8        88.888889  \n",
       "83                15        88.235294  \n",
       "84                18       100.000000  \n",
       "85                40        72.727273  \n",
       "86                 9       100.000000  \n",
       "87                11       100.000000  \n",
       "88                24       100.000000  \n",
       "89                15        93.750000  \n",
       "90                21       100.000000  \n",
       "91                 5       100.000000  \n",
       "92                 5       100.000000  \n",
       "93                16        88.888889  \n",
       "94                21        87.500000  \n",
       "95                13       100.000000  \n",
       "96                39        86.666667  \n",
       "97                35        94.594595  \n",
       "98                21        84.000000  \n",
       "99                27        96.428571  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_data.csv', encoding = \"utf8\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[~df['label'].isnull()]\n",
    "test_df = df[df['label'].isnull()]\n",
    "\n",
    "train_comments = train_df['comment'].fillna(\"none\").values\n",
    "test_comments = test_df['comment'].fillna(\"none\").values\n",
    "\n",
    "y_train = train_df['label'].values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = './word2vec/wiki.vi.model.bin'\n",
    "#Load word2vec model\n",
    "if os.path.isfile(model):\n",
    "    print ('Loading word2vec model ...')\n",
    "if LooseVersion(gensim.__version__) >= LooseVersion(\"1.0.1\"):\n",
    "    from gensim.models import KeyedVectors\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(model, binary=True)\n",
    "else:\n",
    "    from gensim.models import Word2Vec\n",
    "    word2vec_model = Word2Vec.load_word2vec_format(model, binary=True)\n",
    "word2vec_model.wv.syn0.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from deepai_nlp.word_embedding.word2vec_gensim import BaseWord2Vec\n",
    "word2vec_model = BaseWord2Vec.load_model()\n",
    "word2vec_model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output = []\n",
    "try:\n",
    "    sim_list = word2vec_model.most_similar(\"th√≠ch\")\n",
    "    print(sim_list)\n",
    "    #output = word2vec_model.most_similar('u' + '\\\"' + 'A' + '\\\"', topn=5)\n",
    "\n",
    "    for wordsimilar in sim_list:\n",
    "        # output[wordsimilar[0]] = wordsimilar[1]\n",
    "        output.append(wordsimilar[0] + ' - '+ str(wordsimilar[1]))\n",
    "except:\n",
    "    print('except')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import emoji\n",
    "def extract_emojis(str):\n",
    "    return [c for c in str if c in emoji.UNICODE_EMOJI]\n",
    "emojis_vocab = []\n",
    "for r in train_data['review']:\n",
    "    emojis_vocab += extract_emojis(r)\n",
    "emojis_vocab = np.unique(np.asarray(emojis_vocab))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def getEmojiBowFeatures(reviews,vocab):\n",
    "    bow_emoji_features = []\n",
    "    for r in reviews:\n",
    "        emojis = extract_emojis(r)\n",
    "        bag_vector = np.zeros(len(vocab))\n",
    "        #print(emojis)\n",
    "        for e in emojis:\n",
    "            for i,emojii in enumerate(emojis_vocab):\n",
    "                if emojii == e: \n",
    "                    bag_vector[i] += 1\n",
    "        bow_emoji_features.append(bag_vector)\n",
    "    return np.asarray(bow_emoji_features)\n",
    "\n",
    "#getEmojiBowFeatures(\"üíöüíöüòëüíïüíï\", emojis_vocab)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_data.review, train_data.label, test_size=0.2,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "num_features = 400\n",
    "clean_train_reviews = []\n",
    "for review in train_data.review:\n",
    "    clean_train_reviews.append(review_wordlist(review, remove_stopwords=False))\n",
    "bow_train_features = getEmojiBowFeatures(train_data.review, emojis_vocab)\n",
    "bow_train_features = sc.fit_transform(bow_train_features)\n",
    "#print(bow_train_features.shape)\n",
    "length = np.asarray([len(r) for r in train_data.review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-d77d74f946c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvietnamese_chars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-ce41740a2f42>\u001b[0m in \u001b[0;36mclean_text\u001b[1;34m(review, char_reg, stopwords)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# 5. Remove stopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mreview_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"\\b%s\\b\"\u001b[0m\u001b[1;33m%\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreview_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 192\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"first argument must be string or compiled pattern\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\sre_compile.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    762\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\sre_parse.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(str, flags, pattern)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mSRE_FLAG_VERBOSE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mVerbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         \u001b[1;31m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\sre_parse.py\u001b[0m in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m         itemsappend(_parse(source, state, verbose, nested + 1,\n\u001b[1;32m--> 426\u001b[1;33m                            not nested and not items))\n\u001b[0m\u001b[0;32m    427\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msourcematch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"|\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\sre_parse.py\u001b[0m in \u001b[0;36m_parse\u001b[1;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[0;32m    833\u001b[0m     \u001b[1;31m# unpack non-capturing groups\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m         \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mav\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubpattern\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mop\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mSUBPATTERN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m             \u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_flags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdel_flags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mav\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "for review in train_data.review:\n",
    "    X_train.append(clean_text(review, char_reg = vietnamese_chars, stopwords = stopwords))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = tfidf.fit_transform(train_data.review)\n",
    "X_test = tfidf.transform(test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 16087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 16087\n",
      "Review 2000 of 16087\n",
      "Review 3000 of 16087\n",
      "Review 4000 of 16087\n",
      "Review 5000 of 16087\n",
      "Review 6000 of 16087\n",
      "Review 7000 of 16087\n",
      "Review 8000 of 16087\n",
      "Review 9000 of 16087\n",
      "Review 10000 of 16087\n",
      "Review 11000 of 16087\n",
      "Review 12000 of 16087\n",
      "Review 13000 of 16087\n",
      "Review 14000 of 16087\n",
      "Review 15000 of 16087\n",
      "Review 16000 of 16087\n"
     ]
    }
   ],
   "source": [
    "X_train = getAvgFeatureVecs(clean_train_reviews, word2vec_model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = length.reshape(16087,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train,bow_train_features),axis=1)\n",
    "#X_train = np.concatenate((X_train,length),axis=1)\n",
    "\n",
    "y_train = train_data.label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating average feature vactors for test set     \n",
    "clean_test_reviews = []\n",
    "for review in test_data.review:\n",
    "    clean_test_reviews.append(review_wordlist(review))\n",
    "bow_test_features = getEmojiBowFeatures(test_data.review, emojis_vocab)\n",
    "bow_test_features = sc.fit_transform(bow_test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 10981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 10981\n",
      "Review 2000 of 10981\n",
      "Review 3000 of 10981\n",
      "Review 4000 of 10981\n",
      "Review 5000 of 10981\n",
      "Review 6000 of 10981\n",
      "Review 7000 of 10981\n",
      "Review 8000 of 10981\n",
      "Review 9000 of 10981\n",
      "Review 10000 of 10981\n"
     ]
    }
   ],
   "source": [
    "X_test = getAvgFeatureVecs(clean_test_reviews, word2vec_model, num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-270-68ec1a145f3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbow_test_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "X_test = np.concatenate((X_test,bow_test_features),axis=1)\n",
    "X_test = np.concatenate((X_test,length),axis=1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import collections\n",
    "f = clean_train_reviews\n",
    "f = np.concatenate(clean_train_reviews).ravel()\n",
    "wordcount = {}\n",
    "for word in f:\n",
    "    if word not in wordcount:\n",
    "        wordcount[word] = 1\n",
    "    else:\n",
    "        wordcount[word] += 1\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Print most common word\n",
    "word_counter = collections.Counter(wordcount)\n",
    "#for word, count in word_counter.most_common(500):\n",
    " #   print(word, \": \", count)\n",
    "most_common = []\n",
    "\n",
    "for word, count in word_counter.most_common(200):\n",
    "    most_common.append(word)\n",
    "most_common\n",
    "#word_counter.most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16087, 226)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "XX_train, X_val, yy_train, y_val = train_test_split(X_train, y_train, test_size=0.2,\n",
    "    random_state=42)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "forest.fit(pd.DataFrame(XX_train).fillna(0), yy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8104412678682411"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = forest.predict(pd.DataFrame(X_val).fillna(0))\n",
    "accuracy_score(y_val, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8505282784338098"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "XX_train, X_val, yy_train, y_val = train_test_split(X_train, y_train, test_size=0.2,\n",
    "    random_state=42)\n",
    "sc = StandardScaler()\n",
    "#XX_train = sc.fit_transform(XX_train)\n",
    "#X_test = sc.transform(X_test)\n",
    "clf = svm.SVC(gamma='scale',verbose=True)\n",
    "clf.fit(pd.DataFrame(XX_train).fillna(0), yy_train)\n",
    "y_predict = clf.predict(pd.DataFrame(X_val).fillna(0))\n",
    "accuracy_score(y_val, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12869, 525)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "d_train = lgb.Dataset(df, label=y_train)\n",
    "params = {}\n",
    "params['learning_rate'] = 0.003\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'binary'\n",
    "params['metric'] = 'binary_logloss'\n",
    "params['sub_feature'] = 0.5\n",
    "params['num_leaves'] = 80\n",
    "params['min_data'] = 50\n",
    "params['max_depth'] = 20\n",
    "clf = lgb.train(params, d_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LightGBM. Train shape: (16087, 226), test shape: (10981, 350)\n",
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.448914\ttrain's f1: 0.858152\tvalid's binary_logloss: 0.502944\tvalid's f1: 0.756535\n",
      "[200]\ttrain's binary_logloss: 0.347329\ttrain's f1: 0.893712\tvalid's binary_logloss: 0.440639\tvalid's f1: 0.779528\n",
      "[300]\ttrain's binary_logloss: 0.284272\ttrain's f1: 0.919402\tvalid's binary_logloss: 0.411849\tvalid's f1: 0.790167\n",
      "[400]\ttrain's binary_logloss: 0.23836\ttrain's f1: 0.940111\tvalid's binary_logloss: 0.394463\tvalid's f1: 0.793017\n",
      "[500]\ttrain's binary_logloss: 0.202597\ttrain's f1: 0.95693\tvalid's binary_logloss: 0.38381\tvalid's f1: 0.796013\n",
      "[600]\ttrain's binary_logloss: 0.173818\ttrain's f1: 0.97022\tvalid's binary_logloss: 0.376507\tvalid's f1: 0.799858\n",
      "[700]\ttrain's binary_logloss: 0.150022\ttrain's f1: 0.980539\tvalid's binary_logloss: 0.371869\tvalid's f1: 0.80427\n",
      "[800]\ttrain's binary_logloss: 0.130127\ttrain's f1: 0.988078\tvalid's binary_logloss: 0.369203\tvalid's f1: 0.804826\n",
      "[900]\ttrain's binary_logloss: 0.113292\ttrain's f1: 0.992835\tvalid's binary_logloss: 0.367151\tvalid's f1: 0.807379\n",
      "[1000]\ttrain's binary_logloss: 0.0989237\ttrain's f1: 0.995587\tvalid's binary_logloss: 0.366586\tvalid's f1: 0.806532\n",
      "[1100]\ttrain's binary_logloss: 0.0867645\ttrain's f1: 0.99715\tvalid's binary_logloss: 0.366177\tvalid's f1: 0.808797\n",
      "Early stopping, best iteration is:\n",
      "[1044]\ttrain's binary_logloss: 0.0933486\ttrain's f1: 0.996231\tvalid's binary_logloss: 0.366307\tvalid's f1: 0.810082\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.451073\ttrain's f1: 0.852468\tvalid's binary_logloss: 0.498411\tvalid's f1: 0.75\n",
      "[200]\ttrain's binary_logloss: 0.349822\ttrain's f1: 0.888769\tvalid's binary_logloss: 0.432897\tvalid's f1: 0.775687\n",
      "[300]\ttrain's binary_logloss: 0.287065\ttrain's f1: 0.915044\tvalid's binary_logloss: 0.40097\tvalid's f1: 0.790079\n",
      "[400]\ttrain's binary_logloss: 0.241092\ttrain's f1: 0.93713\tvalid's binary_logloss: 0.382077\tvalid's f1: 0.802867\n",
      "[500]\ttrain's binary_logloss: 0.205406\ttrain's f1: 0.956616\tvalid's binary_logloss: 0.370453\tvalid's f1: 0.811449\n",
      "[600]\ttrain's binary_logloss: 0.176468\ttrain's f1: 0.968827\tvalid's binary_logloss: 0.362851\tvalid's f1: 0.814204\n",
      "[700]\ttrain's binary_logloss: 0.152583\ttrain's f1: 0.980267\tvalid's binary_logloss: 0.357899\tvalid's f1: 0.816356\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-292-8025c1cdefad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlgb_f1_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     )\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    214\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   1758\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0;32m   1759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1760\u001b[1;33m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[0;32m   1761\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1762\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(X_train.shape, X_test.shape))\n",
    "\n",
    "# Cross validation model\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=69)\n",
    "\n",
    "# Create arrays and dataframes to store results\n",
    "oof_preds = np.zeros(X_train.shape[0])\n",
    "sub_preds = np.zeros(X_test.shape[0])\n",
    "\n",
    "# k-fold\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "    print(\"Fold %s\" % (n_fold))\n",
    "    train_x, train_y = X_train[train_idx], y_train[train_idx]\n",
    "    valid_x, valid_y = X_train[valid_idx], y_train[valid_idx]\n",
    "\n",
    "    # set data structure\n",
    "    lgb_train = lgb.Dataset(train_x,\n",
    "                            label=train_y,\n",
    "                            free_raw_data=False)\n",
    "    lgb_test = lgb.Dataset(valid_x,\n",
    "                           label=valid_y,\n",
    "                           free_raw_data=False)\n",
    "\n",
    "    params = {\n",
    "        'objective' :'binary',\n",
    "        'learning_rate' : 0.01,\n",
    "        'num_leaves' : 76,\n",
    "        'feature_fraction': 0.64, \n",
    "        'bagging_fraction': 0.8, \n",
    "        'bagging_freq':1,\n",
    "        'boosting_type' : 'gbdt',\n",
    "    }\n",
    "\n",
    "    reg = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_train, lgb_test],\n",
    "        valid_names=['train', 'valid'],\n",
    "        num_boost_round=10000,\n",
    "        verbose_eval=100,\n",
    "        early_stopping_rounds=100,\n",
    "        feval=lgb_f1_score\n",
    "    )\n",
    "\n",
    "    oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=reg.best_iteration)\n",
    "    sub_preds += reg.predict(X_test, num_iteration=reg.best_iteration) / folds.n_splits\n",
    "\n",
    "    del reg, train_x, train_y, valid_x, valid_y\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "preds = (sub_preds > threshold).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'] = preds\n",
    "test_data[['id','label']].to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 10981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 10981\n",
      "Review 2000 of 10981\n",
      "Review 3000 of 10981\n",
      "Review 4000 of 10981\n",
      "Review 5000 of 10981\n",
      "Review 6000 of 10981\n",
      "Review 7000 of 10981\n",
      "Review 8000 of 10981\n",
      "Review 9000 of 10981\n",
      "Review 10000 of 10981\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-3bb7ae807644>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mrealTestDataVecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetAvgFeatureVecs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_real_test_reviews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2vec_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mbow_train_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetEmojiBowFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memojis_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mrealTestDataVecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrealTestDataVecs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbow_train_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "# Calculating average feature vactors for test set     \n",
    "clean_real_test_reviews = []\n",
    "for review in test_data['review']:\n",
    "    clean_real_test_reviews.append(review_wordlist(review))\n",
    "    \n",
    "realTestDataVecs = getAvgFeatureVecs(clean_real_test_reviews, word2vec_model, num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_train_features = getEmojiBowFeatures(test_data['review'], emojis_vocab)\n",
    "realTestDataVecs = np.concatenate((realTestDataVecs,bow_train_features),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = realTestDataVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = clf.predict(pd.DataFrame(realTestDataVecs).fillna(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'] = y_predict\n",
    "test_data[['id','label']].to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
