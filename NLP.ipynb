{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk.data\n",
    "import gensim\n",
    "from distutils.version import LooseVersion, StrictVersion\n",
    "import os\n",
    "import codecs\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "global word2vec_model\n",
    "from deepai_nlp.tokenization.crf_tokenizer import CrfTokenizer\n",
    "from deepai_nlp.tokenization.utils import preprocess_text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/github/ngxbac/aivivn_phanloaisacthaibinhluan/blob/master/baseline_lgbm_tfidf.ipynb#scrollTo=ojmynVfZtFRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSource(object):\n",
    "    def _load_raw_data(self,filename, is_train=True):\n",
    "        a = []\n",
    "        b = []\n",
    "        regex = 'train_'\n",
    "        if not is_train:\n",
    "            regex = 'test_'\n",
    "        with open(filename, 'r', encoding=\"utf8\") as file:\n",
    "            for line in file :\n",
    "                if regex in line:\n",
    "                    b.append(a)\n",
    "                    a = [line]\n",
    "                elif line!='\\n':\n",
    "                    a.append(line)       \n",
    "        b.append(a)      \n",
    "        return b[1:]\n",
    "    \n",
    "    def _create_row(self, sample, is_train=True):\n",
    "        d = {}\n",
    "        d['id'] = sample[0].replace('\\n','')\n",
    "        review = \"\"\n",
    "        if is_train:\n",
    "            for clause in sample[1:-1]:\n",
    "                review+= clause.replace('\\n','').strip()\n",
    "            d['label'] = int(sample[-1].replace('\\n',''))          \n",
    "        else:         \n",
    "            for clause in sample[1:]:\n",
    "                review+= clause.replace('\\n','').strip()\n",
    "        d['review'] = review\n",
    "        return d\n",
    "    \n",
    "    \n",
    "    def load_data(self, filename, is_train=True):\n",
    "        raw_data = self._load_raw_data(filename, is_train)\n",
    "        lst = []\n",
    "        for row in raw_data:\n",
    "            lst.append(self._create_row(row, is_train))\n",
    "        return lst"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "search = \"d you don't need a dog. but if you like dogs, you should think of getting one for your own. Or a cat?\"\n",
    "mapping =  {\"you\": \"aaaaa\", r\"\\bd\\b\": \"!!!\" }\n",
    "\n",
    "#search = [search.replace(key, value) for key, value in mapping.items()][0]\n",
    "\n",
    "for key, value in mapping.items():\n",
    "    search = re.sub(key,value,search)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mapping = {\n",
    "    \"ship\": \"vận chuyển\",\n",
    "    \"shop\": \"cửa hàng\",\n",
    "    \"sp\": \"sản phẩm\",\n",
    "    r\"\\bm\\b\": \" mình\",\n",
    "    \"mik\": \"mình\",\n",
    "    r\"\\bk\\b\": \"không\",\n",
    "    r\"\\bkh\\b\": \"không\",\n",
    "    r\"\\btl\\b\": \"trả lời\",\n",
    "    r\"\\br\\b\": \"rồi\",\n",
    "    \"fb\": \"mạng xã hội \", # facebook\n",
    "    \"face\": \"mạng xã hội\",\n",
    "    \"thanks\": \"cảm ơn\",\n",
    "    \"thank\": \"cảm ơn\",\n",
    "    \"tks\": \"cảm ơn\", \n",
    "    r\"\\bdc\\b\": \"được\",\n",
    "    r\"\\bok\\b\": \"tốt\",\n",
    "    r\"\\bdt\\b\": \"điện thoại\",\n",
    "    r\"\\bh\\b\": \"giờ\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dictionary changed size during iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-41ef5a3c4a54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;34m\"ad\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"cửa hàng\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m }\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m#Load stopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: dictionary changed size during iteration"
     ]
    }
   ],
   "source": [
    "\n",
    "mapping = {\n",
    "    \"ship\": \"vận chuyển\",\n",
    "    \"shop\": \"cửa hàng\",\n",
    "    \"sp\": \"sản phẩm\",\n",
    "    \"m\": \" mình\",\n",
    "    \"mik\": \"mình\",\n",
    "    \"k\": \"không\",\n",
    "    \"kh\": \"không\",\n",
    "    \"tl\": \"trả lời\",\n",
    "    \"r\": \"rồi\",\n",
    "    \"fb\": \"mạng xã hội \", # facebook\n",
    "    \"face\": \"mạng xã hội\",\n",
    "    \"thanks\": \"cảm ơn\",\n",
    "    \"thank\": \"cảm ơn\",\n",
    "    \"tks\": \"cảm ơn\", \n",
    "    \"dc\": \"được\",\n",
    "    \"ok\": \"tốt\",\n",
    "    \"dt\": \"điện thoại\",\n",
    "    \"h\": \"giờ\",\n",
    "    \"hsd\": \"hạn sử dụng\",\n",
    "    \"trc\": \"trước\",\n",
    "    \"oki\": \"tốt\",\n",
    "    \"ad\": \"cửa hàng\"\n",
    "}\n",
    "for i, m in enumerate(mapping):\n",
    "    mapping[i] = m.strip().replace(' ','_')\n",
    "#Load stopwords\n",
    "stopwords_file = 'vietnamese-stopwords.txt'\n",
    "stopwords = []\n",
    "with open(stopwords_file, 'r', encoding=\"utf8\") as file:\n",
    "    for line in file :\n",
    "        stopwords.append(line.replace('\\n','').strip().replace(' ','_'))\n",
    "tokenizer = CrfTokenizer()\n",
    "vietnamese_chars = \"[^a-zA-Z_ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚĂĐĨŨƠàáâãèéêìíòóôõùúăđĩũơƯĂẠẢẤẦẨẪẬẮẰẲẴẶẸẺẼỀỀỂưăạảấầẩẫậắằẳẵặẹẻẽềềểỄỆỈỊỌỎỐỒỔỖỘỚỜỞỠỢỤỦỨỪễệỉịọỏốồổỗộớờởỡợụủứừỬỮỰỲỴÝỶỸửữựỳỵỷỹ]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a_lô',\n",
       " 'a_ha',\n",
       " 'ai',\n",
       " 'ai_ai',\n",
       " 'ai_nấy',\n",
       " 'ai_đó',\n",
       " 'alô',\n",
       " 'amen',\n",
       " 'anh',\n",
       " 'anh_ấy',\n",
       " 'ba',\n",
       " 'ba_ba',\n",
       " 'ba_bản',\n",
       " 'ba_cùng',\n",
       " 'ba_họ',\n",
       " 'ba_ngày',\n",
       " 'ba_ngôi',\n",
       " 'ba_tăng',\n",
       " 'bao_giờ',\n",
       " 'bao_lâu',\n",
       " 'bao_nhiêu',\n",
       " 'bao_nả',\n",
       " 'bay_biến',\n",
       " 'biết',\n",
       " 'biết_bao',\n",
       " 'biết_bao_nhiêu',\n",
       " 'biết_chắc',\n",
       " 'biết_chừng_nào',\n",
       " 'biết_mình',\n",
       " 'biết_mấy',\n",
       " 'biết_thế',\n",
       " 'biết_trước',\n",
       " 'biết_việc',\n",
       " 'biết_đâu',\n",
       " 'biết_đâu_chừng',\n",
       " 'biết_đâu_đấy',\n",
       " 'biết_được',\n",
       " 'buổi',\n",
       " 'buổi_làm',\n",
       " 'buổi_mới',\n",
       " 'buổi_ngày',\n",
       " 'buổi_sớm',\n",
       " 'bà',\n",
       " 'bà_ấy',\n",
       " 'bài',\n",
       " 'bài_bác',\n",
       " 'bài_bỏ',\n",
       " 'bài_cái',\n",
       " 'bác',\n",
       " 'bán',\n",
       " 'bán_cấp',\n",
       " 'bán_dạ',\n",
       " 'bán_thế',\n",
       " 'bây_bẩy',\n",
       " 'bây_chừ',\n",
       " 'bây_giờ',\n",
       " 'bây_nhiêu',\n",
       " 'bèn',\n",
       " 'béng',\n",
       " 'bên',\n",
       " 'bên_bị',\n",
       " 'bên_có',\n",
       " 'bên_cạnh',\n",
       " 'bông',\n",
       " 'bước',\n",
       " 'bước_khỏi',\n",
       " 'bước_tới',\n",
       " 'bước_đi',\n",
       " 'bạn',\n",
       " 'bản',\n",
       " 'bản_bộ',\n",
       " 'bản_riêng',\n",
       " 'bản_thân',\n",
       " 'bản_ý',\n",
       " 'bất_chợt',\n",
       " 'bất_cứ',\n",
       " 'bất_giác',\n",
       " 'bất_kì',\n",
       " 'bất_kể',\n",
       " 'bất_kỳ',\n",
       " 'bất_luận',\n",
       " 'bất_ngờ',\n",
       " 'bất_nhược',\n",
       " 'bất_quá',\n",
       " 'bất_quá_chỉ',\n",
       " 'bất_thình_lình',\n",
       " 'bất_tử',\n",
       " 'bất_đồ',\n",
       " 'bấy',\n",
       " 'bấy_chầy',\n",
       " 'bấy_chừ',\n",
       " 'bấy_giờ',\n",
       " 'bấy_lâu',\n",
       " 'bấy_lâu_nay',\n",
       " 'bấy_nay',\n",
       " 'bấy_nhiêu',\n",
       " 'bập_bà_bập_bõm',\n",
       " 'bập_bõm',\n",
       " 'bắt_đầu',\n",
       " 'bắt_đầu_từ',\n",
       " 'bằng',\n",
       " 'bằng_cứ',\n",
       " 'bằng_không',\n",
       " 'bằng_người',\n",
       " 'bằng_nhau',\n",
       " 'bằng_như',\n",
       " 'bằng_nào',\n",
       " 'bằng_nấy',\n",
       " 'bằng_vào',\n",
       " 'bằng_được',\n",
       " 'bằng_ấy',\n",
       " 'bển',\n",
       " 'bệt',\n",
       " 'bị',\n",
       " 'bị_chú',\n",
       " 'bị_vì',\n",
       " 'bỏ',\n",
       " 'bỏ_bà',\n",
       " 'bỏ_cha',\n",
       " 'bỏ_cuộc',\n",
       " 'bỏ_không',\n",
       " 'bỏ_lại',\n",
       " 'bỏ_mình',\n",
       " 'bỏ_mất',\n",
       " 'bỏ_mẹ',\n",
       " 'bỏ_nhỏ',\n",
       " 'bỏ_quá',\n",
       " 'bỏ_ra',\n",
       " 'bỏ_riêng',\n",
       " 'bỏ_việc',\n",
       " 'bỏ_xa',\n",
       " 'bỗng',\n",
       " 'bỗng_chốc',\n",
       " 'bỗng_dưng',\n",
       " 'bỗng_không',\n",
       " 'bỗng_nhiên',\n",
       " 'bỗng_nhưng',\n",
       " 'bỗng_thấy',\n",
       " 'bỗng_đâu',\n",
       " 'bộ',\n",
       " 'bộ_thuộc',\n",
       " 'bộ_điều',\n",
       " 'bội_phần',\n",
       " 'bớ',\n",
       " 'bởi',\n",
       " 'bởi_ai',\n",
       " 'bởi_chưng',\n",
       " 'bởi_nhưng',\n",
       " 'bởi_sao',\n",
       " 'bởi_thế',\n",
       " 'bởi_thế_cho_nên',\n",
       " 'bởi_tại',\n",
       " 'bởi_vì',\n",
       " 'bởi_vậy',\n",
       " 'bởi_đâu',\n",
       " 'bức',\n",
       " 'cao',\n",
       " 'cao_lâu',\n",
       " 'cao_ráo',\n",
       " 'cao_răng',\n",
       " 'cao_sang',\n",
       " 'cao_số',\n",
       " 'cao_thấp',\n",
       " 'cao_thế',\n",
       " 'cao_xa',\n",
       " 'cha',\n",
       " 'cha_chả',\n",
       " 'chao_ôi',\n",
       " 'chia_sẻ',\n",
       " 'chiếc',\n",
       " 'cho',\n",
       " 'cho_biết',\n",
       " 'cho_chắc',\n",
       " 'cho_hay',\n",
       " 'cho_nhau',\n",
       " 'cho_nên',\n",
       " 'cho_rằng',\n",
       " 'cho_rồi',\n",
       " 'cho_thấy',\n",
       " 'cho_tin',\n",
       " 'cho_tới',\n",
       " 'cho_tới_khi',\n",
       " 'cho_về',\n",
       " 'cho_ăn',\n",
       " 'cho_đang',\n",
       " 'cho_được',\n",
       " 'cho_đến',\n",
       " 'cho_đến_khi',\n",
       " 'cho_đến_nỗi',\n",
       " 'choa',\n",
       " 'chu_cha',\n",
       " 'chui_cha',\n",
       " 'chung',\n",
       " 'chung_cho',\n",
       " 'chung_chung',\n",
       " 'chung_cuộc',\n",
       " 'chung_cục',\n",
       " 'chung_nhau',\n",
       " 'chung_qui',\n",
       " 'chung_quy',\n",
       " 'chung_quy_lại',\n",
       " 'chung_ái',\n",
       " 'chuyển',\n",
       " 'chuyển_tự',\n",
       " 'chuyển_đạt',\n",
       " 'chuyện',\n",
       " 'chuẩn_bị',\n",
       " 'chành_chạnh',\n",
       " 'chí_chết',\n",
       " 'chính',\n",
       " 'chính_bản',\n",
       " 'chính_giữa',\n",
       " 'chính_là',\n",
       " 'chính_thị',\n",
       " 'chính_điểm',\n",
       " 'chùn_chùn',\n",
       " 'chùn_chũn',\n",
       " 'chú',\n",
       " 'chú_dẫn',\n",
       " 'chú_khách',\n",
       " 'chú_mày',\n",
       " 'chú_mình',\n",
       " 'chúng',\n",
       " 'chúng_mình',\n",
       " 'chúng_ta',\n",
       " 'chúng_tôi',\n",
       " 'chúng_ông',\n",
       " 'chăn_chắn',\n",
       " 'chăng',\n",
       " 'chăng_chắc',\n",
       " 'chăng_nữa',\n",
       " 'chơi',\n",
       " 'chơi_họ',\n",
       " 'chưa',\n",
       " 'chưa_bao_giờ',\n",
       " 'chưa_chắc',\n",
       " 'chưa_có',\n",
       " 'chưa_cần',\n",
       " 'chưa_dùng',\n",
       " 'chưa_dễ',\n",
       " 'chưa_kể',\n",
       " 'chưa_tính',\n",
       " 'chưa_từng',\n",
       " 'chầm_chập',\n",
       " 'chậc',\n",
       " 'chắc',\n",
       " 'chắc_chắn',\n",
       " 'chắc_dạ',\n",
       " 'chắc_hẳn',\n",
       " 'chắc_lòng',\n",
       " 'chắc_người',\n",
       " 'chắc_vào',\n",
       " 'chắc_ăn',\n",
       " 'chẳng_lẽ',\n",
       " 'chẳng_những',\n",
       " 'chẳng_nữa',\n",
       " 'chẳng_phải',\n",
       " 'chết_nỗi',\n",
       " 'chết_thật',\n",
       " 'chết_tiệt',\n",
       " 'chỉ',\n",
       " 'chỉ_chính',\n",
       " 'chỉ_có',\n",
       " 'chỉ_là',\n",
       " 'chỉ_tên',\n",
       " 'chỉn',\n",
       " 'chị',\n",
       " 'chị_bộ',\n",
       " 'chị_ấy',\n",
       " 'chịu',\n",
       " 'chịu_chưa',\n",
       " 'chịu_lời',\n",
       " 'chịu_tốt',\n",
       " 'chịu_ăn',\n",
       " 'chọn',\n",
       " 'chọn_bên',\n",
       " 'chọn_ra',\n",
       " 'chốc_chốc',\n",
       " 'chớ',\n",
       " 'chớ_chi',\n",
       " 'chớ_gì',\n",
       " 'chớ_không',\n",
       " 'chớ_kể',\n",
       " 'chớ_như',\n",
       " 'chợt',\n",
       " 'chợt_nghe',\n",
       " 'chợt_nhìn',\n",
       " 'chủn',\n",
       " 'chứ',\n",
       " 'chứ_ai',\n",
       " 'chứ_còn',\n",
       " 'chứ_gì',\n",
       " 'chứ_không',\n",
       " 'chứ_không_phải',\n",
       " 'chứ_lại',\n",
       " 'chứ_lị',\n",
       " 'chứ_như',\n",
       " 'chứ_sao',\n",
       " 'coi_bộ',\n",
       " 'coi_mòi',\n",
       " 'con',\n",
       " 'con_con',\n",
       " 'con_dạ',\n",
       " 'con_nhà',\n",
       " 'con_tính',\n",
       " 'cu_cậu',\n",
       " 'cuối',\n",
       " 'cuối_cùng',\n",
       " 'cuối_điểm',\n",
       " 'cuốn',\n",
       " 'cuộc',\n",
       " 'càng',\n",
       " 'càng_càng',\n",
       " 'càng_hay',\n",
       " 'cá_nhân',\n",
       " 'các',\n",
       " 'các_cậu',\n",
       " 'cách',\n",
       " 'cách_bức',\n",
       " 'cách_không',\n",
       " 'cách_nhau',\n",
       " 'cách_đều',\n",
       " 'cái',\n",
       " 'cái_gì',\n",
       " 'cái_họ',\n",
       " 'cái_đã',\n",
       " 'cái_đó',\n",
       " 'cái_ấy',\n",
       " 'câu_hỏi',\n",
       " 'cây',\n",
       " 'cây_nước',\n",
       " 'còn',\n",
       " 'còn_như',\n",
       " 'còn_nữa',\n",
       " 'còn_thời_gian',\n",
       " 'còn_về',\n",
       " 'có',\n",
       " 'có_ai',\n",
       " 'có_chuyện',\n",
       " 'có_chăng',\n",
       " 'có_chăng_là',\n",
       " 'có_chứ',\n",
       " 'có_cơ',\n",
       " 'có_dễ',\n",
       " 'có_họ',\n",
       " 'có_khi',\n",
       " 'có_ngày',\n",
       " 'có_người',\n",
       " 'có_nhiều',\n",
       " 'có_nhà',\n",
       " 'có_phải',\n",
       " 'có_số',\n",
       " 'có_tháng',\n",
       " 'có_thế',\n",
       " 'có_thể',\n",
       " 'có_vẻ',\n",
       " 'có_ý',\n",
       " 'có_ăn',\n",
       " 'có_điều',\n",
       " 'có_điều_kiện',\n",
       " 'có_đáng',\n",
       " 'có_đâu',\n",
       " 'có_được',\n",
       " 'cóc_khô',\n",
       " 'cô',\n",
       " 'cô_mình',\n",
       " 'cô_quả',\n",
       " 'cô_tăng',\n",
       " 'cô_ấy',\n",
       " 'công_nhiên',\n",
       " 'cùng',\n",
       " 'cùng_chung',\n",
       " 'cùng_cực',\n",
       " 'cùng_nhau',\n",
       " 'cùng_tuổi',\n",
       " 'cùng_tột',\n",
       " 'cùng_với',\n",
       " 'cùng_ăn',\n",
       " 'căn',\n",
       " 'căn_cái',\n",
       " 'căn_cắt',\n",
       " 'căn_tính',\n",
       " 'cũng',\n",
       " 'cũng_như',\n",
       " 'cũng_nên',\n",
       " 'cũng_thế',\n",
       " 'cũng_vậy',\n",
       " 'cũng_vậy_thôi',\n",
       " 'cũng_được',\n",
       " 'cơ',\n",
       " 'cơ_chỉ',\n",
       " 'cơ_chừng',\n",
       " 'cơ_cùng',\n",
       " 'cơ_dẫn',\n",
       " 'cơ_hồ',\n",
       " 'cơ_hội',\n",
       " 'cơ_mà',\n",
       " 'cơn',\n",
       " 'cả',\n",
       " 'cả_nghe',\n",
       " 'cả_nghĩ',\n",
       " 'cả_ngày',\n",
       " 'cả_người',\n",
       " 'cả_nhà',\n",
       " 'cả_năm',\n",
       " 'cả_thảy',\n",
       " 'cả_thể',\n",
       " 'cả_tin',\n",
       " 'cả_ăn',\n",
       " 'cả_đến',\n",
       " 'cảm_thấy',\n",
       " 'cảm_ơn',\n",
       " 'cấp',\n",
       " 'cấp_số',\n",
       " 'cấp_trực_tiếp',\n",
       " 'cần',\n",
       " 'cần_cấp',\n",
       " 'cần_gì',\n",
       " 'cần_số',\n",
       " 'cật_lực',\n",
       " 'cật_sức',\n",
       " 'cậu',\n",
       " 'cổ_lai',\n",
       " 'cụ_thể',\n",
       " 'cụ_thể_là',\n",
       " 'cụ_thể_như',\n",
       " 'của',\n",
       " 'của_ngọt',\n",
       " 'của_tin',\n",
       " 'cứ',\n",
       " 'cứ_như',\n",
       " 'cứ_việc',\n",
       " 'cứ_điểm',\n",
       " 'cực_lực',\n",
       " 'do',\n",
       " 'do_vì',\n",
       " 'do_vậy',\n",
       " 'do_đó',\n",
       " 'duy',\n",
       " 'duy_chỉ',\n",
       " 'duy_có',\n",
       " 'dài',\n",
       " 'dài_lời',\n",
       " 'dài_ra',\n",
       " 'dành',\n",
       " 'dành_dành',\n",
       " 'dào',\n",
       " 'dì',\n",
       " 'dù',\n",
       " 'dù_cho',\n",
       " 'dù_dì',\n",
       " 'dù_gì',\n",
       " 'dù_rằng',\n",
       " 'dù_sao',\n",
       " 'dùng',\n",
       " 'dùng_cho',\n",
       " 'dùng_hết',\n",
       " 'dùng_làm',\n",
       " 'dùng_đến',\n",
       " 'dưới',\n",
       " 'dưới_nước',\n",
       " 'dạ',\n",
       " 'dạ_bán',\n",
       " 'dạ_con',\n",
       " 'dạ_dài',\n",
       " 'dạ_dạ',\n",
       " 'dạ_khách',\n",
       " 'dần_dà',\n",
       " 'dần_dần',\n",
       " 'dầu_sao',\n",
       " 'dẫn',\n",
       " 'dẫu',\n",
       " 'dẫu_mà',\n",
       " 'dẫu_rằng',\n",
       " 'dẫu_sao',\n",
       " 'dễ',\n",
       " 'dễ_dùng',\n",
       " 'dễ_gì',\n",
       " 'dễ_khiến',\n",
       " 'dễ_nghe',\n",
       " 'dễ_ngươi',\n",
       " 'dễ_như_chơi',\n",
       " 'dễ_sợ',\n",
       " 'dễ_sử_dụng',\n",
       " 'dễ_thường',\n",
       " 'dễ_thấy',\n",
       " 'dễ_ăn',\n",
       " 'dễ_đâu',\n",
       " 'dở_chừng',\n",
       " 'dữ',\n",
       " 'dữ_cách',\n",
       " 'em',\n",
       " 'em_em',\n",
       " 'giá_trị',\n",
       " 'giá_trị_thực_tế',\n",
       " 'giảm',\n",
       " 'giảm_chính',\n",
       " 'giảm_thấp',\n",
       " 'giảm_thế',\n",
       " 'giống',\n",
       " 'giống_người',\n",
       " 'giống_nhau',\n",
       " 'giống_như',\n",
       " 'giờ',\n",
       " 'giờ_lâu',\n",
       " 'giờ_này',\n",
       " 'giờ_đi',\n",
       " 'giờ_đây',\n",
       " 'giờ_đến',\n",
       " 'giữ',\n",
       " 'giữ_lấy',\n",
       " 'giữ_ý',\n",
       " 'giữa',\n",
       " 'giữa_lúc',\n",
       " 'gây',\n",
       " 'gây_cho',\n",
       " 'gây_giống',\n",
       " 'gây_ra',\n",
       " 'gây_thêm',\n",
       " 'gì',\n",
       " 'gì_gì',\n",
       " 'gì_đó',\n",
       " 'gần',\n",
       " 'gần_bên',\n",
       " 'gần_hết',\n",
       " 'gần_ngày',\n",
       " 'gần_như',\n",
       " 'gần_xa',\n",
       " 'gần_đây',\n",
       " 'gần_đến',\n",
       " 'gặp',\n",
       " 'gặp_khó_khăn',\n",
       " 'gặp_phải',\n",
       " 'gồm',\n",
       " 'hay',\n",
       " 'hay_biết',\n",
       " 'hay_hay',\n",
       " 'hay_không',\n",
       " 'hay_là',\n",
       " 'hay_làm',\n",
       " 'hay_nhỉ',\n",
       " 'hay_nói',\n",
       " 'hay_sao',\n",
       " 'hay_tin',\n",
       " 'hay_đâu',\n",
       " 'hiểu',\n",
       " 'hiện_nay',\n",
       " 'hiện_tại',\n",
       " 'hoàn_toàn',\n",
       " 'hoặc',\n",
       " 'hoặc_là',\n",
       " 'hãy',\n",
       " 'hãy_còn',\n",
       " 'hơn',\n",
       " 'hơn_cả',\n",
       " 'hơn_hết',\n",
       " 'hơn_là',\n",
       " 'hơn_nữa',\n",
       " 'hơn_trước',\n",
       " 'hầu_hết',\n",
       " 'hết',\n",
       " 'hết_chuyện',\n",
       " 'hết_cả',\n",
       " 'hết_của',\n",
       " 'hết_nói',\n",
       " 'hết_ráo',\n",
       " 'hết_rồi',\n",
       " 'hết_ý',\n",
       " 'họ',\n",
       " 'họ_gần',\n",
       " 'họ_xa',\n",
       " 'hỏi',\n",
       " 'hỏi_lại',\n",
       " 'hỏi_xem',\n",
       " 'hỏi_xin',\n",
       " 'hỗ_trợ',\n",
       " 'khi',\n",
       " 'khi_khác',\n",
       " 'khi_không',\n",
       " 'khi_nào',\n",
       " 'khi_nên',\n",
       " 'khi_trước',\n",
       " 'khiến',\n",
       " 'khoảng',\n",
       " 'khoảng_cách',\n",
       " 'khoảng_không',\n",
       " 'khá',\n",
       " 'khá_tốt',\n",
       " 'khác',\n",
       " 'khác_gì',\n",
       " 'khác_khác',\n",
       " 'khác_nhau',\n",
       " 'khác_nào',\n",
       " 'khác_thường',\n",
       " 'khác_xa',\n",
       " 'khách',\n",
       " 'khó',\n",
       " 'khó_biết',\n",
       " 'khó_chơi',\n",
       " 'khó_khăn',\n",
       " 'khó_làm',\n",
       " 'khó_mở',\n",
       " 'khó_nghe',\n",
       " 'khó_nghĩ',\n",
       " 'khó_nói',\n",
       " 'khó_thấy',\n",
       " 'khó_tránh',\n",
       " 'không',\n",
       " 'không_ai',\n",
       " 'không_bao_giờ',\n",
       " 'không_bao_lâu',\n",
       " 'không_biết',\n",
       " 'không_bán',\n",
       " 'không_chỉ',\n",
       " 'không_còn',\n",
       " 'không_có',\n",
       " 'không_có_gì',\n",
       " 'không_cùng',\n",
       " 'không_cần',\n",
       " 'không_cứ',\n",
       " 'không_dùng',\n",
       " 'không_gì',\n",
       " 'không_hay',\n",
       " 'không_khỏi',\n",
       " 'không_kể',\n",
       " 'không_ngoài',\n",
       " 'không_nhận',\n",
       " 'không_những',\n",
       " 'không_phải',\n",
       " 'không_phải_không',\n",
       " 'không_thể',\n",
       " 'không_tính',\n",
       " 'không_điều_kiện',\n",
       " 'không_được',\n",
       " 'không_đầy',\n",
       " 'không_để',\n",
       " 'khẳng_định',\n",
       " 'khỏi',\n",
       " 'khỏi_nói',\n",
       " 'kể',\n",
       " 'kể_cả',\n",
       " 'kể_như',\n",
       " 'kể_tới',\n",
       " 'kể_từ',\n",
       " 'liên_quan',\n",
       " 'loại',\n",
       " 'loại_từ',\n",
       " 'luôn',\n",
       " 'luôn_cả',\n",
       " 'luôn_luôn',\n",
       " 'luôn_tay',\n",
       " 'là',\n",
       " 'là_cùng',\n",
       " 'là_là',\n",
       " 'là_nhiều',\n",
       " 'là_phải',\n",
       " 'là_thế_nào',\n",
       " 'là_vì',\n",
       " 'là_ít',\n",
       " 'làm',\n",
       " 'làm_bằng',\n",
       " 'làm_cho',\n",
       " 'làm_dần_dần',\n",
       " 'làm_gì',\n",
       " 'làm_lòng',\n",
       " 'làm_lại',\n",
       " 'làm_lấy',\n",
       " 'làm_mất',\n",
       " 'làm_ngay',\n",
       " 'làm_như',\n",
       " 'làm_nên',\n",
       " 'làm_ra',\n",
       " 'làm_riêng',\n",
       " 'làm_sao',\n",
       " 'làm_theo',\n",
       " 'làm_thế_nào',\n",
       " 'làm_tin',\n",
       " 'làm_tôi',\n",
       " 'làm_tăng',\n",
       " 'làm_tại',\n",
       " 'làm_tắp_lự',\n",
       " 'làm_vì',\n",
       " 'làm_đúng',\n",
       " 'làm_được',\n",
       " 'lâu',\n",
       " 'lâu_các',\n",
       " 'lâu_lâu',\n",
       " 'lâu_nay',\n",
       " 'lâu_ngày',\n",
       " 'lên',\n",
       " 'lên_cao',\n",
       " 'lên_cơn',\n",
       " 'lên_mạnh',\n",
       " 'lên_ngôi',\n",
       " 'lên_nước',\n",
       " 'lên_số',\n",
       " 'lên_xuống',\n",
       " 'lên_đến',\n",
       " 'lòng',\n",
       " 'lòng_không',\n",
       " 'lúc',\n",
       " 'lúc_khác',\n",
       " 'lúc_lâu',\n",
       " 'lúc_nào',\n",
       " 'lúc_này',\n",
       " 'lúc_sáng',\n",
       " 'lúc_trước',\n",
       " 'lúc_đi',\n",
       " 'lúc_đó',\n",
       " 'lúc_đến',\n",
       " 'lúc_ấy',\n",
       " 'lý_do',\n",
       " 'lượng',\n",
       " 'lượng_cả',\n",
       " 'lượng_số',\n",
       " 'lượng_từ',\n",
       " 'lại',\n",
       " 'lại_bộ',\n",
       " 'lại_cái',\n",
       " 'lại_còn',\n",
       " 'lại_giống',\n",
       " 'lại_làm',\n",
       " 'lại_người',\n",
       " 'lại_nói',\n",
       " 'lại_nữa',\n",
       " 'lại_quả',\n",
       " 'lại_thôi',\n",
       " 'lại_ăn',\n",
       " 'lại_đây',\n",
       " 'lấy',\n",
       " 'lấy_có',\n",
       " 'lấy_cả',\n",
       " 'lấy_giống',\n",
       " 'lấy_làm',\n",
       " 'lấy_lý_do',\n",
       " 'lấy_lại',\n",
       " 'lấy_ra',\n",
       " 'lấy_ráo',\n",
       " 'lấy_sau',\n",
       " 'lấy_số',\n",
       " 'lấy_thêm',\n",
       " 'lấy_thế',\n",
       " 'lấy_vào',\n",
       " 'lấy_xuống',\n",
       " 'lấy_được',\n",
       " 'lấy_để',\n",
       " 'lần',\n",
       " 'lần_khác',\n",
       " 'lần_lần',\n",
       " 'lần_nào',\n",
       " 'lần_này',\n",
       " 'lần_sang',\n",
       " 'lần_sau',\n",
       " 'lần_theo',\n",
       " 'lần_trước',\n",
       " 'lần_tìm',\n",
       " 'lớn',\n",
       " 'lớn_lên',\n",
       " 'lớn_nhỏ',\n",
       " 'lời',\n",
       " 'lời_chú',\n",
       " 'lời_nói',\n",
       " 'mang',\n",
       " 'mang_lại',\n",
       " 'mang_mang',\n",
       " 'mang_nặng',\n",
       " 'mang_về',\n",
       " 'muốn',\n",
       " 'mà',\n",
       " 'mà_cả',\n",
       " 'mà_không',\n",
       " 'mà_lại',\n",
       " 'mà_thôi',\n",
       " 'mà_vẫn',\n",
       " 'mình',\n",
       " 'mạnh',\n",
       " 'mất',\n",
       " 'mất_còn',\n",
       " 'mọi',\n",
       " 'mọi_giờ',\n",
       " 'mọi_khi',\n",
       " 'mọi_lúc',\n",
       " 'mọi_người',\n",
       " 'mọi_nơi',\n",
       " 'mọi_sự',\n",
       " 'mọi_thứ',\n",
       " 'mọi_việc',\n",
       " 'mối',\n",
       " 'mỗi',\n",
       " 'mỗi_lúc',\n",
       " 'mỗi_lần',\n",
       " 'mỗi_một',\n",
       " 'mỗi_ngày',\n",
       " 'mỗi_người',\n",
       " 'một',\n",
       " 'một_cách',\n",
       " 'một_cơn',\n",
       " 'một_khi',\n",
       " 'một_lúc',\n",
       " 'một_số',\n",
       " 'một_vài',\n",
       " 'một_ít',\n",
       " 'mới',\n",
       " 'mới_hay',\n",
       " 'mới_rồi',\n",
       " 'mới_đây',\n",
       " 'mở',\n",
       " 'mở_mang',\n",
       " 'mở_nước',\n",
       " 'mở_ra',\n",
       " 'mợ',\n",
       " 'mức',\n",
       " 'nay',\n",
       " 'ngay',\n",
       " 'ngay_bây_giờ',\n",
       " 'ngay_cả',\n",
       " 'ngay_khi',\n",
       " 'ngay_khi_đến',\n",
       " 'ngay_lúc',\n",
       " 'ngay_lúc_này',\n",
       " 'ngay_lập_tức',\n",
       " 'ngay_thật',\n",
       " 'ngay_tức_khắc',\n",
       " 'ngay_tức_thì',\n",
       " 'ngay_từ',\n",
       " 'nghe',\n",
       " 'nghe_chừng',\n",
       " 'nghe_hiểu',\n",
       " 'nghe_không',\n",
       " 'nghe_lại',\n",
       " 'nghe_nhìn',\n",
       " 'nghe_như',\n",
       " 'nghe_nói',\n",
       " 'nghe_ra',\n",
       " 'nghe_rõ',\n",
       " 'nghe_thấy',\n",
       " 'nghe_tin',\n",
       " 'nghe_trực_tiếp',\n",
       " 'nghe_đâu',\n",
       " 'nghe_đâu_như',\n",
       " 'nghe_được',\n",
       " 'nghen',\n",
       " 'nghiễm_nhiên',\n",
       " 'nghĩ',\n",
       " 'nghĩ_lại',\n",
       " 'nghĩ_ra',\n",
       " 'nghĩ_tới',\n",
       " 'nghĩ_xa',\n",
       " 'nghĩ_đến',\n",
       " 'nghỉm',\n",
       " 'ngoài',\n",
       " 'ngoài_này',\n",
       " 'ngoài_ra',\n",
       " 'ngoài_xa',\n",
       " 'ngoải',\n",
       " 'nguồn',\n",
       " 'ngày',\n",
       " 'ngày_càng',\n",
       " 'ngày_cấp',\n",
       " 'ngày_giờ',\n",
       " 'ngày_ngày',\n",
       " 'ngày_nào',\n",
       " 'ngày_này',\n",
       " 'ngày_nọ',\n",
       " 'ngày_qua',\n",
       " 'ngày_rày',\n",
       " 'ngày_tháng',\n",
       " 'ngày_xưa',\n",
       " 'ngày_xửa',\n",
       " 'ngày_đến',\n",
       " 'ngày_ấy',\n",
       " 'ngôi',\n",
       " 'ngôi_nhà',\n",
       " 'ngôi_thứ',\n",
       " 'ngõ_hầu',\n",
       " 'ngăn_ngắt',\n",
       " 'ngươi',\n",
       " 'người',\n",
       " 'người_hỏi',\n",
       " 'người_khác',\n",
       " 'người_khách',\n",
       " 'người_mình',\n",
       " 'người_nghe',\n",
       " 'người_người',\n",
       " 'người_nhận',\n",
       " 'ngọn',\n",
       " 'ngọn_nguồn',\n",
       " 'ngọt',\n",
       " 'ngồi',\n",
       " 'ngồi_bệt',\n",
       " 'ngồi_không',\n",
       " 'ngồi_sau',\n",
       " 'ngồi_trệt',\n",
       " 'ngộ_nhỡ',\n",
       " 'nhanh',\n",
       " 'nhanh_lên',\n",
       " 'nhanh_tay',\n",
       " 'nhau',\n",
       " 'nhiên_hậu',\n",
       " 'nhiều',\n",
       " 'nhiều_ít',\n",
       " 'nhiệt_liệt',\n",
       " 'nhung_nhăng',\n",
       " 'nhà',\n",
       " 'nhà_chung',\n",
       " 'nhà_khó',\n",
       " 'nhà_làm',\n",
       " 'nhà_ngoài',\n",
       " 'nhà_ngươi',\n",
       " 'nhà_tôi',\n",
       " 'nhà_việc',\n",
       " 'nhân_dịp',\n",
       " 'nhân_tiện',\n",
       " 'nhé',\n",
       " 'nhìn',\n",
       " 'nhìn_chung',\n",
       " 'nhìn_lại',\n",
       " 'nhìn_nhận',\n",
       " 'nhìn_theo',\n",
       " 'nhìn_thấy',\n",
       " 'nhìn_xuống',\n",
       " 'nhóm',\n",
       " 'nhón_nhén',\n",
       " 'như',\n",
       " 'như_ai',\n",
       " 'như_chơi',\n",
       " 'như_không',\n",
       " 'như_là',\n",
       " 'như_nhau',\n",
       " 'như_quả',\n",
       " 'như_sau',\n",
       " 'như_thường',\n",
       " 'như_thế',\n",
       " 'như_thế_nào',\n",
       " 'như_thể',\n",
       " 'như_trên',\n",
       " 'như_trước',\n",
       " 'như_tuồng',\n",
       " 'như_vậy',\n",
       " 'như_ý',\n",
       " 'nhưng',\n",
       " 'nhưng_mà',\n",
       " 'nhược_bằng',\n",
       " 'nhất',\n",
       " 'nhất_loạt',\n",
       " 'nhất_luật',\n",
       " 'nhất_là',\n",
       " 'nhất_mực',\n",
       " 'nhất_nhất',\n",
       " 'nhất_quyết',\n",
       " 'nhất_sinh',\n",
       " 'nhất_thiết',\n",
       " 'nhất_thì',\n",
       " 'nhất_tâm',\n",
       " 'nhất_tề',\n",
       " 'nhất_đán',\n",
       " 'nhất_định',\n",
       " 'nhận',\n",
       " 'nhận_biết',\n",
       " 'nhận_họ',\n",
       " 'nhận_làm',\n",
       " 'nhận_nhau',\n",
       " 'nhận_ra',\n",
       " 'nhận_thấy',\n",
       " 'nhận_việc',\n",
       " 'nhận_được',\n",
       " 'nhằm',\n",
       " 'nhằm_khi',\n",
       " 'nhằm_lúc',\n",
       " 'nhằm_vào',\n",
       " 'nhằm_để',\n",
       " 'nhỉ',\n",
       " 'nhỏ',\n",
       " 'nhỏ_người',\n",
       " 'nhớ',\n",
       " 'nhớ_bập_bõm',\n",
       " 'nhớ_lại',\n",
       " 'nhớ_lấy',\n",
       " 'nhớ_ra',\n",
       " 'nhờ',\n",
       " 'nhờ_chuyển',\n",
       " 'nhờ_có',\n",
       " 'nhờ_nhờ',\n",
       " 'nhờ_đó',\n",
       " 'nhỡ_ra',\n",
       " 'những',\n",
       " 'những_ai',\n",
       " 'những_khi',\n",
       " 'những_là',\n",
       " 'những_lúc',\n",
       " 'những_muốn',\n",
       " 'những_như',\n",
       " 'nào',\n",
       " 'nào_cũng',\n",
       " 'nào_hay',\n",
       " 'nào_là',\n",
       " 'nào_phải',\n",
       " 'nào_đâu',\n",
       " 'nào_đó',\n",
       " 'này',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vietnamese_chars = \"[^a-z0-9A-Z_ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚĂĐĨŨƠàáâãèéêìíòóôõùúăđĩũơƯĂẠẢẤẦẨẪẬẮẰẲẴẶẸẺẼỀỀỂưăạảấầẩẫậắằẳẵặẹẻẽềềểỄỆỈỊỌỎỐỒỔỖỘỚỜỞỠỢỤỦỨỪễệỉịọỏốồổỗộớờởỡợụủứừỬỮỰỲỴÝỶỸửữựỳỵỷỹ]\"\n",
    "def review_wordlist(review, remove_stopwords= False):\n",
    "    review_text = str(review)\n",
    "    # 2. Removing non-letter.\n",
    "    review_text = re.sub(vietnamese_chars,\" \",review_text)\n",
    "    # 3. Converting to lower case and splitting\n",
    "    words = review_text.lower().split()\n",
    "    # 4. Optionally remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords)     \n",
    "        words = [w for w in words if not w in stops]\n",
    "    for i,w in enumerate(words):\n",
    "        if w in mapping:\n",
    "            words[i]= mapping[w]\n",
    "    \n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokenizer = CrfTokenizer()\n",
    "vietnamese_chars = \"[^a-zA-Z_ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚĂĐĨŨƠàáâãèéêìíòóôõùúăđĩũơƯĂẠẢẤẦẨẪẬẮẰẲẴẶẸẺẼỀỀỂưăạảấầẩẫậắằẳẵặẹẻẽềềểỄỆỈỊỌỎỐỒỔỖỘỚỜỞỠỢỤỦỨỪễệỉịọỏốồổỗộớờởỡợụủứừỬỮỰỲỴÝỶỸửữựỳỵỷỹ]\"\n",
    "def review_wordlist(review, remove_stopwords= False):\n",
    "    review_text = str(review)\n",
    "    # 2. Removing non-letter.\n",
    "    review_text = re.sub(vietnamese_chars,\" \",review_text)\n",
    "    # 3. Converting to lower case and splitting\n",
    "    review_text = review_text.lower()\n",
    "    for key, value in mapping.items():\n",
    "        review_text = re.sub(key,value,review_text)\n",
    "    words = tokenizer.tokenize(review_text)\n",
    "    # 4. Optionally remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords)     \n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CrfTokenizer()\n",
    "def clean_text(review, char_reg, stopwords, mapping):\n",
    "    review_text = str(review)\n",
    "    # 2. Removing non-letter.\n",
    "    review_text = re.sub(char_reg,\" \",review_text)\n",
    "    # 3. Converting to lower case and splitting\n",
    "    review_text = review_text.lower()\n",
    "    # 4. Subtitute words\n",
    "    #words = review_text.split()\n",
    "    words = tokenizer.tokenize(review_text)\n",
    "    print(words)\n",
    "    # 5. Remove stopwords\n",
    "    #words = [w.replace('_',' ') for w in words]\n",
    "    stops = set(stopwords)  \n",
    "    #print(stops)\n",
    "    words = [w for w in words if not w in stops]\n",
    "    print(words)\n",
    "    for i,w in enumerate(words):\n",
    "        if w in mapping:\n",
    "            words[i]= mapping[w]\n",
    "    review_text = ' '.join(words)\n",
    "    return(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from file C:\\ProgramData\\Anaconda3\\lib\\site-packages\\deepai_nlp-0.0.1-py3.7.egg\\deepai_nlp\\models/pretrained_tokenizer.crfsuite\n",
      "['chưa', 'dùng', 'thử', 'nên', 'chưa', 'bi', 't']\n",
      "['thử', 'bi', 't']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'thử bi t'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"chưa dùng thử nên chưa biết\",char_reg = vietnamese_chars, mapping =mapping, stopwords = stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from file C:\\ProgramData\\Anaconda3\\lib\\site-packages\\deepai_nlp-0.0.1-py3.7.egg\\deepai_nlp\\models/pretrained_tokenizer.crfsuite\n"
     ]
    }
   ],
   "source": [
    "ds = DataSource()\n",
    "train_data = pd.DataFrame(ds.load_data('dataset/train.crash'))\n",
    "test_data = pd.DataFrame(ds.load_data('dataset/test.crash', is_train=False))\n",
    "#train_data['review'] = train_data['review'].fillna(\"none\")\n",
    "#test_data['review'] = test_data['review'].fillna(\"none\")\n",
    "df = pd.concat([train_data,test_data], axis=0, sort=False)\n",
    "df['review_cleaned'] = df['review'].apply(lambda s: clean_text(s,char_reg = vietnamese_chars, mapping =mapping, stopwords = stopwords))\n",
    "df['num_words'] = df['review'].apply(lambda s: len(s.split()))\n",
    "df['num_unique_words'] = df['review'].apply(lambda s: len(set(w for w in s.split())))\n",
    "df['words_vs_unique'] = df['num_unique_words'] / df['num_words'] * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "      <th>review_cleaned</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Dung dc sp tot cam onshop Đóng gói sản phẩm r...</td>\n",
       "      <td>dung được sản phẩm tot cam onshop đóng_gói sản...</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>90.476190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Chất lượng sản phẩm tuyệt vời . Son mịn nhưn...</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời son mịn đánh màu...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Chất lượng sản phẩm tuyệt vời nhưng k có hộp...</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời không hộp không ...</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>78.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_000003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\":(( Mình hơi thất vọng 1 chút vì mình đã kỳ v...</td>\n",
       "      <td>hơi thất_vọng chút kỳ_vọng sách hi_vọng học_tậ...</td>\n",
       "      <td>114</td>\n",
       "      <td>92</td>\n",
       "      <td>80.701754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_000004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Lần trước mình mua áo gió màu hồng rất ok mà ...</td>\n",
       "      <td>mua áo_gió màu hồng tốt đợt giao áo_gió chất v...</td>\n",
       "      <td>26</td>\n",
       "      <td>24</td>\n",
       "      <td>92.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train_000005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Chất lượng sản phẩm tuyệt vời có điều không ...</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời cứng_cáp cố_định...</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>95.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train_000006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Đã nhận đc hàng rất nhanh mới đặt buổi tối mà...</td>\n",
       "      <td>đc hàng tối trưa mai đóng_gói sản_phẩm đẹp cửa...</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>93.548387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train_000007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Các siêu phẩm thấy cấu hình toàn tựa tựa nhau...</td>\n",
       "      <td>siêu phẩm cấu_hình toàn tựa tựa ko đột_phá nân...</td>\n",
       "      <td>48</td>\n",
       "      <td>44</td>\n",
       "      <td>91.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train_000008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Hàng ship nhanh  chất lượng tốt  tư vấn nhiệt...</td>\n",
       "      <td>hàng vận chuyển chất_lượng tư_vấn nhiệt_tình v...</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>95.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train_000009</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Đồng hồ đẹp nhưng 1 cái đứt dây  1 cái k chạy...</td>\n",
       "      <td>đồng_hồ đẹp đứt dây không chạy mua ve sửa</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>87.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>train_000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Chất lượng sản phẩm tuyệt vời.y hình chụp.đá...</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời y hình chụp tiền</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>90.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>train_000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Hjhj shop giao hàng nhanh quá. Đẹp lắm ạ bé n...</td>\n",
       "      <td>hjhj cửa hàng giao hàng đẹp lắm bé  mình</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>train_000012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"\"nhìn đẹp phết nhỉ..\"\"</td>\n",
       "      <td>đẹp ph t</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>train_000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Đóng gói rất đẹp. Chất lượng sản phẩm rất tốt...</td>\n",
       "      <td>đóng_gói đẹp chất_lượng sản_phẩm chất_lượng sả...</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>68.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>train_000014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Săn đc với giá 11k. Toẹt vời\"</td>\n",
       "      <td>săn đc giá không vời</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>train_000015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"OK rất hài lòng\"</td>\n",
       "      <td>tốt hài_lòng</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>train_000016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Giao thiếu mình cái này rồi shop ơi T^T\"</td>\n",
       "      <td>giao thi u cửa hàng t t</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>train_000017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Chất lượng sản phẩm tuyệt vời tôi rất thích\"</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>train_000018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Giày đẹp lắm có điều dây hơi ngắn tí ạ  Chất ...</td>\n",
       "      <td>giày đẹp lắm dây hơi ngắn tí chất_lượng sản_ph...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>train_000019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Yếm vải đẹp nhưng ít mẫu đẹp\"</td>\n",
       "      <td>y  mình vải đẹp mẫu đẹp</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>train_000020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Chất lượng sản phẩm tuyệt vời Đóng gói sản p...</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời đóng_gói sản_phẩ...</td>\n",
       "      <td>33</td>\n",
       "      <td>28</td>\n",
       "      <td>84.848485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>train_000021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"không hài lòng sản phẩm cho lắm. giặt lan đầu...</td>\n",
       "      <td>hài_lòng sản_phẩm lắm giặt lan da nhoe màu giờ t</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>train_000022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Giao hàng nhanh, mặc đẹpCám ơn shop\"</td>\n",
       "      <td>giao hàng mặc đẹpcám ơn cửa hàng</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>train_000023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Chất lượng sản phẩm tuyệt vời bao bì cute ph...</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời bao_bì cute phô_...</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>train_000024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Đồng hồ thì đẹp thật. Nhưng tại sao kim lúc c...</td>\n",
       "      <td>đồng_hồ đẹp kim chạy đứng nhắn_tin ko trả_lời ...</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>92.592593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>train_000025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Giao hàng siêu nhanh.Đóng gói cẩn thận và tư ...</td>\n",
       "      <td>giao hàng siêu đóng_gói cẩn_thận tư_vấn nhiệt_...</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>94.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>train_000026</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"\"Cũng hơi bất tiện xu thế này e rằng đa phằn ...</td>\n",
       "      <td>hơi bất_tiện xu th e đa phằn ưa_thích</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>train_000027</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Toàn hàng trungkhi mua quên ko coi kĩ\"</td>\n",
       "      <td>toàn hàng trungkhi mua quên ko coi kĩ</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>train_000028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Đóng gói sản phẩm rất đẹp và chắc chắn. Được...</td>\n",
       "      <td>đóng_gói sản_phẩm đẹp cửa hàng tặng quà</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>train_000029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Hôm nay chiên thử cá hồi, cá chiên ăn ngọt hơ...</td>\n",
       "      <td>hôm_nay chiên thử cá_hồi cá_chiên chiên kiểu t...</td>\n",
       "      <td>33</td>\n",
       "      <td>28</td>\n",
       "      <td>84.848485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>train_000070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Đẹp thích hết\"</td>\n",
       "      <td>đẹp giờ t</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>train_000071</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Kem dùng ổn mỗi tội hơi tối do với da mình\"</td>\n",
       "      <td>kem ổn mỗi_tội hơi tối da</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>train_000072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Đóng gói sản phẩm rất đẹp và chắc chắn chất l...</td>\n",
       "      <td>đóng_gói sản_phẩm đẹp chất_lượng sản_phẩm</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>85.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>train_000073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Good value for money Good value for money Fa...</td>\n",
       "      <td>good value for money good value for money fast...</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>73.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>train_000074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Chất lượng sản phẩm tốt Đóng gói sản phẩm rấ...</td>\n",
       "      <td>chất_lượng sản_phẩm đóng_gói sản_phẩm cẩn_thận...</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>86.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>train_000075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Chị chủ tư vấn rất nhiệt tình 😆😆 hình dáng và...</td>\n",
       "      <td>chủ tư_vấn nhiệt_tình_hình_dáng chất_lượng sản...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>train_000076</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"SHOP ĐĂNG ẢNH MỘT ĐẰNG SẢN PHẨM MỘT LẺO .BÁN ...</td>\n",
       "      <td>cửa hàng đăng ảnh đằng sản_phẩm lẻo bán_cáp úp...</td>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>87.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>train_000077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Chất lượng sản phẩm tuyệt vời. Shop tư vấn n...</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời cửa hàng tư_vấn ...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>train_000078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\" Poor delivery speed Poor product quality\"</td>\n",
       "      <td>poor delivery speed poor product quality</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>85.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>train_000079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Đúng là tiền nào của đó ko ngon mùi kì kì ko ...</td>\n",
       "      <td>tiền ko ngon mùi kì kì ko thơm lạp_xưởng bình_...</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>90.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>train_000080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Chất lượng sản phẩm tuyệt vời. Đóng gói sản ...</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời đóng_gói sản_phẩ...</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>92.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>train_000081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Giày Ok lắm shop ship hàng nhanh nữa ❤️\"</td>\n",
       "      <td>giày tốt lắm cửa hàng vận chuyển hàng</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>train_000082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Chất lượng sản phẩm tuyệt vời.rất  OK \"</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời tốt</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>88.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>train_000083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Giày chắc chắn . Đế hơi bẩn tí nhưng chắc sẽ ...</td>\n",
       "      <td>giày đ hơi bẩn tí lau giao hàng</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>88.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>train_000084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Hàng giao thiếu kêu gửi bù cho mà cả tháng ch...</td>\n",
       "      <td>hàng giao thi u kêu gửi bù gửi nhắn_tin ko trả...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>train_000085</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Shop hết hàng nhưng không báo. Mình đặt 1 cặp...</td>\n",
       "      <td>cửa hàng giờ t hàng báo cặp th gởi ốp kèm tiền...</td>\n",
       "      <td>55</td>\n",
       "      <td>40</td>\n",
       "      <td>72.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>train_000086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Giao ko đúng Đồ. Tôi đã bị lừa \"</td>\n",
       "      <td>giao ko đồ lừa</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>train_000087</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"hk giong mẩu gì cả da vay bi lỗi nua \"</td>\n",
       "      <td>hk giong mẩu da vay bi lỗi nua</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>train_000088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Tương đối hài lòng giày chắc chắn êm nhưng đư...</td>\n",
       "      <td>tương_đối hài_lòng giày êm đường may thừa tốt ...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>train_000089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Dép xinh. Chủ shop rep tn rất nhiệt tình. Chú...</td>\n",
       "      <td>dép xinh chủ cửa hàng rep tn nhiệt_tình chúc c...</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>93.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>train_000090</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Mua 2đ có 2 lỗ xỏ dây ib shop kh trả lời giày...</td>\n",
       "      <td>mua đ lỗ xỏ dây ib cửa hàng không trả_lời giày...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>train_000091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Tốt tgian giao hàng nhanh\"</td>\n",
       "      <td>tgian giao hàng</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>train_000092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Nên mua nha mọi người\"</td>\n",
       "      <td>mua nha</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>train_000093</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Thấy ghi đc tặng kèm tã quần.đến khi nhận hàn...</td>\n",
       "      <td>ghi đc tặng kèm tã quần đ n hàng không cửa hàn...</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>88.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>train_000094</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\" Shop phục vụ rất kém. Đã phải nhắn tin hỏi c...</td>\n",
       "      <td>cửa hàng phục_vụ kém nhắn_tin cẩn_thận không c...</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>87.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>train_000095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Shop phục vụ rất tốt. Sản phẩm đúng với thông...</td>\n",
       "      <td>cửa hàng phục_vụ sản_phẩm thông_tin đăng_tải</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>train_000096</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Đặt combo mà shop chỉ giao bánh gạo và sốt k ...</td>\n",
       "      <td>combo cửa hàng giao bánh gạo sốt không phô_mai...</td>\n",
       "      <td>45</td>\n",
       "      <td>39</td>\n",
       "      <td>86.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>train_000097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Shop này uy tín nè hàng đã đẹp mà còn đầy đủ ...</td>\n",
       "      <td>cửa hàng uy_tín nè hàng đẹp đầy_đủ hộp pin tặn...</td>\n",
       "      <td>37</td>\n",
       "      <td>35</td>\n",
       "      <td>94.594595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>train_000098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Chất lượng sản phẩm tuyệt vời Đóng gói sản p...</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời đóng_gói sản_phẩ...</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>train_000099</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Sản phẩm k như quảng cáo là 9058 liên hệ theo...</td>\n",
       "      <td>sản_phẩm không quảng_cáo liên_hệ đt cửa hàng v...</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>96.428571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  label                                             review  \\\n",
       "0   train_000000    0.0  \"Dung dc sp tot cam onshop Đóng gói sản phẩm r...   \n",
       "1   train_000001    0.0  \" Chất lượng sản phẩm tuyệt vời . Son mịn nhưn...   \n",
       "2   train_000002    0.0  \" Chất lượng sản phẩm tuyệt vời nhưng k có hộp...   \n",
       "3   train_000003    1.0  \":(( Mình hơi thất vọng 1 chút vì mình đã kỳ v...   \n",
       "4   train_000004    1.0  \"Lần trước mình mua áo gió màu hồng rất ok mà ...   \n",
       "5   train_000005    0.0  \" Chất lượng sản phẩm tuyệt vời có điều không ...   \n",
       "6   train_000006    0.0  \"Đã nhận đc hàng rất nhanh mới đặt buổi tối mà...   \n",
       "7   train_000007    1.0  \"Các siêu phẩm thấy cấu hình toàn tựa tựa nhau...   \n",
       "8   train_000008    0.0  \"Hàng ship nhanh  chất lượng tốt  tư vấn nhiệt...   \n",
       "9   train_000009    1.0  \"Đồng hồ đẹp nhưng 1 cái đứt dây  1 cái k chạy...   \n",
       "10  train_000010    0.0  \" Chất lượng sản phẩm tuyệt vời.y hình chụp.đá...   \n",
       "11  train_000011    0.0  \"Hjhj shop giao hàng nhanh quá. Đẹp lắm ạ bé n...   \n",
       "12  train_000012    0.0                            \"\"nhìn đẹp phết nhỉ..\"\"   \n",
       "13  train_000013    0.0  \"Đóng gói rất đẹp. Chất lượng sản phẩm rất tốt...   \n",
       "14  train_000014    0.0                     \"Săn đc với giá 11k. Toẹt vời\"   \n",
       "15  train_000015    0.0                                  \"OK rất hài lòng\"   \n",
       "16  train_000016    1.0          \"Giao thiếu mình cái này rồi shop ơi T^T\"   \n",
       "17  train_000017    0.0      \"Chất lượng sản phẩm tuyệt vời tôi rất thích\"   \n",
       "18  train_000018    0.0  \"Giày đẹp lắm có điều dây hơi ngắn tí ạ  Chất ...   \n",
       "19  train_000019    0.0                     \"Yếm vải đẹp nhưng ít mẫu đẹp\"   \n",
       "20  train_000020    0.0  \" Chất lượng sản phẩm tuyệt vời Đóng gói sản p...   \n",
       "21  train_000021    1.0  \"không hài lòng sản phẩm cho lắm. giặt lan đầu...   \n",
       "22  train_000022    0.0              \"Giao hàng nhanh, mặc đẹpCám ơn shop\"   \n",
       "23  train_000023    0.0  \" Chất lượng sản phẩm tuyệt vời bao bì cute ph...   \n",
       "24  train_000024    1.0  \"Đồng hồ thì đẹp thật. Nhưng tại sao kim lúc c...   \n",
       "25  train_000025    0.0  \"Giao hàng siêu nhanh.Đóng gói cẩn thận và tư ...   \n",
       "26  train_000026    1.0  \"\"Cũng hơi bất tiện xu thế này e rằng đa phằn ...   \n",
       "27  train_000027    1.0            \"Toàn hàng trungkhi mua quên ko coi kĩ\"   \n",
       "28  train_000028    0.0  \" Đóng gói sản phẩm rất đẹp và chắc chắn. Được...   \n",
       "29  train_000029    0.0  \"Hôm nay chiên thử cá hồi, cá chiên ăn ngọt hơ...   \n",
       "..           ...    ...                                                ...   \n",
       "70  train_000070    0.0                                    \"Đẹp thích hết\"   \n",
       "71  train_000071    0.0       \"Kem dùng ổn mỗi tội hơi tối do với da mình\"   \n",
       "72  train_000072    0.0  \"Đóng gói sản phẩm rất đẹp và chắc chắn chất l...   \n",
       "73  train_000073    0.0  \" Good value for money Good value for money Fa...   \n",
       "74  train_000074    0.0  \" Chất lượng sản phẩm tốt Đóng gói sản phẩm rấ...   \n",
       "75  train_000075    0.0  \"Chị chủ tư vấn rất nhiệt tình 😆😆 hình dáng và...   \n",
       "76  train_000076    1.0  \"SHOP ĐĂNG ẢNH MỘT ĐẰNG SẢN PHẨM MỘT LẺO .BÁN ...   \n",
       "77  train_000077    0.0  \" Chất lượng sản phẩm tuyệt vời. Shop tư vấn n...   \n",
       "78  train_000078    1.0        \" Poor delivery speed Poor product quality\"   \n",
       "79  train_000079    0.0  \"Đúng là tiền nào của đó ko ngon mùi kì kì ko ...   \n",
       "80  train_000080    0.0  \" Chất lượng sản phẩm tuyệt vời. Đóng gói sản ...   \n",
       "81  train_000081    0.0          \"Giày Ok lắm shop ship hàng nhanh nữa ❤️\"   \n",
       "82  train_000082    0.0          \" Chất lượng sản phẩm tuyệt vời.rất  OK \"   \n",
       "83  train_000083    0.0  \"Giày chắc chắn . Đế hơi bẩn tí nhưng chắc sẽ ...   \n",
       "84  train_000084    0.0  \"Hàng giao thiếu kêu gửi bù cho mà cả tháng ch...   \n",
       "85  train_000085    1.0  \"Shop hết hàng nhưng không báo. Mình đặt 1 cặp...   \n",
       "86  train_000086    0.0                  \"Giao ko đúng Đồ. Tôi đã bị lừa \"   \n",
       "87  train_000087    1.0            \"hk giong mẩu gì cả da vay bi lỗi nua \"   \n",
       "88  train_000088    0.0  \"Tương đối hài lòng giày chắc chắn êm nhưng đư...   \n",
       "89  train_000089    0.0  \"Dép xinh. Chủ shop rep tn rất nhiệt tình. Chú...   \n",
       "90  train_000090    1.0  \"Mua 2đ có 2 lỗ xỏ dây ib shop kh trả lời giày...   \n",
       "91  train_000091    0.0                        \"Tốt tgian giao hàng nhanh\"   \n",
       "92  train_000092    0.0                            \"Nên mua nha mọi người\"   \n",
       "93  train_000093    1.0  \"Thấy ghi đc tặng kèm tã quần.đến khi nhận hàn...   \n",
       "94  train_000094    1.0  \" Shop phục vụ rất kém. Đã phải nhắn tin hỏi c...   \n",
       "95  train_000095    0.0  \"Shop phục vụ rất tốt. Sản phẩm đúng với thông...   \n",
       "96  train_000096    1.0  \"Đặt combo mà shop chỉ giao bánh gạo và sốt k ...   \n",
       "97  train_000097    0.0  \"Shop này uy tín nè hàng đã đẹp mà còn đầy đủ ...   \n",
       "98  train_000098    0.0  \" Chất lượng sản phẩm tuyệt vời Đóng gói sản p...   \n",
       "99  train_000099    1.0  \"Sản phẩm k như quảng cáo là 9058 liên hệ theo...   \n",
       "\n",
       "                                       review_cleaned  num_words  \\\n",
       "0   dung được sản phẩm tot cam onshop đóng_gói sản...         21   \n",
       "1   chất_lượng sản_phẩm tuyệt_vời son mịn đánh màu...         19   \n",
       "2   chất_lượng sản_phẩm tuyệt_vời không hộp không ...         19   \n",
       "3   hơi thất_vọng chút kỳ_vọng sách hi_vọng học_tậ...        114   \n",
       "4   mua áo_gió màu hồng tốt đợt giao áo_gió chất v...         26   \n",
       "5   chất_lượng sản_phẩm tuyệt_vời cứng_cáp cố_định...         23   \n",
       "6   đc hàng tối trưa mai đóng_gói sản_phẩm đẹp cửa...         31   \n",
       "7   siêu phẩm cấu_hình toàn tựa tựa ko đột_phá nân...         48   \n",
       "8   hàng vận chuyển chất_lượng tư_vấn nhiệt_tình v...         20   \n",
       "9           đồng_hồ đẹp đứt dây không chạy mua ve sửa         16   \n",
       "10     chất_lượng sản_phẩm tuyệt_vời y hình chụp tiền         11   \n",
       "11           hjhj cửa hàng giao hàng đẹp lắm bé  mình         14   \n",
       "12                                           đẹp ph t          4   \n",
       "13  đóng_gói đẹp chất_lượng sản_phẩm chất_lượng sả...         16   \n",
       "14                               săn đc giá không vời          7   \n",
       "15                                       tốt hài_lòng          4   \n",
       "16                            giao thi u cửa hàng t t          9   \n",
       "17                      chất_lượng sản_phẩm tuyệt_vời          9   \n",
       "18  giày đẹp lắm dây hơi ngắn tí chất_lượng sản_ph...         16   \n",
       "19                            y  mình vải đẹp mẫu đẹp          7   \n",
       "20  chất_lượng sản_phẩm tuyệt_vời đóng_gói sản_phẩ...         33   \n",
       "21   hài_lòng sản_phẩm lắm giặt lan da nhoe màu giờ t         16   \n",
       "22                   giao hàng mặc đẹpcám ơn cửa hàng          7   \n",
       "23  chất_lượng sản_phẩm tuyệt_vời bao_bì cute phô_...         28   \n",
       "24  đồng_hồ đẹp kim chạy đứng nhắn_tin ko trả_lời ...         27   \n",
       "25  giao hàng siêu đóng_gói cẩn_thận tư_vấn nhiệt_...         18   \n",
       "26              hơi bất_tiện xu th e đa phằn ưa_thích         17   \n",
       "27              toàn hàng trungkhi mua quên ko coi kĩ          8   \n",
       "28            đóng_gói sản_phẩm đẹp cửa hàng tặng quà         17   \n",
       "29  hôm_nay chiên thử cá_hồi cá_chiên chiên kiểu t...         33   \n",
       "..                                                ...        ...   \n",
       "70                                          đẹp giờ t          3   \n",
       "71                          kem ổn mỗi_tội hơi tối da         11   \n",
       "72          đóng_gói sản_phẩm đẹp chất_lượng sản_phẩm         14   \n",
       "73  good value for money good value for money fast...         15   \n",
       "74  chất_lượng sản_phẩm đóng_gói sản_phẩm cẩn_thận...         22   \n",
       "75  chủ tư_vấn nhiệt_tình_hình_dáng chất_lượng sản...         17   \n",
       "76  cửa hàng đăng ảnh đằng sản_phẩm lẻo bán_cáp úp...         54   \n",
       "77  chất_lượng sản_phẩm tuyệt_vời cửa hàng tư_vấn ...         20   \n",
       "78           poor delivery speed poor product quality          7   \n",
       "79  tiền ko ngon mùi kì kì ko thơm lạp_xưởng bình_...         22   \n",
       "80  chất_lượng sản_phẩm tuyệt_vời đóng_gói sản_phẩ...         25   \n",
       "81              giày tốt lắm cửa hàng vận chuyển hàng          9   \n",
       "82                  chất_lượng sản_phẩm tuyệt_vời tốt          9   \n",
       "83                    giày đ hơi bẩn tí lau giao hàng         17   \n",
       "84  hàng giao thi u kêu gửi bù gửi nhắn_tin ko trả...         18   \n",
       "85  cửa hàng giờ t hàng báo cặp th gởi ốp kèm tiền...         55   \n",
       "86                                     giao ko đồ lừa          9   \n",
       "87                     hk giong mẩu da vay bi lỗi nua         11   \n",
       "88  tương_đối hài_lòng giày êm đường may thừa tốt ...         24   \n",
       "89  dép xinh chủ cửa hàng rep tn nhiệt_tình chúc c...         16   \n",
       "90  mua đ lỗ xỏ dây ib cửa hàng không trả_lời giày...         21   \n",
       "91                                    tgian giao hàng          5   \n",
       "92                                            mua nha          5   \n",
       "93  ghi đc tặng kèm tã quần đ n hàng không cửa hàn...         18   \n",
       "94  cửa hàng phục_vụ kém nhắn_tin cẩn_thận không c...         24   \n",
       "95       cửa hàng phục_vụ sản_phẩm thông_tin đăng_tải         13   \n",
       "96  combo cửa hàng giao bánh gạo sốt không phô_mai...         45   \n",
       "97  cửa hàng uy_tín nè hàng đẹp đầy_đủ hộp pin tặn...         37   \n",
       "98  chất_lượng sản_phẩm tuyệt_vời đóng_gói sản_phẩ...         25   \n",
       "99  sản_phẩm không quảng_cáo liên_hệ đt cửa hàng v...         28   \n",
       "\n",
       "    num_unique_words  words_vs_unique  \n",
       "0                 19        90.476190  \n",
       "1                 19       100.000000  \n",
       "2                 15        78.947368  \n",
       "3                 92        80.701754  \n",
       "4                 24        92.307692  \n",
       "5                 22        95.652174  \n",
       "6                 29        93.548387  \n",
       "7                 44        91.666667  \n",
       "8                 19        95.000000  \n",
       "9                 14        87.500000  \n",
       "10                10        90.909091  \n",
       "11                14       100.000000  \n",
       "12                 4       100.000000  \n",
       "13                11        68.750000  \n",
       "14                 7       100.000000  \n",
       "15                 4       100.000000  \n",
       "16                 9       100.000000  \n",
       "17                 9       100.000000  \n",
       "18                16       100.000000  \n",
       "19                 7       100.000000  \n",
       "20                28        84.848485  \n",
       "21                16       100.000000  \n",
       "22                 7       100.000000  \n",
       "23                28       100.000000  \n",
       "24                25        92.592593  \n",
       "25                17        94.444444  \n",
       "26                17       100.000000  \n",
       "27                 8       100.000000  \n",
       "28                17       100.000000  \n",
       "29                28        84.848485  \n",
       "..               ...              ...  \n",
       "70                 3       100.000000  \n",
       "71                11       100.000000  \n",
       "72                12        85.714286  \n",
       "73                11        73.333333  \n",
       "74                19        86.363636  \n",
       "75                17       100.000000  \n",
       "76                47        87.037037  \n",
       "77                20       100.000000  \n",
       "78                 6        85.714286  \n",
       "79                20        90.909091  \n",
       "80                23        92.000000  \n",
       "81                 9       100.000000  \n",
       "82                 8        88.888889  \n",
       "83                15        88.235294  \n",
       "84                18       100.000000  \n",
       "85                40        72.727273  \n",
       "86                 9       100.000000  \n",
       "87                11       100.000000  \n",
       "88                24       100.000000  \n",
       "89                15        93.750000  \n",
       "90                21       100.000000  \n",
       "91                 5       100.000000  \n",
       "92                 5       100.000000  \n",
       "93                16        88.888889  \n",
       "94                21        87.500000  \n",
       "95                13       100.000000  \n",
       "96                39        86.666667  \n",
       "97                35        94.594595  \n",
       "98                21        84.000000  \n",
       "99                27        96.428571  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_data.csv', encoding = \"utf8\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[~df['label'].isnull()]\n",
    "test_df = df[df['label'].isnull()]\n",
    "\n",
    "train_comments = train_df['comment'].fillna(\"none\").values\n",
    "test_comments = test_df['comment'].fillna(\"none\").values\n",
    "\n",
    "y_train = train_df['label'].values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = './word2vec/wiki.vi.model.bin'\n",
    "#Load word2vec model\n",
    "if os.path.isfile(model):\n",
    "    print ('Loading word2vec model ...')\n",
    "if LooseVersion(gensim.__version__) >= LooseVersion(\"1.0.1\"):\n",
    "    from gensim.models import KeyedVectors\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(model, binary=True)\n",
    "else:\n",
    "    from gensim.models import Word2Vec\n",
    "    word2vec_model = Word2Vec.load_word2vec_format(model, binary=True)\n",
    "word2vec_model.wv.syn0.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from deepai_nlp.word_embedding.word2vec_gensim import BaseWord2Vec\n",
    "word2vec_model = BaseWord2Vec.load_model()\n",
    "word2vec_model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output = []\n",
    "try:\n",
    "    sim_list = word2vec_model.most_similar(\"thích\")\n",
    "    print(sim_list)\n",
    "    #output = word2vec_model.most_similar('u' + '\\\"' + 'A' + '\\\"', topn=5)\n",
    "\n",
    "    for wordsimilar in sim_list:\n",
    "        # output[wordsimilar[0]] = wordsimilar[1]\n",
    "        output.append(wordsimilar[0] + ' - '+ str(wordsimilar[1]))\n",
    "except:\n",
    "    print('except')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import emoji\n",
    "def extract_emojis(str):\n",
    "    return [c for c in str if c in emoji.UNICODE_EMOJI]\n",
    "emojis_vocab = []\n",
    "for r in train_data['review']:\n",
    "    emojis_vocab += extract_emojis(r)\n",
    "emojis_vocab = np.unique(np.asarray(emojis_vocab))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def getEmojiBowFeatures(reviews,vocab):\n",
    "    bow_emoji_features = []\n",
    "    for r in reviews:\n",
    "        emojis = extract_emojis(r)\n",
    "        bag_vector = np.zeros(len(vocab))\n",
    "        #print(emojis)\n",
    "        for e in emojis:\n",
    "            for i,emojii in enumerate(emojis_vocab):\n",
    "                if emojii == e: \n",
    "                    bag_vector[i] += 1\n",
    "        bow_emoji_features.append(bag_vector)\n",
    "    return np.asarray(bow_emoji_features)\n",
    "\n",
    "#getEmojiBowFeatures(\"💚💚😑💕💕\", emojis_vocab)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_data.review, train_data.label, test_size=0.2,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "num_features = 400\n",
    "clean_train_reviews = []\n",
    "for review in train_data.review:\n",
    "    clean_train_reviews.append(review_wordlist(review, remove_stopwords=False))\n",
    "bow_train_features = getEmojiBowFeatures(train_data.review, emojis_vocab)\n",
    "bow_train_features = sc.fit_transform(bow_train_features)\n",
    "#print(bow_train_features.shape)\n",
    "length = np.asarray([len(r) for r in train_data.review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-d77d74f946c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvietnamese_chars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-ce41740a2f42>\u001b[0m in \u001b[0;36mclean_text\u001b[1;34m(review, char_reg, stopwords)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# 5. Remove stopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mreview_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"\\b%s\\b\"\u001b[0m\u001b[1;33m%\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreview_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 192\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"first argument must be string or compiled pattern\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\sre_compile.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    762\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\sre_parse.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(str, flags, pattern)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mSRE_FLAG_VERBOSE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mVerbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         \u001b[1;31m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\sre_parse.py\u001b[0m in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m         itemsappend(_parse(source, state, verbose, nested + 1,\n\u001b[1;32m--> 426\u001b[1;33m                            not nested and not items))\n\u001b[0m\u001b[0;32m    427\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msourcematch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"|\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\sre_parse.py\u001b[0m in \u001b[0;36m_parse\u001b[1;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[0;32m    833\u001b[0m     \u001b[1;31m# unpack non-capturing groups\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m         \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mav\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubpattern\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mop\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mSUBPATTERN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m             \u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_flags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdel_flags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mav\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "for review in train_data.review:\n",
    "    X_train.append(clean_text(review, char_reg = vietnamese_chars, stopwords = stopwords))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = tfidf.fit_transform(train_data.review)\n",
    "X_test = tfidf.transform(test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 16087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 16087\n",
      "Review 2000 of 16087\n",
      "Review 3000 of 16087\n",
      "Review 4000 of 16087\n",
      "Review 5000 of 16087\n",
      "Review 6000 of 16087\n",
      "Review 7000 of 16087\n",
      "Review 8000 of 16087\n",
      "Review 9000 of 16087\n",
      "Review 10000 of 16087\n",
      "Review 11000 of 16087\n",
      "Review 12000 of 16087\n",
      "Review 13000 of 16087\n",
      "Review 14000 of 16087\n",
      "Review 15000 of 16087\n",
      "Review 16000 of 16087\n"
     ]
    }
   ],
   "source": [
    "X_train = getAvgFeatureVecs(clean_train_reviews, word2vec_model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = length.reshape(16087,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train,bow_train_features),axis=1)\n",
    "#X_train = np.concatenate((X_train,length),axis=1)\n",
    "\n",
    "y_train = train_data.label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating average feature vactors for test set     \n",
    "clean_test_reviews = []\n",
    "for review in test_data.review:\n",
    "    clean_test_reviews.append(review_wordlist(review))\n",
    "bow_test_features = getEmojiBowFeatures(test_data.review, emojis_vocab)\n",
    "bow_test_features = sc.fit_transform(bow_test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 10981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 10981\n",
      "Review 2000 of 10981\n",
      "Review 3000 of 10981\n",
      "Review 4000 of 10981\n",
      "Review 5000 of 10981\n",
      "Review 6000 of 10981\n",
      "Review 7000 of 10981\n",
      "Review 8000 of 10981\n",
      "Review 9000 of 10981\n",
      "Review 10000 of 10981\n"
     ]
    }
   ],
   "source": [
    "X_test = getAvgFeatureVecs(clean_test_reviews, word2vec_model, num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-270-68ec1a145f3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbow_test_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "X_test = np.concatenate((X_test,bow_test_features),axis=1)\n",
    "X_test = np.concatenate((X_test,length),axis=1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import collections\n",
    "f = clean_train_reviews\n",
    "f = np.concatenate(clean_train_reviews).ravel()\n",
    "wordcount = {}\n",
    "for word in f:\n",
    "    if word not in wordcount:\n",
    "        wordcount[word] = 1\n",
    "    else:\n",
    "        wordcount[word] += 1\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Print most common word\n",
    "word_counter = collections.Counter(wordcount)\n",
    "#for word, count in word_counter.most_common(500):\n",
    " #   print(word, \": \", count)\n",
    "most_common = []\n",
    "\n",
    "for word, count in word_counter.most_common(200):\n",
    "    most_common.append(word)\n",
    "most_common\n",
    "#word_counter.most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16087, 226)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "XX_train, X_val, yy_train, y_val = train_test_split(X_train, y_train, test_size=0.2,\n",
    "    random_state=42)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "forest.fit(pd.DataFrame(XX_train).fillna(0), yy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8104412678682411"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = forest.predict(pd.DataFrame(X_val).fillna(0))\n",
    "accuracy_score(y_val, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8505282784338098"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "XX_train, X_val, yy_train, y_val = train_test_split(X_train, y_train, test_size=0.2,\n",
    "    random_state=42)\n",
    "sc = StandardScaler()\n",
    "#XX_train = sc.fit_transform(XX_train)\n",
    "#X_test = sc.transform(X_test)\n",
    "clf = svm.SVC(gamma='scale',verbose=True)\n",
    "clf.fit(pd.DataFrame(XX_train).fillna(0), yy_train)\n",
    "y_predict = clf.predict(pd.DataFrame(X_val).fillna(0))\n",
    "accuracy_score(y_val, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12869, 525)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "d_train = lgb.Dataset(df, label=y_train)\n",
    "params = {}\n",
    "params['learning_rate'] = 0.003\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'binary'\n",
    "params['metric'] = 'binary_logloss'\n",
    "params['sub_feature'] = 0.5\n",
    "params['num_leaves'] = 80\n",
    "params['min_data'] = 50\n",
    "params['max_depth'] = 20\n",
    "clf = lgb.train(params, d_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LightGBM. Train shape: (16087, 226), test shape: (10981, 350)\n",
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.448914\ttrain's f1: 0.858152\tvalid's binary_logloss: 0.502944\tvalid's f1: 0.756535\n",
      "[200]\ttrain's binary_logloss: 0.347329\ttrain's f1: 0.893712\tvalid's binary_logloss: 0.440639\tvalid's f1: 0.779528\n",
      "[300]\ttrain's binary_logloss: 0.284272\ttrain's f1: 0.919402\tvalid's binary_logloss: 0.411849\tvalid's f1: 0.790167\n",
      "[400]\ttrain's binary_logloss: 0.23836\ttrain's f1: 0.940111\tvalid's binary_logloss: 0.394463\tvalid's f1: 0.793017\n",
      "[500]\ttrain's binary_logloss: 0.202597\ttrain's f1: 0.95693\tvalid's binary_logloss: 0.38381\tvalid's f1: 0.796013\n",
      "[600]\ttrain's binary_logloss: 0.173818\ttrain's f1: 0.97022\tvalid's binary_logloss: 0.376507\tvalid's f1: 0.799858\n",
      "[700]\ttrain's binary_logloss: 0.150022\ttrain's f1: 0.980539\tvalid's binary_logloss: 0.371869\tvalid's f1: 0.80427\n",
      "[800]\ttrain's binary_logloss: 0.130127\ttrain's f1: 0.988078\tvalid's binary_logloss: 0.369203\tvalid's f1: 0.804826\n",
      "[900]\ttrain's binary_logloss: 0.113292\ttrain's f1: 0.992835\tvalid's binary_logloss: 0.367151\tvalid's f1: 0.807379\n",
      "[1000]\ttrain's binary_logloss: 0.0989237\ttrain's f1: 0.995587\tvalid's binary_logloss: 0.366586\tvalid's f1: 0.806532\n",
      "[1100]\ttrain's binary_logloss: 0.0867645\ttrain's f1: 0.99715\tvalid's binary_logloss: 0.366177\tvalid's f1: 0.808797\n",
      "Early stopping, best iteration is:\n",
      "[1044]\ttrain's binary_logloss: 0.0933486\ttrain's f1: 0.996231\tvalid's binary_logloss: 0.366307\tvalid's f1: 0.810082\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.451073\ttrain's f1: 0.852468\tvalid's binary_logloss: 0.498411\tvalid's f1: 0.75\n",
      "[200]\ttrain's binary_logloss: 0.349822\ttrain's f1: 0.888769\tvalid's binary_logloss: 0.432897\tvalid's f1: 0.775687\n",
      "[300]\ttrain's binary_logloss: 0.287065\ttrain's f1: 0.915044\tvalid's binary_logloss: 0.40097\tvalid's f1: 0.790079\n",
      "[400]\ttrain's binary_logloss: 0.241092\ttrain's f1: 0.93713\tvalid's binary_logloss: 0.382077\tvalid's f1: 0.802867\n",
      "[500]\ttrain's binary_logloss: 0.205406\ttrain's f1: 0.956616\tvalid's binary_logloss: 0.370453\tvalid's f1: 0.811449\n",
      "[600]\ttrain's binary_logloss: 0.176468\ttrain's f1: 0.968827\tvalid's binary_logloss: 0.362851\tvalid's f1: 0.814204\n",
      "[700]\ttrain's binary_logloss: 0.152583\ttrain's f1: 0.980267\tvalid's binary_logloss: 0.357899\tvalid's f1: 0.816356\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-292-8025c1cdefad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlgb_f1_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     )\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    214\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   1758\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0;32m   1759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1760\u001b[1;33m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[0;32m   1761\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1762\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(X_train.shape, X_test.shape))\n",
    "\n",
    "# Cross validation model\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=69)\n",
    "\n",
    "# Create arrays and dataframes to store results\n",
    "oof_preds = np.zeros(X_train.shape[0])\n",
    "sub_preds = np.zeros(X_test.shape[0])\n",
    "\n",
    "# k-fold\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "    print(\"Fold %s\" % (n_fold))\n",
    "    train_x, train_y = X_train[train_idx], y_train[train_idx]\n",
    "    valid_x, valid_y = X_train[valid_idx], y_train[valid_idx]\n",
    "\n",
    "    # set data structure\n",
    "    lgb_train = lgb.Dataset(train_x,\n",
    "                            label=train_y,\n",
    "                            free_raw_data=False)\n",
    "    lgb_test = lgb.Dataset(valid_x,\n",
    "                           label=valid_y,\n",
    "                           free_raw_data=False)\n",
    "\n",
    "    params = {\n",
    "        'objective' :'binary',\n",
    "        'learning_rate' : 0.01,\n",
    "        'num_leaves' : 76,\n",
    "        'feature_fraction': 0.64, \n",
    "        'bagging_fraction': 0.8, \n",
    "        'bagging_freq':1,\n",
    "        'boosting_type' : 'gbdt',\n",
    "    }\n",
    "\n",
    "    reg = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_train, lgb_test],\n",
    "        valid_names=['train', 'valid'],\n",
    "        num_boost_round=10000,\n",
    "        verbose_eval=100,\n",
    "        early_stopping_rounds=100,\n",
    "        feval=lgb_f1_score\n",
    "    )\n",
    "\n",
    "    oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=reg.best_iteration)\n",
    "    sub_preds += reg.predict(X_test, num_iteration=reg.best_iteration) / folds.n_splits\n",
    "\n",
    "    del reg, train_x, train_y, valid_x, valid_y\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "preds = (sub_preds > threshold).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'] = preds\n",
    "test_data[['id','label']].to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 10981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 10981\n",
      "Review 2000 of 10981\n",
      "Review 3000 of 10981\n",
      "Review 4000 of 10981\n",
      "Review 5000 of 10981\n",
      "Review 6000 of 10981\n",
      "Review 7000 of 10981\n",
      "Review 8000 of 10981\n",
      "Review 9000 of 10981\n",
      "Review 10000 of 10981\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-3bb7ae807644>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mrealTestDataVecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetAvgFeatureVecs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_real_test_reviews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2vec_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mbow_train_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetEmojiBowFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memojis_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mrealTestDataVecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrealTestDataVecs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbow_train_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "# Calculating average feature vactors for test set     \n",
    "clean_real_test_reviews = []\n",
    "for review in test_data['review']:\n",
    "    clean_real_test_reviews.append(review_wordlist(review))\n",
    "    \n",
    "realTestDataVecs = getAvgFeatureVecs(clean_real_test_reviews, word2vec_model, num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_train_features = getEmojiBowFeatures(test_data['review'], emojis_vocab)\n",
    "realTestDataVecs = np.concatenate((realTestDataVecs,bow_train_features),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = realTestDataVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = clf.predict(pd.DataFrame(realTestDataVecs).fillna(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'] = y_predict\n",
    "test_data[['id','label']].to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
