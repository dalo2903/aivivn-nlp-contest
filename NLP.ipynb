{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk.data\n",
    "import gensim\n",
    "from distutils.version import LooseVersion, StrictVersion\n",
    "import os\n",
    "import codecs\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "global word2vec_model\n",
    "from deepai_nlp.tokenization.crf_tokenizer import CrfTokenizer\n",
    "from deepai_nlp.tokenization.utils import preprocess_text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/github/ngxbac/aivivn_phanloaisacthaibinhluan/blob/master/baseline_lgbm_tfidf.ipynb#scrollTo=ojmynVfZtFRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSource(object):\n",
    "    def _load_raw_data(self,filename, is_train=True):\n",
    "        a = []\n",
    "        b = []\n",
    "        regex = 'train_'\n",
    "        if not is_train:\n",
    "            regex = 'test_'\n",
    "        with open(filename, 'r', encoding=\"utf8\") as file:\n",
    "            for line in file :\n",
    "                if regex in line:\n",
    "                    b.append(a)\n",
    "                    a = [line]\n",
    "                elif line!='\\n':\n",
    "                    a.append(line)       \n",
    "        b.append(a)      \n",
    "        return b[1:]\n",
    "    \n",
    "    def _create_row(self, sample, is_train=True):\n",
    "        d = {}\n",
    "        d['id'] = sample[0].replace('\\n','')\n",
    "        review = \"\"\n",
    "        if is_train:\n",
    "            for clause in sample[1:-1]:\n",
    "                review+= clause.replace('\\n','').strip()\n",
    "            d['label'] = int(sample[-1].replace('\\n',''))          \n",
    "        else:         \n",
    "            for clause in sample[1:]:\n",
    "                review+= clause.replace('\\n','').strip()\n",
    "        d['review'] = review\n",
    "        return d\n",
    "    \n",
    "    \n",
    "    def load_data(self, filename, is_train=True):\n",
    "        raw_data = self._load_raw_data(filename, is_train)\n",
    "        lst = []\n",
    "        for row in raw_data:\n",
    "            lst.append(self._create_row(row, is_train))\n",
    "        return lst"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "search = \"d you don't need a dog. but if you like dogs, you should think of getting one for your own. Or a cat?\"\n",
    "mapping =  {\"you\": \"aaaaa\", r\"\\bd\\b\": \"!!!\" }\n",
    "\n",
    "#search = [search.replace(key, value) for key, value in mapping.items()][0]\n",
    "\n",
    "for key, value in mapping.items():\n",
    "    search = re.sub(key,value,search)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mapping = {\n",
    "    \"ship\": \"vận chuyển\",\n",
    "    \"shop\": \"cửa hàng\",\n",
    "    \"sp\": \"sản phẩm\",\n",
    "    r\"\\bm\\b\": \" mình\",\n",
    "    \"mik\": \"mình\",\n",
    "    r\"\\bk\\b\": \"không\",\n",
    "    r\"\\bkh\\b\": \"không\",\n",
    "    r\"\\btl\\b\": \"trả lời\",\n",
    "    r\"\\br\\b\": \"rồi\",\n",
    "    \"fb\": \"mạng xã hội \", # facebook\n",
    "    \"face\": \"mạng xã hội\",\n",
    "    \"thanks\": \"cảm ơn\",\n",
    "    \"thank\": \"cảm ơn\",\n",
    "    \"tks\": \"cảm ơn\", \n",
    "    r\"\\bdc\\b\": \"được\",\n",
    "    r\"\\bok\\b\": \"tốt\",\n",
    "    r\"\\bdt\\b\": \"điện thoại\",\n",
    "    r\"\\bh\\b\": \"giờ\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mapping = {\n",
    "    \"ship\": \"vận chuyển\",\n",
    "    \"shop\": \"cửa hàng\",\n",
    "    \"sp\": \"sản phẩm\",\n",
    "    \"m\": \" mình\",\n",
    "    \"mik\": \"mình\",\n",
    "    \"k\": \"không\",\n",
    "    \"kh\": \"không\",\n",
    "    \"tl\": \"trả lời\",\n",
    "    \"r\": \"rồi\",\n",
    "    \"fb\": \"mạng xã hội \", # facebook\n",
    "    \"face\": \"mạng xã hội\",\n",
    "    \"thanks\": \"cảm ơn\",\n",
    "    \"thank\": \"cảm ơn\",\n",
    "    \"tks\": \"cảm ơn\", \n",
    "    \"dc\": \"được\",\n",
    "    \"ok\": \"tốt\",\n",
    "    \"dt\": \"điện thoại\",\n",
    "    \"h\": \"giờ\",\n",
    "    \"hsd\": \"hạn sử dụng\",\n",
    "    \"trc\": \"trước\",\n",
    "    \"oki\": \"tốt\",\n",
    "    \"ad\": \"cửa hàng\"\n",
    "}\n",
    "for i, m in mapping.items():\n",
    "    mapping[i] = m.strip().replace(' ','_')\n",
    "#Load stopwords\n",
    "stopwords_file = 'vietnamese-stopwords.txt'\n",
    "stopwords = []\n",
    "with open(stopwords_file, 'r', encoding=\"utf8\") as file:\n",
    "    for line in file :\n",
    "        stopwords.append(line.replace('\\n','').strip().replace(' ','_'))\n",
    "tokenizer = CrfTokenizer()\n",
    "vietnamese_chars = \"[^a-zA-Z_ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚĂĐĨŨƠàáâãèéêìíòóôõùúăđĩũơƯĂẠẢẤẦẨẪẬẮẰẲẴẶẸẺẼỀỀỂưăạảấầẩẫậắằẳẵặẹẻẽềềểỄỆỈỊỌỎỐỒỔỖỘỚỜỞỠỢỤỦỨỪễệỉịọỏốồổỗộớờởỡợụủứừỬỮỰỲỴÝỶỸửữựỳỵỷỹ]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ship': 'vận_chuyển',\n",
       " 'shop': 'cửa_hàng',\n",
       " 'sp': 'sản_phẩm',\n",
       " 'm': 'mình',\n",
       " 'mik': 'mình',\n",
       " 'k': 'không',\n",
       " 'kh': 'không',\n",
       " 'tl': 'trả_lời',\n",
       " 'r': 'rồi',\n",
       " 'fb': 'mạng_xã_hội',\n",
       " 'face': 'mạng_xã_hội',\n",
       " 'thanks': 'cảm_ơn',\n",
       " 'thank': 'cảm_ơn',\n",
       " 'tks': 'cảm_ơn',\n",
       " 'dc': 'được',\n",
       " 'ok': 'tốt',\n",
       " 'dt': 'điện_thoại',\n",
       " 'h': 'giờ',\n",
       " 'hsd': 'hạn_sử_dụng',\n",
       " 'trc': 'trước',\n",
       " 'oki': 'tốt',\n",
       " 'ad': 'cửa_hàng'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vietnamese_chars = \"[^a-z0-9A-Z_ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚĂĐĨŨƠàáâãèéêìíòóôõùúăđĩũơƯĂẠẢẤẦẨẪẬẮẰẲẴẶẸẺẼỀỀỂưăạảấầẩẫậắằẳẵặẹẻẽềềểỄỆỈỊỌỎỐỒỔỖỘỚỜỞỠỢỤỦỨỪễệỉịọỏốồổỗộớờởỡợụủứừỬỮỰỲỴÝỶỸửữựỳỵỷỹ]\"\n",
    "def review_wordlist(review, remove_stopwords= False):\n",
    "    review_text = str(review)\n",
    "    # 2. Removing non-letter.\n",
    "    review_text = re.sub(vietnamese_chars,\" \",review_text)\n",
    "    # 3. Converting to lower case and splitting\n",
    "    words = review_text.lower().split()\n",
    "    # 4. Optionally remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords)     \n",
    "        words = [w for w in words if not w in stops]\n",
    "    for i,w in enumerate(words):\n",
    "        if w in mapping:\n",
    "            words[i]= mapping[w]\n",
    "    \n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokenizer = CrfTokenizer()\n",
    "vietnamese_chars = \"[^a-zA-Z_ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚĂĐĨŨƠàáâãèéêìíòóôõùúăđĩũơƯĂẠẢẤẦẨẪẬẮẰẲẴẶẸẺẼỀỀỂưăạảấầẩẫậắằẳẵặẹẻẽềềểỄỆỈỊỌỎỐỒỔỖỘỚỜỞỠỢỤỦỨỪễệỉịọỏốồổỗộớờởỡợụủứừỬỮỰỲỴÝỶỸửữựỳỵỷỹ]\"\n",
    "def review_wordlist(review, remove_stopwords= False):\n",
    "    review_text = str(review)\n",
    "    # 2. Removing non-letter.\n",
    "    review_text = re.sub(vietnamese_chars,\" \",review_text)\n",
    "    # 3. Converting to lower case and splitting\n",
    "    review_text = review_text.lower()\n",
    "    for key, value in mapping.items():\n",
    "        review_text = re.sub(key,value,review_text)\n",
    "    words = tokenizer.tokenize(review_text)\n",
    "    # 4. Optionally remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords)     \n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CrfTokenizer()\n",
    "def clean_text(review, char_reg, stopwords, mapping):\n",
    "    review_text = str(review)\n",
    "    # 2. Removing non-letter.\n",
    "    review_text = re.sub(char_reg,\" \",review_text)\n",
    "    # 3. Converting to lower case and splitting\n",
    "    review_text = review_text.lower()\n",
    "    # 4. Subtitute words\n",
    "    #words = review_text.split()\n",
    "    words = tokenizer.tokenize(review_text)\n",
    "    print(words)\n",
    "    # 5. Remove stopwords\n",
    "    #words = [w.replace('_',' ') for w in words]\n",
    "    stops = set(stopwords)  \n",
    "    #print(stops)\n",
    "    words = [w for w in words if not w in stops]\n",
    "    print(words)\n",
    "    for i,w in enumerate(words):\n",
    "        if w in mapping:\n",
    "            words[i]= mapping[w]\n",
    "    review_text = ' '.join(words)\n",
    "    return(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from file C:\\ProgramData\\Anaconda3\\lib\\site-packages\\deepai_nlp-0.0.1-py3.7.egg\\deepai_nlp\\models/pretrained_tokenizer.crfsuite\n",
      "['chưa', 'dùng', 'thử', 'nên', 'chưa', 'bi', 't']\n",
      "['thử', 'bi', 't']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'thử bi t'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"chưa dùng thử nên chưa biết\",char_reg = vietnamese_chars, mapping =mapping, stopwords = stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from file C:\\ProgramData\\Anaconda3\\lib\\site-packages\\deepai_nlp-0.0.1-py3.7.egg\\deepai_nlp\\models/pretrained_tokenizer.crfsuite\n"
     ]
    }
   ],
   "source": [
    "ds = DataSource()\n",
    "train_data = pd.DataFrame(ds.load_data('dataset/train.crash'))\n",
    "test_data = pd.DataFrame(ds.load_data('dataset/test.crash', is_train=False))\n",
    "#train_data['review'] = train_data['review'].fillna(\"none\")\n",
    "#test_data['review'] = test_data['review'].fillna(\"none\")\n",
    "df = pd.concat([train_data,test_data], axis=0, sort=False)\n",
    "df['review_cleaned'] = df['review'].apply(lambda s: clean_text(s,char_reg = vietnamese_chars, mapping =mapping, stopwords = stopwords))\n",
    "df['num_words'] = df['review'].apply(lambda s: len(s.split()))\n",
    "df['num_unique_words'] = df['review'].apply(lambda s: len(set(w for w in s.split())))\n",
    "df['words_vs_unique'] = df['num_unique_words'] / df['num_words'] * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "      <th>review_cleaned</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Dung dc sp tot cam onshop Đóng gói sản phẩm r...</td>\n",
       "      <td>dung được sản phẩm tot cam onshop đóng_gói sản...</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>90.476190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Chất lượng sản phẩm tuyệt vời . Son mịn nhưn...</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời son mịn đánh màu...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Chất lượng sản phẩm tuyệt vời nhưng k có hộp...</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời không hộp không ...</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>78.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_000003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\":(( Mình hơi thất vọng 1 chút vì mình đã kỳ v...</td>\n",
       "      <td>hơi thất_vọng chút kỳ_vọng sách hi_vọng học_tậ...</td>\n",
       "      <td>114</td>\n",
       "      <td>92</td>\n",
       "      <td>80.701754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_000004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Lần trước mình mua áo gió màu hồng rất ok mà ...</td>\n",
       "      <td>mua áo_gió màu hồng tốt đợt giao áo_gió chất v...</td>\n",
       "      <td>26</td>\n",
       "      <td>24</td>\n",
       "      <td>92.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train_000005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Chất lượng sản phẩm tuyệt vời có điều không ...</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời cứng_cáp cố_định...</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>95.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train_000006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Đã nhận đc hàng rất nhanh mới đặt buổi tối mà...</td>\n",
       "      <td>đc hàng tối trưa mai đóng_gói sản_phẩm đẹp cửa...</td>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>93.548387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train_000007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Các siêu phẩm thấy cấu hình toàn tựa tựa nhau...</td>\n",
       "      <td>siêu phẩm cấu_hình toàn tựa tựa ko đột_phá nân...</td>\n",
       "      <td>48</td>\n",
       "      <td>44</td>\n",
       "      <td>91.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train_000008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Hàng ship nhanh  chất lượng tốt  tư vấn nhiệt...</td>\n",
       "      <td>hàng vận chuyển chất_lượng tư_vấn nhiệt_tình v...</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>95.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train_000009</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Đồng hồ đẹp nhưng 1 cái đứt dây  1 cái k chạy...</td>\n",
       "      <td>đồng_hồ đẹp đứt dây không chạy mua ve sửa</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>87.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>train_000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Chất lượng sản phẩm tuyệt vời.y hình chụp.đá...</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời y hình chụp tiền</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>90.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>train_000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Hjhj shop giao hàng nhanh quá. Đẹp lắm ạ bé n...</td>\n",
       "      <td>hjhj cửa hàng giao hàng đẹp lắm bé  mình</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>train_000012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"\"nhìn đẹp phết nhỉ..\"\"</td>\n",
       "      <td>đẹp ph t</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>train_000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Đóng gói rất đẹp. Chất lượng sản phẩm rất tốt...</td>\n",
       "      <td>đóng_gói đẹp chất_lượng sản_phẩm chất_lượng sả...</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>68.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>train_000014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Săn đc với giá 11k. Toẹt vời\"</td>\n",
       "      <td>săn đc giá không vời</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>train_000015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"OK rất hài lòng\"</td>\n",
       "      <td>tốt hài_lòng</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>train_000016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Giao thiếu mình cái này rồi shop ơi T^T\"</td>\n",
       "      <td>giao thi u cửa hàng t t</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>train_000017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Chất lượng sản phẩm tuyệt vời tôi rất thích\"</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>train_000018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Giày đẹp lắm có điều dây hơi ngắn tí ạ  Chất ...</td>\n",
       "      <td>giày đẹp lắm dây hơi ngắn tí chất_lượng sản_ph...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>train_000019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Yếm vải đẹp nhưng ít mẫu đẹp\"</td>\n",
       "      <td>y  mình vải đẹp mẫu đẹp</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>train_000020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Chất lượng sản phẩm tuyệt vời Đóng gói sản p...</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời đóng_gói sản_phẩ...</td>\n",
       "      <td>33</td>\n",
       "      <td>28</td>\n",
       "      <td>84.848485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>train_000021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"không hài lòng sản phẩm cho lắm. giặt lan đầu...</td>\n",
       "      <td>hài_lòng sản_phẩm lắm giặt lan da nhoe màu giờ t</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>train_000022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Giao hàng nhanh, mặc đẹpCám ơn shop\"</td>\n",
       "      <td>giao hàng mặc đẹpcám ơn cửa hàng</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>train_000023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Chất lượng sản phẩm tuyệt vời bao bì cute ph...</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời bao_bì cute phô_...</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>train_000024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Đồng hồ thì đẹp thật. Nhưng tại sao kim lúc c...</td>\n",
       "      <td>đồng_hồ đẹp kim chạy đứng nhắn_tin ko trả_lời ...</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>92.592593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>train_000025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Giao hàng siêu nhanh.Đóng gói cẩn thận và tư ...</td>\n",
       "      <td>giao hàng siêu đóng_gói cẩn_thận tư_vấn nhiệt_...</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>94.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>train_000026</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"\"Cũng hơi bất tiện xu thế này e rằng đa phằn ...</td>\n",
       "      <td>hơi bất_tiện xu th e đa phằn ưa_thích</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>train_000027</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Toàn hàng trungkhi mua quên ko coi kĩ\"</td>\n",
       "      <td>toàn hàng trungkhi mua quên ko coi kĩ</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>train_000028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Đóng gói sản phẩm rất đẹp và chắc chắn. Được...</td>\n",
       "      <td>đóng_gói sản_phẩm đẹp cửa hàng tặng quà</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>train_000029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Hôm nay chiên thử cá hồi, cá chiên ăn ngọt hơ...</td>\n",
       "      <td>hôm_nay chiên thử cá_hồi cá_chiên chiên kiểu t...</td>\n",
       "      <td>33</td>\n",
       "      <td>28</td>\n",
       "      <td>84.848485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>train_000070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Đẹp thích hết\"</td>\n",
       "      <td>đẹp giờ t</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>train_000071</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Kem dùng ổn mỗi tội hơi tối do với da mình\"</td>\n",
       "      <td>kem ổn mỗi_tội hơi tối da</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>train_000072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Đóng gói sản phẩm rất đẹp và chắc chắn chất l...</td>\n",
       "      <td>đóng_gói sản_phẩm đẹp chất_lượng sản_phẩm</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>85.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>train_000073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Good value for money Good value for money Fa...</td>\n",
       "      <td>good value for money good value for money fast...</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>73.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>train_000074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Chất lượng sản phẩm tốt Đóng gói sản phẩm rấ...</td>\n",
       "      <td>chất_lượng sản_phẩm đóng_gói sản_phẩm cẩn_thận...</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>86.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>train_000075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Chị chủ tư vấn rất nhiệt tình 😆😆 hình dáng và...</td>\n",
       "      <td>chủ tư_vấn nhiệt_tình_hình_dáng chất_lượng sản...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>train_000076</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"SHOP ĐĂNG ẢNH MỘT ĐẰNG SẢN PHẨM MỘT LẺO .BÁN ...</td>\n",
       "      <td>cửa hàng đăng ảnh đằng sản_phẩm lẻo bán_cáp úp...</td>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>87.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>train_000077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Chất lượng sản phẩm tuyệt vời. Shop tư vấn n...</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời cửa hàng tư_vấn ...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>train_000078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\" Poor delivery speed Poor product quality\"</td>\n",
       "      <td>poor delivery speed poor product quality</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>85.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>train_000079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Đúng là tiền nào của đó ko ngon mùi kì kì ko ...</td>\n",
       "      <td>tiền ko ngon mùi kì kì ko thơm lạp_xưởng bình_...</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>90.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>train_000080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Chất lượng sản phẩm tuyệt vời. Đóng gói sản ...</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời đóng_gói sản_phẩ...</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>92.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>train_000081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Giày Ok lắm shop ship hàng nhanh nữa ❤️\"</td>\n",
       "      <td>giày tốt lắm cửa hàng vận chuyển hàng</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>train_000082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Chất lượng sản phẩm tuyệt vời.rất  OK \"</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời tốt</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>88.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>train_000083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Giày chắc chắn . Đế hơi bẩn tí nhưng chắc sẽ ...</td>\n",
       "      <td>giày đ hơi bẩn tí lau giao hàng</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>88.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>train_000084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Hàng giao thiếu kêu gửi bù cho mà cả tháng ch...</td>\n",
       "      <td>hàng giao thi u kêu gửi bù gửi nhắn_tin ko trả...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>train_000085</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Shop hết hàng nhưng không báo. Mình đặt 1 cặp...</td>\n",
       "      <td>cửa hàng giờ t hàng báo cặp th gởi ốp kèm tiền...</td>\n",
       "      <td>55</td>\n",
       "      <td>40</td>\n",
       "      <td>72.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>train_000086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Giao ko đúng Đồ. Tôi đã bị lừa \"</td>\n",
       "      <td>giao ko đồ lừa</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>train_000087</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"hk giong mẩu gì cả da vay bi lỗi nua \"</td>\n",
       "      <td>hk giong mẩu da vay bi lỗi nua</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>train_000088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Tương đối hài lòng giày chắc chắn êm nhưng đư...</td>\n",
       "      <td>tương_đối hài_lòng giày êm đường may thừa tốt ...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>train_000089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Dép xinh. Chủ shop rep tn rất nhiệt tình. Chú...</td>\n",
       "      <td>dép xinh chủ cửa hàng rep tn nhiệt_tình chúc c...</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>93.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>train_000090</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Mua 2đ có 2 lỗ xỏ dây ib shop kh trả lời giày...</td>\n",
       "      <td>mua đ lỗ xỏ dây ib cửa hàng không trả_lời giày...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>train_000091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Tốt tgian giao hàng nhanh\"</td>\n",
       "      <td>tgian giao hàng</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>train_000092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Nên mua nha mọi người\"</td>\n",
       "      <td>mua nha</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>train_000093</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Thấy ghi đc tặng kèm tã quần.đến khi nhận hàn...</td>\n",
       "      <td>ghi đc tặng kèm tã quần đ n hàng không cửa hàn...</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>88.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>train_000094</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\" Shop phục vụ rất kém. Đã phải nhắn tin hỏi c...</td>\n",
       "      <td>cửa hàng phục_vụ kém nhắn_tin cẩn_thận không c...</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>87.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>train_000095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Shop phục vụ rất tốt. Sản phẩm đúng với thông...</td>\n",
       "      <td>cửa hàng phục_vụ sản_phẩm thông_tin đăng_tải</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>train_000096</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Đặt combo mà shop chỉ giao bánh gạo và sốt k ...</td>\n",
       "      <td>combo cửa hàng giao bánh gạo sốt không phô_mai...</td>\n",
       "      <td>45</td>\n",
       "      <td>39</td>\n",
       "      <td>86.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>train_000097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"Shop này uy tín nè hàng đã đẹp mà còn đầy đủ ...</td>\n",
       "      <td>cửa hàng uy_tín nè hàng đẹp đầy_đủ hộp pin tặn...</td>\n",
       "      <td>37</td>\n",
       "      <td>35</td>\n",
       "      <td>94.594595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>train_000098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Chất lượng sản phẩm tuyệt vời Đóng gói sản p...</td>\n",
       "      <td>chất_lượng sản_phẩm tuyệt_vời đóng_gói sản_phẩ...</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>train_000099</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Sản phẩm k như quảng cáo là 9058 liên hệ theo...</td>\n",
       "      <td>sản_phẩm không quảng_cáo liên_hệ đt cửa hàng v...</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>96.428571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  label                                             review  \\\n",
       "0   train_000000    0.0  \"Dung dc sp tot cam onshop Đóng gói sản phẩm r...   \n",
       "1   train_000001    0.0  \" Chất lượng sản phẩm tuyệt vời . Son mịn nhưn...   \n",
       "2   train_000002    0.0  \" Chất lượng sản phẩm tuyệt vời nhưng k có hộp...   \n",
       "3   train_000003    1.0  \":(( Mình hơi thất vọng 1 chút vì mình đã kỳ v...   \n",
       "4   train_000004    1.0  \"Lần trước mình mua áo gió màu hồng rất ok mà ...   \n",
       "5   train_000005    0.0  \" Chất lượng sản phẩm tuyệt vời có điều không ...   \n",
       "6   train_000006    0.0  \"Đã nhận đc hàng rất nhanh mới đặt buổi tối mà...   \n",
       "7   train_000007    1.0  \"Các siêu phẩm thấy cấu hình toàn tựa tựa nhau...   \n",
       "8   train_000008    0.0  \"Hàng ship nhanh  chất lượng tốt  tư vấn nhiệt...   \n",
       "9   train_000009    1.0  \"Đồng hồ đẹp nhưng 1 cái đứt dây  1 cái k chạy...   \n",
       "10  train_000010    0.0  \" Chất lượng sản phẩm tuyệt vời.y hình chụp.đá...   \n",
       "11  train_000011    0.0  \"Hjhj shop giao hàng nhanh quá. Đẹp lắm ạ bé n...   \n",
       "12  train_000012    0.0                            \"\"nhìn đẹp phết nhỉ..\"\"   \n",
       "13  train_000013    0.0  \"Đóng gói rất đẹp. Chất lượng sản phẩm rất tốt...   \n",
       "14  train_000014    0.0                     \"Săn đc với giá 11k. Toẹt vời\"   \n",
       "15  train_000015    0.0                                  \"OK rất hài lòng\"   \n",
       "16  train_000016    1.0          \"Giao thiếu mình cái này rồi shop ơi T^T\"   \n",
       "17  train_000017    0.0      \"Chất lượng sản phẩm tuyệt vời tôi rất thích\"   \n",
       "18  train_000018    0.0  \"Giày đẹp lắm có điều dây hơi ngắn tí ạ  Chất ...   \n",
       "19  train_000019    0.0                     \"Yếm vải đẹp nhưng ít mẫu đẹp\"   \n",
       "20  train_000020    0.0  \" Chất lượng sản phẩm tuyệt vời Đóng gói sản p...   \n",
       "21  train_000021    1.0  \"không hài lòng sản phẩm cho lắm. giặt lan đầu...   \n",
       "22  train_000022    0.0              \"Giao hàng nhanh, mặc đẹpCám ơn shop\"   \n",
       "23  train_000023    0.0  \" Chất lượng sản phẩm tuyệt vời bao bì cute ph...   \n",
       "24  train_000024    1.0  \"Đồng hồ thì đẹp thật. Nhưng tại sao kim lúc c...   \n",
       "25  train_000025    0.0  \"Giao hàng siêu nhanh.Đóng gói cẩn thận và tư ...   \n",
       "26  train_000026    1.0  \"\"Cũng hơi bất tiện xu thế này e rằng đa phằn ...   \n",
       "27  train_000027    1.0            \"Toàn hàng trungkhi mua quên ko coi kĩ\"   \n",
       "28  train_000028    0.0  \" Đóng gói sản phẩm rất đẹp và chắc chắn. Được...   \n",
       "29  train_000029    0.0  \"Hôm nay chiên thử cá hồi, cá chiên ăn ngọt hơ...   \n",
       "..           ...    ...                                                ...   \n",
       "70  train_000070    0.0                                    \"Đẹp thích hết\"   \n",
       "71  train_000071    0.0       \"Kem dùng ổn mỗi tội hơi tối do với da mình\"   \n",
       "72  train_000072    0.0  \"Đóng gói sản phẩm rất đẹp và chắc chắn chất l...   \n",
       "73  train_000073    0.0  \" Good value for money Good value for money Fa...   \n",
       "74  train_000074    0.0  \" Chất lượng sản phẩm tốt Đóng gói sản phẩm rấ...   \n",
       "75  train_000075    0.0  \"Chị chủ tư vấn rất nhiệt tình 😆😆 hình dáng và...   \n",
       "76  train_000076    1.0  \"SHOP ĐĂNG ẢNH MỘT ĐẰNG SẢN PHẨM MỘT LẺO .BÁN ...   \n",
       "77  train_000077    0.0  \" Chất lượng sản phẩm tuyệt vời. Shop tư vấn n...   \n",
       "78  train_000078    1.0        \" Poor delivery speed Poor product quality\"   \n",
       "79  train_000079    0.0  \"Đúng là tiền nào của đó ko ngon mùi kì kì ko ...   \n",
       "80  train_000080    0.0  \" Chất lượng sản phẩm tuyệt vời. Đóng gói sản ...   \n",
       "81  train_000081    0.0          \"Giày Ok lắm shop ship hàng nhanh nữa ❤️\"   \n",
       "82  train_000082    0.0          \" Chất lượng sản phẩm tuyệt vời.rất  OK \"   \n",
       "83  train_000083    0.0  \"Giày chắc chắn . Đế hơi bẩn tí nhưng chắc sẽ ...   \n",
       "84  train_000084    0.0  \"Hàng giao thiếu kêu gửi bù cho mà cả tháng ch...   \n",
       "85  train_000085    1.0  \"Shop hết hàng nhưng không báo. Mình đặt 1 cặp...   \n",
       "86  train_000086    0.0                  \"Giao ko đúng Đồ. Tôi đã bị lừa \"   \n",
       "87  train_000087    1.0            \"hk giong mẩu gì cả da vay bi lỗi nua \"   \n",
       "88  train_000088    0.0  \"Tương đối hài lòng giày chắc chắn êm nhưng đư...   \n",
       "89  train_000089    0.0  \"Dép xinh. Chủ shop rep tn rất nhiệt tình. Chú...   \n",
       "90  train_000090    1.0  \"Mua 2đ có 2 lỗ xỏ dây ib shop kh trả lời giày...   \n",
       "91  train_000091    0.0                        \"Tốt tgian giao hàng nhanh\"   \n",
       "92  train_000092    0.0                            \"Nên mua nha mọi người\"   \n",
       "93  train_000093    1.0  \"Thấy ghi đc tặng kèm tã quần.đến khi nhận hàn...   \n",
       "94  train_000094    1.0  \" Shop phục vụ rất kém. Đã phải nhắn tin hỏi c...   \n",
       "95  train_000095    0.0  \"Shop phục vụ rất tốt. Sản phẩm đúng với thông...   \n",
       "96  train_000096    1.0  \"Đặt combo mà shop chỉ giao bánh gạo và sốt k ...   \n",
       "97  train_000097    0.0  \"Shop này uy tín nè hàng đã đẹp mà còn đầy đủ ...   \n",
       "98  train_000098    0.0  \" Chất lượng sản phẩm tuyệt vời Đóng gói sản p...   \n",
       "99  train_000099    1.0  \"Sản phẩm k như quảng cáo là 9058 liên hệ theo...   \n",
       "\n",
       "                                       review_cleaned  num_words  \\\n",
       "0   dung được sản phẩm tot cam onshop đóng_gói sản...         21   \n",
       "1   chất_lượng sản_phẩm tuyệt_vời son mịn đánh màu...         19   \n",
       "2   chất_lượng sản_phẩm tuyệt_vời không hộp không ...         19   \n",
       "3   hơi thất_vọng chút kỳ_vọng sách hi_vọng học_tậ...        114   \n",
       "4   mua áo_gió màu hồng tốt đợt giao áo_gió chất v...         26   \n",
       "5   chất_lượng sản_phẩm tuyệt_vời cứng_cáp cố_định...         23   \n",
       "6   đc hàng tối trưa mai đóng_gói sản_phẩm đẹp cửa...         31   \n",
       "7   siêu phẩm cấu_hình toàn tựa tựa ko đột_phá nân...         48   \n",
       "8   hàng vận chuyển chất_lượng tư_vấn nhiệt_tình v...         20   \n",
       "9           đồng_hồ đẹp đứt dây không chạy mua ve sửa         16   \n",
       "10     chất_lượng sản_phẩm tuyệt_vời y hình chụp tiền         11   \n",
       "11           hjhj cửa hàng giao hàng đẹp lắm bé  mình         14   \n",
       "12                                           đẹp ph t          4   \n",
       "13  đóng_gói đẹp chất_lượng sản_phẩm chất_lượng sả...         16   \n",
       "14                               săn đc giá không vời          7   \n",
       "15                                       tốt hài_lòng          4   \n",
       "16                            giao thi u cửa hàng t t          9   \n",
       "17                      chất_lượng sản_phẩm tuyệt_vời          9   \n",
       "18  giày đẹp lắm dây hơi ngắn tí chất_lượng sản_ph...         16   \n",
       "19                            y  mình vải đẹp mẫu đẹp          7   \n",
       "20  chất_lượng sản_phẩm tuyệt_vời đóng_gói sản_phẩ...         33   \n",
       "21   hài_lòng sản_phẩm lắm giặt lan da nhoe màu giờ t         16   \n",
       "22                   giao hàng mặc đẹpcám ơn cửa hàng          7   \n",
       "23  chất_lượng sản_phẩm tuyệt_vời bao_bì cute phô_...         28   \n",
       "24  đồng_hồ đẹp kim chạy đứng nhắn_tin ko trả_lời ...         27   \n",
       "25  giao hàng siêu đóng_gói cẩn_thận tư_vấn nhiệt_...         18   \n",
       "26              hơi bất_tiện xu th e đa phằn ưa_thích         17   \n",
       "27              toàn hàng trungkhi mua quên ko coi kĩ          8   \n",
       "28            đóng_gói sản_phẩm đẹp cửa hàng tặng quà         17   \n",
       "29  hôm_nay chiên thử cá_hồi cá_chiên chiên kiểu t...         33   \n",
       "..                                                ...        ...   \n",
       "70                                          đẹp giờ t          3   \n",
       "71                          kem ổn mỗi_tội hơi tối da         11   \n",
       "72          đóng_gói sản_phẩm đẹp chất_lượng sản_phẩm         14   \n",
       "73  good value for money good value for money fast...         15   \n",
       "74  chất_lượng sản_phẩm đóng_gói sản_phẩm cẩn_thận...         22   \n",
       "75  chủ tư_vấn nhiệt_tình_hình_dáng chất_lượng sản...         17   \n",
       "76  cửa hàng đăng ảnh đằng sản_phẩm lẻo bán_cáp úp...         54   \n",
       "77  chất_lượng sản_phẩm tuyệt_vời cửa hàng tư_vấn ...         20   \n",
       "78           poor delivery speed poor product quality          7   \n",
       "79  tiền ko ngon mùi kì kì ko thơm lạp_xưởng bình_...         22   \n",
       "80  chất_lượng sản_phẩm tuyệt_vời đóng_gói sản_phẩ...         25   \n",
       "81              giày tốt lắm cửa hàng vận chuyển hàng          9   \n",
       "82                  chất_lượng sản_phẩm tuyệt_vời tốt          9   \n",
       "83                    giày đ hơi bẩn tí lau giao hàng         17   \n",
       "84  hàng giao thi u kêu gửi bù gửi nhắn_tin ko trả...         18   \n",
       "85  cửa hàng giờ t hàng báo cặp th gởi ốp kèm tiền...         55   \n",
       "86                                     giao ko đồ lừa          9   \n",
       "87                     hk giong mẩu da vay bi lỗi nua         11   \n",
       "88  tương_đối hài_lòng giày êm đường may thừa tốt ...         24   \n",
       "89  dép xinh chủ cửa hàng rep tn nhiệt_tình chúc c...         16   \n",
       "90  mua đ lỗ xỏ dây ib cửa hàng không trả_lời giày...         21   \n",
       "91                                    tgian giao hàng          5   \n",
       "92                                            mua nha          5   \n",
       "93  ghi đc tặng kèm tã quần đ n hàng không cửa hàn...         18   \n",
       "94  cửa hàng phục_vụ kém nhắn_tin cẩn_thận không c...         24   \n",
       "95       cửa hàng phục_vụ sản_phẩm thông_tin đăng_tải         13   \n",
       "96  combo cửa hàng giao bánh gạo sốt không phô_mai...         45   \n",
       "97  cửa hàng uy_tín nè hàng đẹp đầy_đủ hộp pin tặn...         37   \n",
       "98  chất_lượng sản_phẩm tuyệt_vời đóng_gói sản_phẩ...         25   \n",
       "99  sản_phẩm không quảng_cáo liên_hệ đt cửa hàng v...         28   \n",
       "\n",
       "    num_unique_words  words_vs_unique  \n",
       "0                 19        90.476190  \n",
       "1                 19       100.000000  \n",
       "2                 15        78.947368  \n",
       "3                 92        80.701754  \n",
       "4                 24        92.307692  \n",
       "5                 22        95.652174  \n",
       "6                 29        93.548387  \n",
       "7                 44        91.666667  \n",
       "8                 19        95.000000  \n",
       "9                 14        87.500000  \n",
       "10                10        90.909091  \n",
       "11                14       100.000000  \n",
       "12                 4       100.000000  \n",
       "13                11        68.750000  \n",
       "14                 7       100.000000  \n",
       "15                 4       100.000000  \n",
       "16                 9       100.000000  \n",
       "17                 9       100.000000  \n",
       "18                16       100.000000  \n",
       "19                 7       100.000000  \n",
       "20                28        84.848485  \n",
       "21                16       100.000000  \n",
       "22                 7       100.000000  \n",
       "23                28       100.000000  \n",
       "24                25        92.592593  \n",
       "25                17        94.444444  \n",
       "26                17       100.000000  \n",
       "27                 8       100.000000  \n",
       "28                17       100.000000  \n",
       "29                28        84.848485  \n",
       "..               ...              ...  \n",
       "70                 3       100.000000  \n",
       "71                11       100.000000  \n",
       "72                12        85.714286  \n",
       "73                11        73.333333  \n",
       "74                19        86.363636  \n",
       "75                17       100.000000  \n",
       "76                47        87.037037  \n",
       "77                20       100.000000  \n",
       "78                 6        85.714286  \n",
       "79                20        90.909091  \n",
       "80                23        92.000000  \n",
       "81                 9       100.000000  \n",
       "82                 8        88.888889  \n",
       "83                15        88.235294  \n",
       "84                18       100.000000  \n",
       "85                40        72.727273  \n",
       "86                 9       100.000000  \n",
       "87                11       100.000000  \n",
       "88                24       100.000000  \n",
       "89                15        93.750000  \n",
       "90                21       100.000000  \n",
       "91                 5       100.000000  \n",
       "92                 5       100.000000  \n",
       "93                16        88.888889  \n",
       "94                21        87.500000  \n",
       "95                13       100.000000  \n",
       "96                39        86.666667  \n",
       "97                35        94.594595  \n",
       "98                21        84.000000  \n",
       "99                27        96.428571  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_data.csv', encoding = \"utf8\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[~df['label'].isnull()]\n",
    "test_df = df[df['label'].isnull()]\n",
    "\n",
    "train_comments = train_df['comment'].fillna(\"none\").values\n",
    "test_comments = test_df['comment'].fillna(\"none\").values\n",
    "\n",
    "y_train = train_df['label'].values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = './word2vec/wiki.vi.model.bin'\n",
    "#Load word2vec model\n",
    "if os.path.isfile(model):\n",
    "    print ('Loading word2vec model ...')\n",
    "if LooseVersion(gensim.__version__) >= LooseVersion(\"1.0.1\"):\n",
    "    from gensim.models import KeyedVectors\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(model, binary=True)\n",
    "else:\n",
    "    from gensim.models import Word2Vec\n",
    "    word2vec_model = Word2Vec.load_word2vec_format(model, binary=True)\n",
    "word2vec_model.wv.syn0.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from deepai_nlp.word_embedding.word2vec_gensim import BaseWord2Vec\n",
    "word2vec_model = BaseWord2Vec.load_model()\n",
    "word2vec_model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output = []\n",
    "try:\n",
    "    sim_list = word2vec_model.most_similar(\"thích\")\n",
    "    print(sim_list)\n",
    "    #output = word2vec_model.most_similar('u' + '\\\"' + 'A' + '\\\"', topn=5)\n",
    "\n",
    "    for wordsimilar in sim_list:\n",
    "        # output[wordsimilar[0]] = wordsimilar[1]\n",
    "        output.append(wordsimilar[0] + ' - '+ str(wordsimilar[1]))\n",
    "except:\n",
    "    print('except')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import emoji\n",
    "def extract_emojis(str):\n",
    "    return [c for c in str if c in emoji.UNICODE_EMOJI]\n",
    "emojis_vocab = []\n",
    "for r in train_data['review']:\n",
    "    emojis_vocab += extract_emojis(r)\n",
    "emojis_vocab = np.unique(np.asarray(emojis_vocab))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def getEmojiBowFeatures(reviews,vocab):\n",
    "    bow_emoji_features = []\n",
    "    for r in reviews:\n",
    "        emojis = extract_emojis(r)\n",
    "        bag_vector = np.zeros(len(vocab))\n",
    "        #print(emojis)\n",
    "        for e in emojis:\n",
    "            for i,emojii in enumerate(emojis_vocab):\n",
    "                if emojii == e: \n",
    "                    bag_vector[i] += 1\n",
    "        bow_emoji_features.append(bag_vector)\n",
    "    return np.asarray(bow_emoji_features)\n",
    "\n",
    "#getEmojiBowFeatures(\"💚💚😑💕💕\", emojis_vocab)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_data.review, train_data.label, test_size=0.2,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "num_features = 400\n",
    "clean_train_reviews = []\n",
    "for review in train_data.review:\n",
    "    clean_train_reviews.append(review_wordlist(review, remove_stopwords=False))\n",
    "bow_train_features = getEmojiBowFeatures(train_data.review, emojis_vocab)\n",
    "bow_train_features = sc.fit_transform(bow_train_features)\n",
    "#print(bow_train_features.shape)\n",
    "length = np.asarray([len(r) for r in train_data.review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-d77d74f946c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvietnamese_chars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-ce41740a2f42>\u001b[0m in \u001b[0;36mclean_text\u001b[1;34m(review, char_reg, stopwords)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# 5. Remove stopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mreview_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"\\b%s\\b\"\u001b[0m\u001b[1;33m%\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreview_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 192\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"first argument must be string or compiled pattern\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\sre_compile.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    762\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\sre_parse.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(str, flags, pattern)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mSRE_FLAG_VERBOSE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mVerbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         \u001b[1;31m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\sre_parse.py\u001b[0m in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m         itemsappend(_parse(source, state, verbose, nested + 1,\n\u001b[1;32m--> 426\u001b[1;33m                            not nested and not items))\n\u001b[0m\u001b[0;32m    427\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msourcematch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"|\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\sre_parse.py\u001b[0m in \u001b[0;36m_parse\u001b[1;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[0;32m    833\u001b[0m     \u001b[1;31m# unpack non-capturing groups\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m         \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mav\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubpattern\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mop\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mSUBPATTERN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m             \u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_flags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdel_flags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mav\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "for review in train_data.review:\n",
    "    X_train.append(clean_text(review, char_reg = vietnamese_chars, stopwords = stopwords))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = tfidf.fit_transform(train_data.review)\n",
    "X_test = tfidf.transform(test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 16087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 16087\n",
      "Review 2000 of 16087\n",
      "Review 3000 of 16087\n",
      "Review 4000 of 16087\n",
      "Review 5000 of 16087\n",
      "Review 6000 of 16087\n",
      "Review 7000 of 16087\n",
      "Review 8000 of 16087\n",
      "Review 9000 of 16087\n",
      "Review 10000 of 16087\n",
      "Review 11000 of 16087\n",
      "Review 12000 of 16087\n",
      "Review 13000 of 16087\n",
      "Review 14000 of 16087\n",
      "Review 15000 of 16087\n",
      "Review 16000 of 16087\n"
     ]
    }
   ],
   "source": [
    "X_train = getAvgFeatureVecs(clean_train_reviews, word2vec_model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = length.reshape(16087,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train,bow_train_features),axis=1)\n",
    "#X_train = np.concatenate((X_train,length),axis=1)\n",
    "\n",
    "y_train = train_data.label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating average feature vactors for test set     \n",
    "clean_test_reviews = []\n",
    "for review in test_data.review:\n",
    "    clean_test_reviews.append(review_wordlist(review))\n",
    "bow_test_features = getEmojiBowFeatures(test_data.review, emojis_vocab)\n",
    "bow_test_features = sc.fit_transform(bow_test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 10981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 10981\n",
      "Review 2000 of 10981\n",
      "Review 3000 of 10981\n",
      "Review 4000 of 10981\n",
      "Review 5000 of 10981\n",
      "Review 6000 of 10981\n",
      "Review 7000 of 10981\n",
      "Review 8000 of 10981\n",
      "Review 9000 of 10981\n",
      "Review 10000 of 10981\n"
     ]
    }
   ],
   "source": [
    "X_test = getAvgFeatureVecs(clean_test_reviews, word2vec_model, num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-270-68ec1a145f3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbow_test_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "X_test = np.concatenate((X_test,bow_test_features),axis=1)\n",
    "X_test = np.concatenate((X_test,length),axis=1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import collections\n",
    "f = clean_train_reviews\n",
    "f = np.concatenate(clean_train_reviews).ravel()\n",
    "wordcount = {}\n",
    "for word in f:\n",
    "    if word not in wordcount:\n",
    "        wordcount[word] = 1\n",
    "    else:\n",
    "        wordcount[word] += 1\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Print most common word\n",
    "word_counter = collections.Counter(wordcount)\n",
    "#for word, count in word_counter.most_common(500):\n",
    " #   print(word, \": \", count)\n",
    "most_common = []\n",
    "\n",
    "for word, count in word_counter.most_common(200):\n",
    "    most_common.append(word)\n",
    "most_common\n",
    "#word_counter.most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16087, 226)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "XX_train, X_val, yy_train, y_val = train_test_split(X_train, y_train, test_size=0.2,\n",
    "    random_state=42)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "forest.fit(pd.DataFrame(XX_train).fillna(0), yy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8104412678682411"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = forest.predict(pd.DataFrame(X_val).fillna(0))\n",
    "accuracy_score(y_val, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8505282784338098"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "XX_train, X_val, yy_train, y_val = train_test_split(X_train, y_train, test_size=0.2,\n",
    "    random_state=42)\n",
    "sc = StandardScaler()\n",
    "#XX_train = sc.fit_transform(XX_train)\n",
    "#X_test = sc.transform(X_test)\n",
    "clf = svm.SVC(gamma='scale',verbose=True)\n",
    "clf.fit(pd.DataFrame(XX_train).fillna(0), yy_train)\n",
    "y_predict = clf.predict(pd.DataFrame(X_val).fillna(0))\n",
    "accuracy_score(y_val, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12869, 525)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "d_train = lgb.Dataset(df, label=y_train)\n",
    "params = {}\n",
    "params['learning_rate'] = 0.003\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'binary'\n",
    "params['metric'] = 'binary_logloss'\n",
    "params['sub_feature'] = 0.5\n",
    "params['num_leaves'] = 80\n",
    "params['min_data'] = 50\n",
    "params['max_depth'] = 20\n",
    "clf = lgb.train(params, d_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LightGBM. Train shape: (16087, 226), test shape: (10981, 350)\n",
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.448914\ttrain's f1: 0.858152\tvalid's binary_logloss: 0.502944\tvalid's f1: 0.756535\n",
      "[200]\ttrain's binary_logloss: 0.347329\ttrain's f1: 0.893712\tvalid's binary_logloss: 0.440639\tvalid's f1: 0.779528\n",
      "[300]\ttrain's binary_logloss: 0.284272\ttrain's f1: 0.919402\tvalid's binary_logloss: 0.411849\tvalid's f1: 0.790167\n",
      "[400]\ttrain's binary_logloss: 0.23836\ttrain's f1: 0.940111\tvalid's binary_logloss: 0.394463\tvalid's f1: 0.793017\n",
      "[500]\ttrain's binary_logloss: 0.202597\ttrain's f1: 0.95693\tvalid's binary_logloss: 0.38381\tvalid's f1: 0.796013\n",
      "[600]\ttrain's binary_logloss: 0.173818\ttrain's f1: 0.97022\tvalid's binary_logloss: 0.376507\tvalid's f1: 0.799858\n",
      "[700]\ttrain's binary_logloss: 0.150022\ttrain's f1: 0.980539\tvalid's binary_logloss: 0.371869\tvalid's f1: 0.80427\n",
      "[800]\ttrain's binary_logloss: 0.130127\ttrain's f1: 0.988078\tvalid's binary_logloss: 0.369203\tvalid's f1: 0.804826\n",
      "[900]\ttrain's binary_logloss: 0.113292\ttrain's f1: 0.992835\tvalid's binary_logloss: 0.367151\tvalid's f1: 0.807379\n",
      "[1000]\ttrain's binary_logloss: 0.0989237\ttrain's f1: 0.995587\tvalid's binary_logloss: 0.366586\tvalid's f1: 0.806532\n",
      "[1100]\ttrain's binary_logloss: 0.0867645\ttrain's f1: 0.99715\tvalid's binary_logloss: 0.366177\tvalid's f1: 0.808797\n",
      "Early stopping, best iteration is:\n",
      "[1044]\ttrain's binary_logloss: 0.0933486\ttrain's f1: 0.996231\tvalid's binary_logloss: 0.366307\tvalid's f1: 0.810082\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.451073\ttrain's f1: 0.852468\tvalid's binary_logloss: 0.498411\tvalid's f1: 0.75\n",
      "[200]\ttrain's binary_logloss: 0.349822\ttrain's f1: 0.888769\tvalid's binary_logloss: 0.432897\tvalid's f1: 0.775687\n",
      "[300]\ttrain's binary_logloss: 0.287065\ttrain's f1: 0.915044\tvalid's binary_logloss: 0.40097\tvalid's f1: 0.790079\n",
      "[400]\ttrain's binary_logloss: 0.241092\ttrain's f1: 0.93713\tvalid's binary_logloss: 0.382077\tvalid's f1: 0.802867\n",
      "[500]\ttrain's binary_logloss: 0.205406\ttrain's f1: 0.956616\tvalid's binary_logloss: 0.370453\tvalid's f1: 0.811449\n",
      "[600]\ttrain's binary_logloss: 0.176468\ttrain's f1: 0.968827\tvalid's binary_logloss: 0.362851\tvalid's f1: 0.814204\n",
      "[700]\ttrain's binary_logloss: 0.152583\ttrain's f1: 0.980267\tvalid's binary_logloss: 0.357899\tvalid's f1: 0.816356\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-292-8025c1cdefad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlgb_f1_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     )\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    214\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   1758\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0;32m   1759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1760\u001b[1;33m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[0;32m   1761\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1762\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(X_train.shape, X_test.shape))\n",
    "\n",
    "# Cross validation model\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=69)\n",
    "\n",
    "# Create arrays and dataframes to store results\n",
    "oof_preds = np.zeros(X_train.shape[0])\n",
    "sub_preds = np.zeros(X_test.shape[0])\n",
    "\n",
    "# k-fold\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "    print(\"Fold %s\" % (n_fold))\n",
    "    train_x, train_y = X_train[train_idx], y_train[train_idx]\n",
    "    valid_x, valid_y = X_train[valid_idx], y_train[valid_idx]\n",
    "\n",
    "    # set data structure\n",
    "    lgb_train = lgb.Dataset(train_x,\n",
    "                            label=train_y,\n",
    "                            free_raw_data=False)\n",
    "    lgb_test = lgb.Dataset(valid_x,\n",
    "                           label=valid_y,\n",
    "                           free_raw_data=False)\n",
    "\n",
    "    params = {\n",
    "        'objective' :'binary',\n",
    "        'learning_rate' : 0.01,\n",
    "        'num_leaves' : 76,\n",
    "        'feature_fraction': 0.64, \n",
    "        'bagging_fraction': 0.8, \n",
    "        'bagging_freq':1,\n",
    "        'boosting_type' : 'gbdt',\n",
    "    }\n",
    "\n",
    "    reg = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_train, lgb_test],\n",
    "        valid_names=['train', 'valid'],\n",
    "        num_boost_round=10000,\n",
    "        verbose_eval=100,\n",
    "        early_stopping_rounds=100,\n",
    "        feval=lgb_f1_score\n",
    "    )\n",
    "\n",
    "    oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=reg.best_iteration)\n",
    "    sub_preds += reg.predict(X_test, num_iteration=reg.best_iteration) / folds.n_splits\n",
    "\n",
    "    del reg, train_x, train_y, valid_x, valid_y\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "preds = (sub_preds > threshold).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'] = preds\n",
    "test_data[['id','label']].to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 10981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 10981\n",
      "Review 2000 of 10981\n",
      "Review 3000 of 10981\n",
      "Review 4000 of 10981\n",
      "Review 5000 of 10981\n",
      "Review 6000 of 10981\n",
      "Review 7000 of 10981\n",
      "Review 8000 of 10981\n",
      "Review 9000 of 10981\n",
      "Review 10000 of 10981\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-3bb7ae807644>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mrealTestDataVecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetAvgFeatureVecs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_real_test_reviews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2vec_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mbow_train_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetEmojiBowFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memojis_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mrealTestDataVecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrealTestDataVecs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbow_train_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "# Calculating average feature vactors for test set     \n",
    "clean_real_test_reviews = []\n",
    "for review in test_data['review']:\n",
    "    clean_real_test_reviews.append(review_wordlist(review))\n",
    "    \n",
    "realTestDataVecs = getAvgFeatureVecs(clean_real_test_reviews, word2vec_model, num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_train_features = getEmojiBowFeatures(test_data['review'], emojis_vocab)\n",
    "realTestDataVecs = np.concatenate((realTestDataVecs,bow_train_features),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = realTestDataVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = clf.predict(pd.DataFrame(realTestDataVecs).fillna(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'] = y_predict\n",
    "test_data[['id','label']].to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
