{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk.data\n",
    "import gensim\n",
    "from distutils.version import LooseVersion, StrictVersion\n",
    "import os\n",
    "import codecs\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "global word2vec_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSource(object):\n",
    "    def _load_raw_data(self,filename, is_train=True):\n",
    "        a = []\n",
    "        b = []\n",
    "        regex = 'train_'\n",
    "        if not is_train:\n",
    "            regex = 'test_'\n",
    "        with open(filename, 'r', encoding=\"utf8\") as file:\n",
    "            for line in file :\n",
    "                if regex in line:\n",
    "                    b.append(a)\n",
    "                    a = [line]\n",
    "                elif line!='\\n':\n",
    "                    a.append(line)       \n",
    "        b.append(a)      \n",
    "        return b[1:]\n",
    "    \n",
    "    def _create_row(self, sample, is_train=True):\n",
    "        d = {}\n",
    "        d['id'] = sample[0].replace('\\n','')\n",
    "        review = \"\"\n",
    "        if is_train:\n",
    "            for clause in sample[1:-1]:\n",
    "                review+= clause.replace('\\n','').strip()\n",
    "            d['label'] = int(sample[-1].replace('\\n',''))          \n",
    "        else:         \n",
    "            for clause in sample[1:]:\n",
    "                review+= clause.replace('\\n','').strip()\n",
    "        d['review'] = review\n",
    "        return d\n",
    "    \n",
    "    \n",
    "    def load_data(self, filename, is_train=True):\n",
    "        raw_data = self._load_raw_data(filename, is_train)\n",
    "        lst = []\n",
    "        for row in raw_data:\n",
    "            lst.append(self._create_row(row, is_train))\n",
    "        return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load stopwords\n",
    "stopwords_file = 'vietnamese-stopwords.txt'\n",
    "stopwords = []\n",
    "with open(stopwords_file, 'r', encoding=\"utf8\") as file:\n",
    "    for line in file :\n",
    "        stopwords.append(line.replace('\\n','').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "vietnamese_chars = \"[^a-z0-9A-Z_√Ä√Å√Ç√É√à√â√ä√å√ç√í√ì√î√ï√ô√öƒÇƒêƒ®≈®∆†√†√°√¢√£√®√©√™√¨√≠√≤√≥√¥√µ√π√∫ƒÉƒëƒ©≈©∆°∆ØƒÇ·∫†·∫¢·∫§·∫¶·∫®·∫™·∫¨·∫Æ·∫∞·∫≤·∫¥·∫∂·∫∏·∫∫·∫º·ªÄ·ªÄ·ªÇ∆∞ƒÉ·∫°·∫£·∫•·∫ß·∫©·∫´·∫≠·∫Ø·∫±·∫≥·∫µ·∫∑·∫π·∫ª·∫Ω·ªÅ·ªÅ·ªÉ·ªÑ·ªÜ·ªà·ªä·ªå·ªé·ªê·ªí·ªî·ªñ·ªò·ªö·ªú·ªû·ª†·ª¢·ª§·ª¶·ª®·ª™·ªÖ·ªá·ªâ·ªã·ªç·ªè·ªë·ªì·ªï·ªó·ªô·ªõ·ªù·ªü·ª°·ª£·ª•·ªß·ª©·ª´·ª¨·ªÆ·ª∞·ª≤·ª¥√ù·ª∂·ª∏·ª≠·ªØ·ª±·ª≥·ªµ·ª∑·ªπ]\"\n",
    "def review_wordlist(review, remove_stopwords= False):\n",
    "    review_text = str(review)\n",
    "    # 2. Removing non-letter.\n",
    "    review_text = re.sub(vietnamese_chars,\" \",review_text)\n",
    "    # 3. Converting to lower case and splitting\n",
    "    words = review_text.lower().split()\n",
    "    # 4. Optionally remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords)     \n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DataSource()\n",
    "train_data = pd.DataFrame(ds.load_data('dataset/train.crash'))\n",
    "test_data = pd.DataFrame(ds.load_data('dataset/test.crash', is_train=False))\n",
    "train_data['review'] = train_data['review'].fillna(\"none\")\n",
    "test_data['review'] = test_data['review'].fillna(\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_000000</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Dung dc sp tot cam onshop ƒê√≥ng g√≥i s·∫£n ph·∫©m r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_000001</td>\n",
       "      <td>0</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi . Son m·ªãn nh∆∞n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_000002</td>\n",
       "      <td>0</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi nh∆∞ng k c√≥ h·ªôp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_000003</td>\n",
       "      <td>1</td>\n",
       "      <td>\":(( M√¨nh h∆°i th·∫•t v·ªçng 1 ch√∫t v√¨ m√¨nh ƒë√£ k·ª≥ v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_000004</td>\n",
       "      <td>1</td>\n",
       "      <td>\"L·∫ßn tr∆∞·ªõc m√¨nh mua √°o gi√≥ m√†u h·ªìng r·∫•t ok m√† ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  label                                             review\n",
       "0  train_000000      0  \"Dung dc sp tot cam onshop ƒê√≥ng g√≥i s·∫£n ph·∫©m r...\n",
       "1  train_000001      0  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi . Son m·ªãn nh∆∞n...\n",
       "2  train_000002      0  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi nh∆∞ng k c√≥ h·ªôp...\n",
       "3  train_000003      1  \":(( M√¨nh h∆°i th·∫•t v·ªçng 1 ch√∫t v√¨ m√¨nh ƒë√£ k·ª≥ v...\n",
       "4  train_000004      1  \"L·∫ßn tr∆∞·ªõc m√¨nh mua √°o gi√≥ m√†u h·ªìng r·∫•t ok m√† ..."
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "def extract_emojis(str):\n",
    "    return [c for c in str if c in emoji.UNICODE_EMOJI]\n",
    "\n",
    "good_df = train_data[train_data['label'] == 0]\n",
    "good_comment = good_df['review'].values\n",
    "good_emoji = []\n",
    "for c in good_comment:\n",
    "      good_emoji += extract_emojis(c)\n",
    "\n",
    "good_emoji = np.unique(np.asarray(good_emoji))\n",
    "\n",
    "\n",
    "bad_df = train_data[train_data['label'] == 1]\n",
    "bad_comment = bad_df['review'].values\n",
    "\n",
    "bad_emoji = []\n",
    "for c in bad_comment:\n",
    "    bad_emoji += extract_emojis(c)\n",
    "\n",
    "bad_emoji = np.unique(np.asarray(bad_emoji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-20 16:00:47,010 : INFO : loading projection weights from ./word2vec/wiki.vi.model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vec model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-20 16:01:04,044 : INFO : loaded (231486, 400) matrix from ./word2vec/wiki.vi.model.bin\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(231486, 400)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = './word2vec/wiki.vi.model.bin'\n",
    "#Load word2vec model\n",
    "if os.path.isfile(model):\n",
    "    print ('Loading word2vec model ...')\n",
    "if LooseVersion(gensim.__version__) >= LooseVersion(\"1.0.1\"):\n",
    "    from gensim.models import KeyedVectors\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(model, binary=True)\n",
    "else:\n",
    "    from gensim.models import Word2Vec\n",
    "    word2vec_model = Word2Vec.load_word2vec_format(model, binary=True)\n",
    "word2vec_model.wv.syn0.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-20 16:01:31,856 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('t·ªÖu', 0.5424543023109436), ('chu·ªông', 0.5406407117843628), ('tho√≤ng', 0.5046262741088867), ('ch·ªông', 0.4467487633228302), ('hydrophilic', 0.44052204489707947), ('√∞√°ng', 0.3866061270236969), ('karlspreis', 0.3651212751865387), ('thik', 0.35866597294807434), ('c√∫n', 0.3559001088142395), ('ghi', 0.3471173048019409)]\n",
      "['t·ªÖu - 0.5424543023109436', 'chu·ªông - 0.5406407117843628', 'tho√≤ng - 0.5046262741088867', 'ch·ªông - 0.4467487633228302', 'hydrophilic - 0.44052204489707947', '√∞√°ng - 0.3866061270236969', 'karlspreis - 0.3651212751865387', 'thik - 0.35866597294807434', 'c√∫n - 0.3559001088142395', 'ghi - 0.3471173048019409']\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "try:\n",
    "    sim_list = word2vec_model.most_similar(\"th√≠ch\")\n",
    "    print(sim_list)\n",
    "    #output = word2vec_model.most_similar('u' + '\\\"' + 'A' + '\\\"', topn=5)\n",
    "\n",
    "    for wordsimilar in sim_list:\n",
    "        # output[wordsimilar[0]] = wordsimilar[1]\n",
    "        output.append(wordsimilar[0] + ' - '+ str(wordsimilar[1]))\n",
    "except:\n",
    "    print('except')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "def extract_emojis(str):\n",
    "    return [c for c in str if c in emoji.UNICODE_EMOJI]\n",
    "emojis_vocab = []\n",
    "for r in train_data['review']:\n",
    "    emojis_vocab += extract_emojis(r)\n",
    "emojis_vocab = np.unique(np.asarray(emojis_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmojiBowFeatures(reviews,vocab):\n",
    "    bow_emoji_features = []\n",
    "    for r in reviews:\n",
    "        emojis = extract_emojis(r)\n",
    "        bag_vector = np.zeros(len(vocab))\n",
    "        # print(emojis_bow)\n",
    "        for e in emojis_bow:\n",
    "            for i,emojii in enumerate(emojis):\n",
    "                if emojii == e: \n",
    "                    bag_vector[i] += 1\n",
    "        bow_emoji_features.append(bag_vector)\n",
    "    return np.asarray(bow_emoji_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_data.review, train_data.label, test_size=0.2,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 400\n",
    "clean_train_reviews = []\n",
    "for review in x_train:\n",
    "    clean_train_reviews.append(review_wordlist(review, remove_stopwords=False))\n",
    "bow_train_features = getEmojiBowFeatures(x_train, emojis_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 11260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-138-107f4df5629d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainDataVecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetAvgFeatureVecs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_train_reviews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2vec_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-127-55a22da5ecf5>\u001b[0m in \u001b[0;36mgetAvgFeatureVecs\u001b[1;34m(reviews, model, num_features)\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Review %d of %d\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mreviewFeatureVecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatureVecMethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcounter\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-126-3a0cc7e3eff1>\u001b[0m in \u001b[0;36mfeatureVecMethod\u001b[1;34m(words, model, num_features)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindex2word_set\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mnwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnwords\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mfeatureVec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureVec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Dividing the result by number of words to get average\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    267\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors_norm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetflags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, word2vec_model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating average feature vactors for test set     \n",
    "clean_test_reviews = []\n",
    "for review in x_val:\n",
    "    clean_test_reviews.append(review_wordlist(review))\n",
    "bow_train_features = getEmojiBowFeatures(x_val, emojis_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 4827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 4827\n",
      "Review 2000 of 4827\n",
      "Review 3000 of 4827\n",
      "Review 4000 of 4827\n"
     ]
    }
   ],
   "source": [
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews, word2vec_model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(trainDataVecs)\n",
    "forest.fit(pd.DataFrame(trainDataVecs).fillna(0), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = forest.predict(pd.DataFrame(testDataVecs).fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8288792210482702"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_val, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(gamma='scale',verbose=True)\n",
    "clf.fit(pd.DataFrame(trainDataVecs).fillna(0), y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = clf.predict(pd.DataFrame(testDataVecs).fillna(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8713486637663145"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_val, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_000000</td>\n",
       "      <td>\"Ch∆∞a d√πng th·ª≠ n√™n ch∆∞a bi·∫øt\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_000001</td>\n",
       "      <td>\" Kh√¥ng ƒë√°ng ti·ªÅnV√¨ ngay ƒë·ª£t sale n√™n m·ªõi mua ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_000002</td>\n",
       "      <td>\"C√°m ∆°n shop. ƒê√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Ø...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_000003</td>\n",
       "      <td>\"V·∫£i ƒë·∫πp.phom oki lu√¥n.qu√° ∆∞ng\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_000004</td>\n",
       "      <td>\"Chu·∫©n h√†ng ƒë√≥ng g√≥i ƒë·∫πp\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test_000005</td>\n",
       "      <td>\" ƒê√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn Shop ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test_000006</td>\n",
       "      <td>\"Sau khi ƒëoÃ£c xong cu√¥ÃÅn truy√™Ã£n thiÃÄ caÃâm xuÃÅ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test_000007</td>\n",
       "      <td>\"Ch·ªâ c·∫£m ·ª©ng khi g·∫ßn d√¢y ƒëi·ªán ·ªï c·∫Øm ko c√≥ v·∫≠t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test_000008</td>\n",
       "      <td>\"T·ªáüò° S·∫£n ph·∫©m ƒë·ª©t ch·ªâ t√πm lumüò° R√°ch qu√° tr·ªùi c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>test_000009</td>\n",
       "      <td>\"Shop  Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m r·∫•t k√©m Shop ph·ª•c v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>test_000010</td>\n",
       "      <td>\"Ad ch·ªâ em c√°ch ch·ªânh ng√†y vs\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>test_000011</td>\n",
       "      <td>\"C·∫Øm ph√°t nh·∫≠n lu√¥n\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>test_000012</td>\n",
       "      <td>\"Ch·∫•t li·ªáu t·ªët g√≥i h√†ng ch·∫Øc ch·∫Øn s·∫£n ph·∫©m ch·∫•...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>test_000013</td>\n",
       "      <td>\"Da m√¨nh l√† h·ªón h·ª£p thi√™n d·∫ßu nh·∫°y c·∫£m ¬†sau kh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>test_000014</td>\n",
       "      <td>\"D√π rep ib h∆°i ch·∫≠m nh∆∞ng ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>test_000015</td>\n",
       "      <td>\"Ban ƒë·∫ßu mua v·ªÅ m·∫´u m√£ th√¨ ƒë·∫πp  nh∆∞ng kh√¥ng ƒë∆∞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>test_000016</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi Ch·∫•t l∆∞·ª£ng s·∫£n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>test_000017</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi. H√†ng test ra ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>test_000018</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi üíñƒê√≥ng g√≥i s·∫£n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>test_000019</td>\n",
       "      <td>\"Size h∆°i nh·ªè so v·ªõi s·ªë k√Ω.¬† Ch·∫•t l∆∞·ª£ng s·∫£n ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>test_000020</td>\n",
       "      <td>\"Shop l√†m vi·ªác an t√¢mc√≥ l√≤ng vs h√†ng r·∫•t t·ªët....\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>test_000021</td>\n",
       "      <td>\"Shop chuy√™n ch·ªânh gi√° v·ªÅ 20k v√† l·∫≠p nick r√°c ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>test_000022</td>\n",
       "      <td>\"Gi√†y c·ª±c k√¨ ok.... L√∫c ƒë·∫ßu mua c≈©ng s·ª£ da b·ªã ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>test_000023</td>\n",
       "      <td>\"H∆°i b·ªã m·∫∑n\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>test_000024</td>\n",
       "      <td>\"S·∫£n ph·∫©m m·ªõi mua m√† ƒë√£ m·∫•t v√¢n tay.ch∆°i game ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>test_000025</td>\n",
       "      <td>\"s·∫£n ph·∫©m tr√™n h√¨nh ch·ªâ l√† minh h·ªça ch·ª© ngo√†i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>test_000026</td>\n",
       "      <td>\"Y√™u d√£ man lu√¥n √≠. 5 m√†u m√†u n√†o c≈©ng ƒë·∫πp ko ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>test_000027</td>\n",
       "      <td>\"Tien nao cua do tam chap nhan dc.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>test_000028</td>\n",
       "      <td>\"ƒê·ªìng h·ªì gi·ªëng h√¨nh..nh·ªè nh·ªè xinh xinh... ƒê√≥ng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>test_000029</td>\n",
       "      <td>\"Ph·ª•c v·ª• r·∫•t k√©m! ƒê√£ ph√¢n lo·∫°i cho kh√°ch th√¨ g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10951</th>\n",
       "      <td>test_010951</td>\n",
       "      <td>\" Th·ªùi gian giao h√†ng r·∫•t nhanh. L·∫ßn ƒë·∫ßu mua h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10952</th>\n",
       "      <td>test_010952</td>\n",
       "      <td>\"G√† ch∆∞a v√†ng.gia v·ªã ch∆∞a th·∫•m.ƒë·∫∑t lo·∫°i cay m√†...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10953</th>\n",
       "      <td>test_010953</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒê√≥ng g√≥i s·∫£n p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10954</th>\n",
       "      <td>test_010954</td>\n",
       "      <td>\"S·ªØa t·∫Øm kh√¥ng th∆°m l·∫Øm giao h√†ng nhanh ƒë√≥ng g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10955</th>\n",
       "      <td>test_010955</td>\n",
       "      <td>\"M·ªõi giao m√† n√∫t chai d·∫ßu g·ªôi ƒë√£ b·ªã g√£y¬† Ch·∫•t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10956</th>\n",
       "      <td>test_010956</td>\n",
       "      <td>\"Giao h√†ng nhanh. Shop nhjet t√¨nh. M√°y th√¨ si√™...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10957</th>\n",
       "      <td>test_010957</td>\n",
       "      <td>\" Shop ph·ª•c v·ª• nhi·ªát t√¨nh \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10958</th>\n",
       "      <td>test_010958</td>\n",
       "      <td>\"S·∫£n ph·∫©m b·ªã m√≥p khi v·∫≠n chuy·ªÉn nh·∫Øn tin shop ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10959</th>\n",
       "      <td>test_010959</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒê√≥ng g√≥i s·∫£n p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10960</th>\n",
       "      <td>test_010960</td>\n",
       "      <td>\"C·ªëm ngon tuy·ªát......\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10961</th>\n",
       "      <td>test_010961</td>\n",
       "      <td>\"G√≥i h√†ng k√©m\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10962</th>\n",
       "      <td>test_010962</td>\n",
       "      <td>\"Chu·∫©n m·∫´u.  \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10963</th>\n",
       "      <td>test_010963</td>\n",
       "      <td>\"shop lam an k c√≥ t√¢m.ƒë·∫∑t ƒë∆°n 99k thi giao han...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10964</th>\n",
       "      <td>test_010964</td>\n",
       "      <td>\"Nh·∫≠n h√†ng xong l√† d√πng th·ª≠ lu√¥n c·∫£m gi√°c ban ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10965</th>\n",
       "      <td>test_010965</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒëuÃÅng nh∆∞ hiÃÄnh \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10966</th>\n",
       "      <td>test_010966</td>\n",
       "      <td>\"Ko th∆°m b·∫±ng lo·∫°i m√†u h·ªìng mua ·ªü bibomar\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10967</th>\n",
       "      <td>test_010967</td>\n",
       "      <td>\"N∆∞·ªõc gi·∫∑t kh√¥ng c√≥ tem ch√≠nh h√£ng nh√£n d√°n l·ªè...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10968</th>\n",
       "      <td>test_010968</td>\n",
       "      <td>\"H√¥m nay m√¨nh xin kh√¥ng h√†i l√≤ng vs nty shop v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10969</th>\n",
       "      <td>test_010969</td>\n",
       "      <td>\"Shop g√≥i h√†ng si√™u kƒ© ^^\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10970</th>\n",
       "      <td>test_010970</td>\n",
       "      <td>\"Giao h√†ng l√¢u. Sai m√†u . Nhanh tr√¥i. H√≥ng m√£i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10971</th>\n",
       "      <td>test_010971</td>\n",
       "      <td>\"S·∫£n ph·∫©m ƒë·∫πp ƒë√∫ng nh∆∞ h√¨nh.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10972</th>\n",
       "      <td>test_010972</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m r·∫•t k√©m. Kh bi·∫øt t·∫°i t x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10973</th>\n",
       "      <td>test_010973</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒê√≥ng g√≥i s·∫£n p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10974</th>\n",
       "      <td>test_010974</td>\n",
       "      <td>\"Shop ph·ª•c v·ª• r·∫•t t·ªët. \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10975</th>\n",
       "      <td>test_010975</td>\n",
       "      <td>\"B√© m·∫∑c ko v·ª´a m√¨nh mu·ªën ƒë·ªïi size thanks shop\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10976</th>\n",
       "      <td>test_010976</td>\n",
       "      <td>\" Th·ªùi gian giao h√†ng r·∫•t nhanh.ngon.m√† cay qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10977</th>\n",
       "      <td>test_010977</td>\n",
       "      <td>\"S·∫£n ph·∫©m h∆°i c≈©\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10978</th>\n",
       "      <td>test_010978</td>\n",
       "      <td>\"S·∫£n ph·∫©m ch·∫Øc ch·∫Øn nh∆∞ng k b√≥ng b·∫±ng trong h√¨nh\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10979</th>\n",
       "      <td>test_010979</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi c√≥ m√πi th∆°m r·∫•...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10980</th>\n",
       "      <td>test_010980</td>\n",
       "      <td>\"nh∆∞ qu·∫£ng c√°o. sim r·∫•t t·ªët\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10981 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                             review\n",
       "0      test_000000                      \"Ch∆∞a d√πng th·ª≠ n√™n ch∆∞a bi·∫øt\"\n",
       "1      test_000001  \" Kh√¥ng ƒë√°ng ti·ªÅnV√¨ ngay ƒë·ª£t sale n√™n m·ªõi mua ...\n",
       "2      test_000002  \"C√°m ∆°n shop. ƒê√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Ø...\n",
       "3      test_000003                    \"V·∫£i ƒë·∫πp.phom oki lu√¥n.qu√° ∆∞ng\"\n",
       "4      test_000004                          \"Chu·∫©n h√†ng ƒë√≥ng g√≥i ƒë·∫πp\"\n",
       "5      test_000005  \" ƒê√≥ng g√≥i s·∫£n ph·∫©m r·∫•t ƒë·∫πp v√† ch·∫Øc ch·∫Øn Shop ...\n",
       "6      test_000006  \"Sau khi ƒëoÃ£c xong cu√¥ÃÅn truy√™Ã£n thiÃÄ caÃâm xuÃÅ...\n",
       "7      test_000007  \"Ch·ªâ c·∫£m ·ª©ng khi g·∫ßn d√¢y ƒëi·ªán ·ªï c·∫Øm ko c√≥ v·∫≠t ...\n",
       "8      test_000008  \"T·ªáüò° S·∫£n ph·∫©m ƒë·ª©t ch·ªâ t√πm lumüò° R√°ch qu√° tr·ªùi c...\n",
       "9      test_000009  \"Shop  Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m r·∫•t k√©m Shop ph·ª•c v...\n",
       "10     test_000010                     \"Ad ch·ªâ em c√°ch ch·ªânh ng√†y vs\"\n",
       "11     test_000011                               \"C·∫Øm ph√°t nh·∫≠n lu√¥n\"\n",
       "12     test_000012  \"Ch·∫•t li·ªáu t·ªët g√≥i h√†ng ch·∫Øc ch·∫Øn s·∫£n ph·∫©m ch·∫•...\n",
       "13     test_000013  \"Da m√¨nh l√† h·ªón h·ª£p thi√™n d·∫ßu nh·∫°y c·∫£m ¬†sau kh...\n",
       "14     test_000014  \"D√π rep ib h∆°i ch·∫≠m nh∆∞ng ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m ...\n",
       "15     test_000015  \"Ban ƒë·∫ßu mua v·ªÅ m·∫´u m√£ th√¨ ƒë·∫πp  nh∆∞ng kh√¥ng ƒë∆∞...\n",
       "16     test_000016  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi Ch·∫•t l∆∞·ª£ng s·∫£n...\n",
       "17     test_000017  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi. H√†ng test ra ...\n",
       "18     test_000018  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi üíñƒê√≥ng g√≥i s·∫£n ...\n",
       "19     test_000019  \"Size h∆°i nh·ªè so v·ªõi s·ªë k√Ω.¬† Ch·∫•t l∆∞·ª£ng s·∫£n ph...\n",
       "20     test_000020  \"Shop l√†m vi·ªác an t√¢mc√≥ l√≤ng vs h√†ng r·∫•t t·ªët....\"\n",
       "21     test_000021  \"Shop chuy√™n ch·ªânh gi√° v·ªÅ 20k v√† l·∫≠p nick r√°c ...\n",
       "22     test_000022  \"Gi√†y c·ª±c k√¨ ok.... L√∫c ƒë·∫ßu mua c≈©ng s·ª£ da b·ªã ...\n",
       "23     test_000023                                       \"H∆°i b·ªã m·∫∑n\"\n",
       "24     test_000024  \"S·∫£n ph·∫©m m·ªõi mua m√† ƒë√£ m·∫•t v√¢n tay.ch∆°i game ...\n",
       "25     test_000025  \"s·∫£n ph·∫©m tr√™n h√¨nh ch·ªâ l√† minh h·ªça ch·ª© ngo√†i ...\n",
       "26     test_000026  \"Y√™u d√£ man lu√¥n √≠. 5 m√†u m√†u n√†o c≈©ng ƒë·∫πp ko ...\n",
       "27     test_000027                \"Tien nao cua do tam chap nhan dc.\"\n",
       "28     test_000028  \"ƒê·ªìng h·ªì gi·ªëng h√¨nh..nh·ªè nh·ªè xinh xinh... ƒê√≥ng...\n",
       "29     test_000029  \"Ph·ª•c v·ª• r·∫•t k√©m! ƒê√£ ph√¢n lo·∫°i cho kh√°ch th√¨ g...\n",
       "...            ...                                                ...\n",
       "10951  test_010951  \" Th·ªùi gian giao h√†ng r·∫•t nhanh. L·∫ßn ƒë·∫ßu mua h...\n",
       "10952  test_010952  \"G√† ch∆∞a v√†ng.gia v·ªã ch∆∞a th·∫•m.ƒë·∫∑t lo·∫°i cay m√†...\n",
       "10953  test_010953  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒê√≥ng g√≥i s·∫£n p...\n",
       "10954  test_010954  \"S·ªØa t·∫Øm kh√¥ng th∆°m l·∫Øm giao h√†ng nhanh ƒë√≥ng g...\n",
       "10955  test_010955  \"M·ªõi giao m√† n√∫t chai d·∫ßu g·ªôi ƒë√£ b·ªã g√£y¬† Ch·∫•t ...\n",
       "10956  test_010956  \"Giao h√†ng nhanh. Shop nhjet t√¨nh. M√°y th√¨ si√™...\n",
       "10957  test_010957                        \" Shop ph·ª•c v·ª• nhi·ªát t√¨nh \"\n",
       "10958  test_010958  \"S·∫£n ph·∫©m b·ªã m√≥p khi v·∫≠n chuy·ªÉn nh·∫Øn tin shop ...\n",
       "10959  test_010959  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒê√≥ng g√≥i s·∫£n p...\n",
       "10960  test_010960                             \"C·ªëm ngon tuy·ªát......\"\n",
       "10961  test_010961                                     \"G√≥i h√†ng k√©m\"\n",
       "10962  test_010962                                     \"Chu·∫©n m·∫´u.  \"\n",
       "10963  test_010963  \"shop lam an k c√≥ t√¢m.ƒë·∫∑t ƒë∆°n 99k thi giao han...\n",
       "10964  test_010964  \"Nh·∫≠n h√†ng xong l√† d√πng th·ª≠ lu√¥n c·∫£m gi√°c ban ...\n",
       "10965  test_010965  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒëuÃÅng nh∆∞ hiÃÄnh \"\n",
       "10966  test_010966         \"Ko th∆°m b·∫±ng lo·∫°i m√†u h·ªìng mua ·ªü bibomar\"\n",
       "10967  test_010967  \"N∆∞·ªõc gi·∫∑t kh√¥ng c√≥ tem ch√≠nh h√£ng nh√£n d√°n l·ªè...\n",
       "10968  test_010968  \"H√¥m nay m√¨nh xin kh√¥ng h√†i l√≤ng vs nty shop v...\n",
       "10969  test_010969                         \"Shop g√≥i h√†ng si√™u kƒ© ^^\"\n",
       "10970  test_010970  \"Giao h√†ng l√¢u. Sai m√†u . Nhanh tr√¥i. H√≥ng m√£i...\n",
       "10971  test_010971                      \"S·∫£n ph·∫©m ƒë·∫πp ƒë√∫ng nh∆∞ h√¨nh.\"\n",
       "10972  test_010972  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m r·∫•t k√©m. Kh bi·∫øt t·∫°i t x...\n",
       "10973  test_010973  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi ƒê√≥ng g√≥i s·∫£n p...\n",
       "10974  test_010974                           \"Shop ph·ª•c v·ª• r·∫•t t·ªët. \"\n",
       "10975  test_010975     \"B√© m·∫∑c ko v·ª´a m√¨nh mu·ªën ƒë·ªïi size thanks shop\"\n",
       "10976  test_010976  \" Th·ªùi gian giao h√†ng r·∫•t nhanh.ngon.m√† cay qu...\n",
       "10977  test_010977                                  \"S·∫£n ph·∫©m h∆°i c≈©\"\n",
       "10978  test_010978  \"S·∫£n ph·∫©m ch·∫Øc ch·∫Øn nh∆∞ng k b√≥ng b·∫±ng trong h√¨nh\"\n",
       "10979  test_010979  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi c√≥ m√πi th∆°m r·∫•...\n",
       "10980  test_010980                       \"nh∆∞ qu·∫£ng c√°o. sim r·∫•t t·ªët\"\n",
       "\n",
       "[10981 rows x 2 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 10981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 10981\n",
      "Review 2000 of 10981\n",
      "Review 3000 of 10981\n",
      "Review 4000 of 10981\n",
      "Review 5000 of 10981\n",
      "Review 6000 of 10981\n",
      "Review 7000 of 10981\n",
      "Review 8000 of 10981\n",
      "Review 9000 of 10981\n",
      "Review 10000 of 10981\n"
     ]
    }
   ],
   "source": [
    "# Calculating average feature vactors for test set     \n",
    "clean_real_test_reviews = []\n",
    "for review in test_data['review']:\n",
    "    clean_real_test_reviews.append(review_wordlist(review))\n",
    "    \n",
    "realTestDataVecs = getAvgFeatureVecs(clean_real_test_reviews, word2vec_model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = clf.predict(pd.DataFrame(realTestDataVecs).fillna(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'] = y_predict\n",
    "test_data[['id','label']].to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
