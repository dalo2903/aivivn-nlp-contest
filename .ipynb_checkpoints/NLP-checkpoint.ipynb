{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk.data\n",
    "import gensim\n",
    "from distutils.version import LooseVersion, StrictVersion\n",
    "import os\n",
    "import codecs\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "global word2vec_model\n",
    "from deepai_nlp.tokenization.crf_tokenizer import CrfTokenizer\n",
    "from deepai_nlp.tokenization.utils import preprocess_text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/github/ngxbac/aivivn_phanloaisacthaibinhluan/blob/master/baseline_lgbm_tfidf.ipynb#scrollTo=ojmynVfZtFRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSource(object):\n",
    "    def _load_raw_data(self,filename, is_train=True):\n",
    "        a = []\n",
    "        b = []\n",
    "        regex = 'train_'\n",
    "        if not is_train:\n",
    "            regex = 'test_'\n",
    "        with open(filename, 'r', encoding=\"utf8\") as file:\n",
    "            for line in file :\n",
    "                if regex in line:\n",
    "                    b.append(a)\n",
    "                    a = [line]\n",
    "                elif line!='\\n':\n",
    "                    a.append(line)       \n",
    "        b.append(a)      \n",
    "        return b[1:]\n",
    "    \n",
    "    def _create_row(self, sample, is_train=True):\n",
    "        d = {}\n",
    "        d['id'] = sample[0].replace('\\n','')\n",
    "        review = \"\"\n",
    "        if is_train:\n",
    "            for clause in sample[1:-1]:\n",
    "                review+= clause.replace('\\n','').strip()\n",
    "            d['label'] = int(sample[-1].replace('\\n',''))          \n",
    "        else:         \n",
    "            for clause in sample[1:]:\n",
    "                review+= clause.replace('\\n','').strip()\n",
    "        d['review'] = review\n",
    "        return d\n",
    "    \n",
    "    \n",
    "    def load_data(self, filename, is_train=True):\n",
    "        raw_data = self._load_raw_data(filename, is_train)\n",
    "        lst = []\n",
    "        for row in raw_data:\n",
    "            lst.append(self._create_row(row, is_train))\n",
    "        return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load stopwords\n",
    "stopwords_file = 'vietnamese-stopwords.txt'\n",
    "stopwords = []\n",
    "with open(stopwords_file, 'r', encoding=\"utf8\") as file:\n",
    "    for line in file :\n",
    "        stopwords.append(line.replace('\\n','').strip())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "search = \"d you don't need a dog. but if you like dogs, you should think of getting one for your own. Or a cat?\"\n",
    "mapping =  {\"you\": \"aaaaa\", r\"\\bd\\b\": \"!!!\" }\n",
    "\n",
    "#search = [search.replace(key, value) for key, value in mapping.items()][0]\n",
    "\n",
    "for key, value in mapping.items():\n",
    "    search = re.sub(key,value,search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"ship\": \"v·∫≠n chuy·ªÉn\",\n",
    "    \"shop\": \"c·ª≠a h√†ng\",\n",
    "    \"sp\": \"s·∫£n ph·∫©m\",\n",
    "    r\"\\bm\\b\": \" m√¨nh\",\n",
    "    \"mik\": \"m√¨nh\",\n",
    "    r\"\\bk\\b\": \"kh√¥ng\",\n",
    "    r\"\\bkh\\b\": \"kh√¥ng\",\n",
    "    r\"\\btl\\b\": \"tr·∫£ l·ªùi\",\n",
    "    r\"\\br\\b\": \"r·ªìi\",\n",
    "    \"fb\": \"m·∫°ng x√£ h·ªôi \", # facebook\n",
    "    \"face\": \"m·∫°ng x√£ h·ªôi\",\n",
    "    \"thanks\": \"c·∫£m ∆°n\",\n",
    "    \"thank\": \"c·∫£m ∆°n\",\n",
    "    \"tks\": \"c·∫£m ∆°n\", \n",
    "    r\"\\bdc\\b\": \"ƒë∆∞·ª£c\",\n",
    "    r\"\\bok\\b\": \"t·ªët\",\n",
    "    r\"\\bdt\\b\": \"ƒëi·ªán tho·∫°i\",\n",
    "    r\"\\bh\\b\": \"gi·ªù\"\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vietnamese_chars = \"[^a-z0-9A-Z_√Ä√Å√Ç√É√à√â√ä√å√ç√í√ì√î√ï√ô√öƒÇƒêƒ®≈®∆†√†√°√¢√£√®√©√™√¨√≠√≤√≥√¥√µ√π√∫ƒÉƒëƒ©≈©∆°∆ØƒÇ·∫†·∫¢·∫§·∫¶·∫®·∫™·∫¨·∫Æ·∫∞·∫≤·∫¥·∫∂·∫∏·∫∫·∫º·ªÄ·ªÄ·ªÇ∆∞ƒÉ·∫°·∫£·∫•·∫ß·∫©·∫´·∫≠·∫Ø·∫±·∫≥·∫µ·∫∑·∫π·∫ª·∫Ω·ªÅ·ªÅ·ªÉ·ªÑ·ªÜ·ªà·ªä·ªå·ªé·ªê·ªí·ªî·ªñ·ªò·ªö·ªú·ªû·ª†·ª¢·ª§·ª¶·ª®·ª™·ªÖ·ªá·ªâ·ªã·ªç·ªè·ªë·ªì·ªï·ªó·ªô·ªõ·ªù·ªü·ª°·ª£·ª•·ªß·ª©·ª´·ª¨·ªÆ·ª∞·ª≤·ª¥√ù·ª∂·ª∏·ª≠·ªØ·ª±·ª≥·ªµ·ª∑·ªπ]\"\n",
    "def review_wordlist(review, remove_stopwords= False):\n",
    "    review_text = str(review)\n",
    "    # 2. Removing non-letter.\n",
    "    review_text = re.sub(vietnamese_chars,\" \",review_text)\n",
    "    # 3. Converting to lower case and splitting\n",
    "    words = review_text.lower().split()\n",
    "    # 4. Optionally remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords)     \n",
    "        words = [w for w in words if not w in stops]\n",
    "    for i,w in enumerate(words):\n",
    "        if w in mapping:\n",
    "            words[i]= mapping[w]\n",
    "    \n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CrfTokenizer()\n",
    "vietnamese_chars = \"[^a-zA-Z_√Ä√Å√Ç√É√à√â√ä√å√ç√í√ì√î√ï√ô√öƒÇƒêƒ®≈®∆†√†√°√¢√£√®√©√™√¨√≠√≤√≥√¥√µ√π√∫ƒÉƒëƒ©≈©∆°∆ØƒÇ·∫†·∫¢·∫§·∫¶·∫®·∫™·∫¨·∫Æ·∫∞·∫≤·∫¥·∫∂·∫∏·∫∫·∫º·ªÄ·ªÄ·ªÇ∆∞ƒÉ·∫°·∫£·∫•·∫ß·∫©·∫´·∫≠·∫Ø·∫±·∫≥·∫µ·∫∑·∫π·∫ª·∫Ω·ªÅ·ªÅ·ªÉ·ªÑ·ªÜ·ªà·ªä·ªå·ªé·ªê·ªí·ªî·ªñ·ªò·ªö·ªú·ªû·ª†·ª¢·ª§·ª¶·ª®·ª™·ªÖ·ªá·ªâ·ªã·ªç·ªè·ªë·ªì·ªï·ªó·ªô·ªõ·ªù·ªü·ª°·ª£·ª•·ªß·ª©·ª´·ª¨·ªÆ·ª∞·ª≤·ª¥√ù·ª∂·ª∏·ª≠·ªØ·ª±·ª≥·ªµ·ª∑·ªπ]\"\n",
    "def review_wordlist(review, remove_stopwords= False):\n",
    "    review_text = str(review)\n",
    "    # 2. Removing non-letter.\n",
    "    review_text = re.sub(vietnamese_chars,\" \",review_text)\n",
    "    # 3. Converting to lower case and splitting\n",
    "    review_text = review_text.lower()\n",
    "    for key, value in mapping.items():\n",
    "        review_text = re.sub(key,value,review_text)\n",
    "    words = tokenizer.tokenize(review_text)\n",
    "    # 4. Optionally remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords)     \n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DataSource()\n",
    "train_data = pd.DataFrame(ds.load_data('dataset/train.crash'))\n",
    "test_data = pd.DataFrame(ds.load_data('dataset/test.crash', is_train=False))\n",
    "train_data['review'] = train_data['review'].fillna(\"none\")\n",
    "test_data['review'] = test_data['review'].fillna(\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_000000</td>\n",
       "      <td>0</td>\n",
       "      <td>\"Dung dc sp tot cam onshop ƒê√≥ng g√≥i s·∫£n ph·∫©m r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_000001</td>\n",
       "      <td>0</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi . Son m·ªãn nh∆∞n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_000002</td>\n",
       "      <td>0</td>\n",
       "      <td>\" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi nh∆∞ng k c√≥ h·ªôp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_000003</td>\n",
       "      <td>1</td>\n",
       "      <td>\":(( M√¨nh h∆°i th·∫•t v·ªçng 1 ch√∫t v√¨ m√¨nh ƒë√£ k·ª≥ v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_000004</td>\n",
       "      <td>1</td>\n",
       "      <td>\"L·∫ßn tr∆∞·ªõc m√¨nh mua √°o gi√≥ m√†u h·ªìng r·∫•t ok m√† ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  label                                             review\n",
       "0  train_000000      0  \"Dung dc sp tot cam onshop ƒê√≥ng g√≥i s·∫£n ph·∫©m r...\n",
       "1  train_000001      0  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi . Son m·ªãn nh∆∞n...\n",
       "2  train_000002      0  \" Ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m tuy·ªát v·ªùi nh∆∞ng k c√≥ h·ªôp...\n",
       "3  train_000003      1  \":(( M√¨nh h∆°i th·∫•t v·ªçng 1 ch√∫t v√¨ m√¨nh ƒë√£ k·ª≥ v...\n",
       "4  train_000004      1  \"L·∫ßn tr∆∞·ªõc m√¨nh mua √°o gi√≥ m√†u h·ªìng r·∫•t ok m√† ..."
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vec model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-22 05:52:13,180 : INFO : loading projection weights from ./word2vec/wiki.vi.model.bin\n",
      "2019-02-22 05:52:17,748 : INFO : loaded (231486, 400) matrix from ./word2vec/wiki.vi.model.bin\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(231486, 400)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = './word2vec/wiki.vi.model.bin'\n",
    "#Load word2vec model\n",
    "if os.path.isfile(model):\n",
    "    print ('Loading word2vec model ...')\n",
    "if LooseVersion(gensim.__version__) >= LooseVersion(\"1.0.1\"):\n",
    "    from gensim.models import KeyedVectors\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(model, binary=True)\n",
    "else:\n",
    "    from gensim.models import Word2Vec\n",
    "    word2vec_model = Word2Vec.load_word2vec_format(model, binary=True)\n",
    "word2vec_model.wv.syn0.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from deepai_nlp.word_embedding.word2vec_gensim import BaseWord2Vec\n",
    "word2vec_model = BaseWord2Vec.load_model()\n",
    "word2vec_model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output = []\n",
    "try:\n",
    "    sim_list = word2vec_model.most_similar(\"th√≠ch\")\n",
    "    print(sim_list)\n",
    "    #output = word2vec_model.most_similar('u' + '\\\"' + 'A' + '\\\"', topn=5)\n",
    "\n",
    "    for wordsimilar in sim_list:\n",
    "        # output[wordsimilar[0]] = wordsimilar[1]\n",
    "        output.append(wordsimilar[0] + ' - '+ str(wordsimilar[1]))\n",
    "except:\n",
    "    print('except')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "def extract_emojis(str):\n",
    "    return [c for c in str if c in emoji.UNICODE_EMOJI]\n",
    "emojis_vocab = []\n",
    "for r in train_data['review']:\n",
    "    emojis_vocab += extract_emojis(r)\n",
    "emojis_vocab = np.unique(np.asarray(emojis_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmojiBowFeatures(reviews,vocab):\n",
    "    bow_emoji_features = []\n",
    "    for r in reviews:\n",
    "        emojis = extract_emojis(r)\n",
    "        bag_vector = np.zeros(len(vocab))\n",
    "        #print(emojis)\n",
    "        for e in emojis:\n",
    "            for i,emojii in enumerate(emojis_vocab):\n",
    "                if emojii == e: \n",
    "                    bag_vector[i] += 1\n",
    "        bow_emoji_features.append(bag_vector)\n",
    "    return np.asarray(bow_emoji_features)\n",
    "\n",
    "#getEmojiBowFeatures(\"üíöüíöüòëüíïüíï\", emojis_vocab)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_data.review, train_data.label, test_size=0.2,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "num_features = 400#100\n",
    "clean_train_reviews = []\n",
    "for review in train_data.review:\n",
    "    clean_train_reviews.append(review_wordlist(review, remove_stopwords=False))\n",
    "bow_train_features = getEmojiBowFeatures(train_data.review, emojis_vocab)\n",
    "bow_train_features = sc.fit_transform(bow_train_features)\n",
    "#print(bow_train_features.shape)\n",
    "length = np.asarray([len(r) for r in train_data.review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 16087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 16087\n",
      "Review 2000 of 16087\n",
      "Review 3000 of 16087\n",
      "Review 4000 of 16087\n",
      "Review 5000 of 16087\n",
      "Review 6000 of 16087\n",
      "Review 7000 of 16087\n",
      "Review 8000 of 16087\n",
      "Review 9000 of 16087\n",
      "Review 10000 of 16087\n",
      "Review 11000 of 16087\n",
      "Review 12000 of 16087\n",
      "Review 13000 of 16087\n",
      "Review 14000 of 16087\n",
      "Review 15000 of 16087\n",
      "Review 16000 of 16087\n"
     ]
    }
   ],
   "source": [
    "X_train = getAvgFeatureVecs(clean_train_reviews, word2vec_model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = length.reshape(16087,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train,bow_train_features),axis=1)\n",
    "#X_train = np.concatenate((X_train,length),axis=1)\n",
    "\n",
    "y_train = train_data.label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating average feature vactors for test set     \n",
    "clean_test_reviews = []\n",
    "for review in test_data.review:\n",
    "    clean_test_reviews.append(review_wordlist(review))\n",
    "bow_test_features = getEmojiBowFeatures(test_data.review, emojis_vocab)\n",
    "bow_test_features = sc.fit_transform(bow_test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 10981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 10981\n",
      "Review 2000 of 10981\n",
      "Review 3000 of 10981\n",
      "Review 4000 of 10981\n",
      "Review 5000 of 10981\n",
      "Review 6000 of 10981\n",
      "Review 7000 of 10981\n",
      "Review 8000 of 10981\n",
      "Review 9000 of 10981\n",
      "Review 10000 of 10981\n"
     ]
    }
   ],
   "source": [
    "X_test = getAvgFeatureVecs(clean_test_reviews, word2vec_model, num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-270-68ec1a145f3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbow_test_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "X_test = np.concatenate((X_test,bow_test_features),axis=1)\n",
    "X_test = np.concatenate((X_test,length),axis=1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import collections\n",
    "f = clean_train_reviews\n",
    "f = np.concatenate(clean_train_reviews).ravel()\n",
    "wordcount = {}\n",
    "for word in f:\n",
    "    if word not in wordcount:\n",
    "        wordcount[word] = 1\n",
    "    else:\n",
    "        wordcount[word] += 1\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Print most common word\n",
    "word_counter = collections.Counter(wordcount)\n",
    "#for word, count in word_counter.most_common(500):\n",
    " #   print(word, \": \", count)\n",
    "most_common = []\n",
    "\n",
    "for word, count in word_counter.most_common(200):\n",
    "    most_common.append(word)\n",
    "most_common\n",
    "#word_counter.most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16087, 226)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "XX_train, X_val, yy_train, y_val = train_test_split(X_train, y_train, test_size=0.2,\n",
    "    random_state=42)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "forest.fit(pd.DataFrame(XX_train).fillna(0), yy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8104412678682411"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = forest.predict(pd.DataFrame(X_val).fillna(0))\n",
    "accuracy_score(y_val, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8505282784338098"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "XX_train, X_val, yy_train, y_val = train_test_split(X_train, y_train, test_size=0.2,\n",
    "    random_state=42)\n",
    "sc = StandardScaler()\n",
    "#XX_train = sc.fit_transform(XX_train)\n",
    "#X_test = sc.transform(X_test)\n",
    "clf = svm.SVC(gamma='scale',verbose=True)\n",
    "clf.fit(pd.DataFrame(XX_train).fillna(0), yy_train)\n",
    "y_predict = clf.predict(pd.DataFrame(X_val).fillna(0))\n",
    "accuracy_score(y_val, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12869, 525)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "d_train = lgb.Dataset(df, label=y_train)\n",
    "params = {}\n",
    "params['learning_rate'] = 0.003\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'binary'\n",
    "params['metric'] = 'binary_logloss'\n",
    "params['sub_feature'] = 0.5\n",
    "params['num_leaves'] = 80\n",
    "params['min_data'] = 50\n",
    "params['max_depth'] = 20\n",
    "clf = lgb.train(params, d_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LightGBM. Train shape: (16087, 226), test shape: (10981, 350)\n",
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.448914\ttrain's f1: 0.858152\tvalid's binary_logloss: 0.502944\tvalid's f1: 0.756535\n",
      "[200]\ttrain's binary_logloss: 0.347329\ttrain's f1: 0.893712\tvalid's binary_logloss: 0.440639\tvalid's f1: 0.779528\n",
      "[300]\ttrain's binary_logloss: 0.284272\ttrain's f1: 0.919402\tvalid's binary_logloss: 0.411849\tvalid's f1: 0.790167\n",
      "[400]\ttrain's binary_logloss: 0.23836\ttrain's f1: 0.940111\tvalid's binary_logloss: 0.394463\tvalid's f1: 0.793017\n",
      "[500]\ttrain's binary_logloss: 0.202597\ttrain's f1: 0.95693\tvalid's binary_logloss: 0.38381\tvalid's f1: 0.796013\n",
      "[600]\ttrain's binary_logloss: 0.173818\ttrain's f1: 0.97022\tvalid's binary_logloss: 0.376507\tvalid's f1: 0.799858\n",
      "[700]\ttrain's binary_logloss: 0.150022\ttrain's f1: 0.980539\tvalid's binary_logloss: 0.371869\tvalid's f1: 0.80427\n",
      "[800]\ttrain's binary_logloss: 0.130127\ttrain's f1: 0.988078\tvalid's binary_logloss: 0.369203\tvalid's f1: 0.804826\n",
      "[900]\ttrain's binary_logloss: 0.113292\ttrain's f1: 0.992835\tvalid's binary_logloss: 0.367151\tvalid's f1: 0.807379\n",
      "[1000]\ttrain's binary_logloss: 0.0989237\ttrain's f1: 0.995587\tvalid's binary_logloss: 0.366586\tvalid's f1: 0.806532\n",
      "[1100]\ttrain's binary_logloss: 0.0867645\ttrain's f1: 0.99715\tvalid's binary_logloss: 0.366177\tvalid's f1: 0.808797\n",
      "Early stopping, best iteration is:\n",
      "[1044]\ttrain's binary_logloss: 0.0933486\ttrain's f1: 0.996231\tvalid's binary_logloss: 0.366307\tvalid's f1: 0.810082\n",
      "Fold 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttrain's binary_logloss: 0.451073\ttrain's f1: 0.852468\tvalid's binary_logloss: 0.498411\tvalid's f1: 0.75\n",
      "[200]\ttrain's binary_logloss: 0.349822\ttrain's f1: 0.888769\tvalid's binary_logloss: 0.432897\tvalid's f1: 0.775687\n",
      "[300]\ttrain's binary_logloss: 0.287065\ttrain's f1: 0.915044\tvalid's binary_logloss: 0.40097\tvalid's f1: 0.790079\n",
      "[400]\ttrain's binary_logloss: 0.241092\ttrain's f1: 0.93713\tvalid's binary_logloss: 0.382077\tvalid's f1: 0.802867\n",
      "[500]\ttrain's binary_logloss: 0.205406\ttrain's f1: 0.956616\tvalid's binary_logloss: 0.370453\tvalid's f1: 0.811449\n",
      "[600]\ttrain's binary_logloss: 0.176468\ttrain's f1: 0.968827\tvalid's binary_logloss: 0.362851\tvalid's f1: 0.814204\n",
      "[700]\ttrain's binary_logloss: 0.152583\ttrain's f1: 0.980267\tvalid's binary_logloss: 0.357899\tvalid's f1: 0.816356\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-292-8025c1cdefad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlgb_f1_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     )\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    214\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   1758\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0;32m   1759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1760\u001b[1;33m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[0;32m   1761\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1762\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(X_train.shape, X_test.shape))\n",
    "\n",
    "# Cross validation model\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=69)\n",
    "\n",
    "# Create arrays and dataframes to store results\n",
    "oof_preds = np.zeros(X_train.shape[0])\n",
    "sub_preds = np.zeros(X_test.shape[0])\n",
    "\n",
    "# k-fold\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X_train, y_train)):\n",
    "    print(\"Fold %s\" % (n_fold))\n",
    "    train_x, train_y = X_train[train_idx], y_train[train_idx]\n",
    "    valid_x, valid_y = X_train[valid_idx], y_train[valid_idx]\n",
    "\n",
    "    # set data structure\n",
    "    lgb_train = lgb.Dataset(train_x,\n",
    "                            label=train_y,\n",
    "                            free_raw_data=False)\n",
    "    lgb_test = lgb.Dataset(valid_x,\n",
    "                           label=valid_y,\n",
    "                           free_raw_data=False)\n",
    "\n",
    "    params = {\n",
    "        'objective' :'binary',\n",
    "        'learning_rate' : 0.01,\n",
    "        'num_leaves' : 76,\n",
    "        'feature_fraction': 0.64, \n",
    "        'bagging_fraction': 0.8, \n",
    "        'bagging_freq':1,\n",
    "        'boosting_type' : 'gbdt',\n",
    "    }\n",
    "\n",
    "    reg = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_train, lgb_test],\n",
    "        valid_names=['train', 'valid'],\n",
    "        num_boost_round=10000,\n",
    "        verbose_eval=100,\n",
    "        early_stopping_rounds=100,\n",
    "        feval=lgb_f1_score\n",
    "    )\n",
    "\n",
    "    oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=reg.best_iteration)\n",
    "    sub_preds += reg.predict(X_test, num_iteration=reg.best_iteration) / folds.n_splits\n",
    "\n",
    "    del reg, train_x, train_y, valid_x, valid_y\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "preds = (sub_preds > threshold).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'] = preds\n",
    "test_data[['id','label']].to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 10981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 10981\n",
      "Review 2000 of 10981\n",
      "Review 3000 of 10981\n",
      "Review 4000 of 10981\n",
      "Review 5000 of 10981\n",
      "Review 6000 of 10981\n",
      "Review 7000 of 10981\n",
      "Review 8000 of 10981\n",
      "Review 9000 of 10981\n",
      "Review 10000 of 10981\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-3bb7ae807644>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mrealTestDataVecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetAvgFeatureVecs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_real_test_reviews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2vec_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mbow_train_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetEmojiBowFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memojis_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mrealTestDataVecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrealTestDataVecs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbow_train_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "# Calculating average feature vactors for test set     \n",
    "clean_real_test_reviews = []\n",
    "for review in test_data['review']:\n",
    "    clean_real_test_reviews.append(review_wordlist(review))\n",
    "    \n",
    "realTestDataVecs = getAvgFeatureVecs(clean_real_test_reviews, word2vec_model, num_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_train_features = getEmojiBowFeatures(test_data['review'], emojis_vocab)\n",
    "realTestDataVecs = np.concatenate((realTestDataVecs,bow_train_features),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = realTestDataVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = clf.predict(pd.DataFrame(realTestDataVecs).fillna(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'] = y_predict\n",
    "test_data[['id','label']].to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
